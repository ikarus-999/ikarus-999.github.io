{"pages":[{"title":"About ikarus-999","text":"Intro. HE11ow World!!ì•ˆë…•í•˜ì„¸ìš” ë…¸ë˜, ìŒì•…, ì‘ê³¡ì„ ì¢‹ì•„í•˜ëŠ”ê°œë°œìì…ë‹ˆë‹¤ ğŸ•¹ê³µë¶€í•˜ëŠ”ê²Œ ì¦ê±°ì›Œì„œ ê°œë°œì„ ì‹œì‘í•˜ê²Œ ë˜ì—ˆì–´ìš” ğŸ’½I like studying ğŸ’»I Like Developing ğŸ–¥ Contacts ğŸ“« EMAIL = next.forr@gmail.com","link":"/about/index.html"}],"posts":[{"title":"GANí”„ë¡œì íŠ¸_try","text":"Style GAN toy í”„ë¡œì íŠ¸StyleGANì˜ íŠ¹ì§• ì´ë¯¸ì§€ë¥¼ Styleì˜ ì¡°í•©ìœ¼ë¡œ ë³´ê³ Generatorì˜ ê° Layerë§ˆë‹¤ Style ì •ë³´ë¥¼ ì…íˆëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ í•©ì„±ì´ ë•Œ ê° Layerì—ì„œ ì¶”ê°€ë˜ëŠ” Styleì€ ì´ë¯¸ì§€ì˜ Coarse Feature(í¬ì¦ˆ, ì„±ë³„ ë“±)ë¶€í„°Fine Detail(ë¨¸ë¦¬ìƒ‰, í”¼ë¶€í†¤ ë“±)ê¹Œì§€ê°ê¸° ë‹¤ë¥¸ Levelì˜ Visual ì†ì„±ë“¤ì„ ì¡°ì ˆ ê°€ëŠ¥StyleGANì€ ìƒê°ë³´ë‹¤ ì•ˆì •ì ì´ê³  ë†’ì€ í€„ë¦¬í‹°ì˜ ì´ë¯¸ì§€ ìƒì„± ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°(Module) GANì´ë€ ì–´ë–¤ ê²ƒì¼ê¹Œ??? Instance Norm? Generator êµ¬ì¡° ì„¤ëª… ì™¼ìª½ì´ Traditional Network, ì˜¤ë¥¸ìª½ì´ ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ Style-gased Generator. ì™¼ìª½ ë„¤íŠ¸ì›Œí¬ì™€ ì˜¤ë¥¸ìª½ì— Synthesis Networkê°€ ë˜‘ê°™ì€ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆì§€ë§Œ,ì´ì „ GANì—ì„œëŠ” Latent zë¥¼ ë°”ë¡œ Inputìœ¼ë¡œ ë„£ì–´ì¤¬ë˜ ê²ƒê³¼ëŠ” ë‹¤ë¥´ê²Œ,StyleGANì—ì„œëŠ” í•™ìŠµëœ Constant, (w) ê°’ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•¨. ìƒˆë¡­ê²Œ Mapping Networkì™€ Noiseê°€ ì¶”ê°€ë¨.. Wë¥¼ Featureì— ë§¤í•‘í•˜ëŠ” ê²½ìš°WëŠ” Zì²˜ëŸ¼ ê³ ì •ëœ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ. Sampling densityëŠ” í•™ìŠµëœ Piecewise Continuous Mapping f(z)(fëŠ” Mapping Network ì…ë‹ˆë‹¤)ì— ì˜í•´ ì •í•´ì§. ë”°ë¼ì„œ, Warping(í‹€ì–´ì§)ì´ ë§ì´ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ.ê·¸ë ‡ê¸° ë•Œë¬¸ì— Factors of variationì€ ë”ìš± Linearí•˜ê³ , Disentangled (ì–½íˆì§€ ì•ŠìŒ).ì´ê²ƒì´ ë°”ë¡œ zë¥¼ ê³§ë°”ë¡œ Featureì— ë§¤í•‘í•˜ëŠ” ê²ƒë³´ë‹¤ wì— ë§¤í•‘í•˜ëŠ” ê²ƒì˜ ì¥ì ì…ë‹ˆë‹¤ ê¸°ì¡´ì˜ Generator (a)ëŠ”Input Latent Vector (z)ê°€ ì§ì ‘ Convolution, Upsampling ë“±ì„ ê±°ì³ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë˜ëŠ” êµ¬ì¡°. Style-based Generator (b) ì˜ ê²½ìš°,(z)ê°€ Fully-connected Layerë¡œ êµ¬ì„±ëœ Mapping Networkì„ ê±°ì³Intermediate Latent Vector (w) ë¨¼ì € ë³€í™˜. (w)ëŠ” Constant Tensorê°€ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë˜ëŠ” ê³¼ì •ì—ì„œìŠ¤íƒ€ì¼ì„ ì…íˆëŠ” ì—­í• ì„ ìˆ˜í–‰. ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±. Style Transferë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°€ëŠ¥ì¼€í•˜ëŠ” Adaptive Instance Norm Synthesis Network (í•©ì„± ë„¤íŠ¸ì›Œí¬)zë¥¼ ì¤‘ê°„ latent space Wì— ë§¤í•‘ì„ í•œ ë’¤ì— ì´ wëŠ” â€œAâ€ë¥¼ ê±°ì³ì„œ style, y=(ys,yb)ë¡œ ë³€í˜•ë¨. ì´ë•Œ AëŠ” í•™ìŠµëœ affine transform ì„. ê·¸ë¦¬ê³  ì´ styleë“¤ì€AdaIN(adaptive instance normalization) opeartionì„ control í•¨. AdaINì€ style transferë¥¼ í•  ë•Œ ë§ì´ ì“°ì´ëŠ” ë°©ë²•ìœ¼ë¡œ, ì„ì˜ì˜ style transferë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°€ëŠ¥í•˜ê²Œ í•¨.ì—¬ê¸°ì„œ feature map xiëŠ”normalized ëœ ë‹¤ìŒì—, styleë¡œ ë³€í™˜ëœ ë‘ yë¡œ scaled, biased ë¨. (styleì´ ì…í˜€ì§)ì´ ê³¼ì •ì„ ë§¤ layer ë§ˆë‹¤ ë°˜ë³µí•¨. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ë°©ë²•ì€ scale-specific control ì„ ê°€ëŠ¥í•˜ê²Œ í•¨. To be continuedâ€¦","link":"/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/"},{"title":"fpost2","text":"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ë¨¸ì‹ ëŸ¬ë‹ ë¨¸ì‹ ëŸ¬ë‹ì€ ì •í˜• ë°ì´í„° í‘œ í˜•íƒœì˜ ë°ì´í„° ë”¥ëŸ¬ë‹ì€ ë¹„ì •í˜• ë°ì´í„° ê·¸ë¦¼, ì‚¬ì§„, ì˜¤ë””ì˜¤ í˜•íƒœì˜ ìì—° ë°ì´í„° ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ì¤€ë¹„ë¬¼ ì¶©ë¶„í•œ ìš©ëŸ‰(RAM 32GB++, SSD 512GB++, i7-10Gen++)ì˜ ë°ìŠ¤í¬íƒ‘ ê¶Œì¥! ê°€ì¥ ë¨¼ì € íŒŒì´ì¬, í…ì„œí”Œë¡œìš° ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. ì„¤ì¹˜ ë²„ì „ í™•ì¸!! ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤! íŒŒì´ì¬ í…ì„œí”Œë¡œìš° ë„˜íŒŒì´ &amp; ì‹¸ì´í‚·ëŸ° íŒŒì´ì¬ ë²„ì „ì„ ì—¬ëŸ¬ê°œ ì„¤ì¹˜ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ë•ŒëŠ” í™˜ê²½ ë³€ìˆ˜, IDE ê²½ë¡œ ì…‹íŒ…ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ìœˆë„ìš°ë¼ë©´ CMDë³´ë‹¤ëŠ” powershellì„ ì“°ëŠ”ê²ƒì´ í¸í•©ë‹ˆë‹¤. ì œì¼ ì‰¬ìš´ ë°©ë²•ì€ ì•„ë‚˜ì½˜ë‹¤ğŸ›â€¦ í•˜ì§€ë§Œ ìš©ëŸ‰ì´ í½ë‹ˆë‹¤. ì¨ë³¸ ê²°ê³¼ ë¦¬ëˆ…ìŠ¤ì— ë„ì»¤ê°€ ìµœì  í™˜ê²½ì…ë‹ˆë‹¤âš™ï¸. ì¶”í›„ í¬ìŠ¤íŒ… ì˜ˆì • ì…ë‹ˆë‹¤.ğŸ“¯ Lets Burn the GPU!!ğŸ”¥ğŸ”¥ğŸ”¥(TF2.0 wow!!)123456789101112131415161718192021import sysimport numpy as npimport tensorflow as tffrom datetime import datetime# tensorflow ì‹¤í–‰ëª¨ë“œë¥¼ í™•ì¸í•©ë‹ˆë‹¤print(tf.executing_eagerly())shape = (int(10000), int(10000))startTime = datetime.now()with tf.device(&quot;/gpu&quot;): random_matrix = tf.random.uniform(shape=shape, minval=0, maxval=1) dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix)) sum_operation = tf.reduce_sum(dot_operation)result = sum_operationprint(result)print(&quot;\\n&quot; * 2)print(&quot;Time taken:&quot;, datetime.now() - startTime)print(&quot;\\n&quot; * 2)","link":"/2020/10/24/fpost2/"},{"title":"firstpost1","text":"HEXO ë¸”ë¡œê·¸ ì„¤ì¹˜ì™€ ì‚¬ìš©ë²•ìƒˆ ê¸€ ì“°ëŠ” ë²•1$ hexo new [ë ˆì´ì•„ì›ƒì´ë¦„] &quot;ìƒˆ í¬ìŠ¤íŠ¸ì´ë¦„&quot; ë ˆì´ì•„ì›ƒ ë””í´íŠ¸(ë°”ë¡œ ë°œí–‰) : post ë°”ë¡œ ë°œí–‰ë˜ì§€ ì•ŠëŠ” ê¸€ : draft 1$ hexo publish &quot;ìƒˆ í¬ìŠ¤íŠ¸ì´ë¦„&quot; ìœ¼ë¡œ draftì—ì„œ publish í•©ë‹ˆë‹¤. publishëŠ” ì˜ ì“°ì§€ ì•ŠëŠ”ë‹¤ íŒŒì¼ëª… ì—ëŸ¬ë‚˜ê¸° ë•Œë¬¸. ê·¸ëƒ¥ëƒ…ë‹¤ MarkDownìœ¼ë¡œ ì“°ê³  hexo g -dë¡œ í•´ë²„ë¦¼","link":"/2020/10/24/firstpost1/"},{"title":"TF2","text":"Tensorflow 2.x ì‚¬ìš©ë²•Tensor ìƒì„± tf.constant() : list, tuple, Array ë¥¼ Tensorë¡œ ë°”ê¿ˆ tensor = tf.constant(arr) tensor.dtype : ë°ì´í„° íƒ€ì… í™•ì¸ tf.cast(tensor, dtype=tf.uint8) : TF int8ë¡œ ë°ì´í„°íƒ€ì… ë°”ê¾¸ê¸° tensor.numpy() : numpy arrayë¡œ ë°”ê¾¸ê¸° Tensorì— ëœë¤í•œ ìˆ«ìë“¤ ìƒì„± numpyì—ì„œëŠ” ê¸°ë³¸ì ì¸ normal distribution ìƒì„±np.random.randn(9) : 9ê°œì˜ ë¶ˆì—°ì†ì ì´ë©° ì¼ì •í•œ ë¶„í¬ ë‚œìˆ˜ ìƒì„± Distributionì— ë”°ë¥¸ ë‚œìˆ˜ ìƒì„± tf.random.normalì¤‘ì‹¬ê·¹í•œ ì´ë¡ ì— ì˜í•œ ì—°ì†ì ì¸ ëª¨ì–‘ tf.random.uniformë¶ˆì—°ì†ì ì´ë©° ì¼ì •í•œ ë¶„í¬","link":"/2020/11/22/TF2/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/10/24/hello-world/"},{"title":"fpost3","text":"ì¼ìƒì„ ë„ì ì´ë‹¤ - Mel Spectrogram ìë™ ë¶„ì„ì„ ìœ„í•œ test page ì…ë‹ˆë‹¤.M2U - March Of Fearë¹„íƒ„ìœ¼ë¡œ ê°€ë“ ì°¬ ì´ ë„ì‹œ ì•ˆì— ë–¨ì–´ì§€ëŠ” Melody, ê°ì •ì„ ì—°ê¸°í•˜ëŠ” ì¶¤ì¶”ëŠ” Endless rainâ€¦ ê±°ì§“ë§ë¡œ ì–¼ë£©ì§„ ê´€ê³„ë“¤ ì†ì— ì›ƒì„ ìˆ˜ ìˆëŠ” ê±°ì•¼? ì„¸ê³„ë¥¼ ë…¸ë˜í•˜ëŠ” The March of Fear. We live in the tragedy! ìœ ëª…í•œ ë¹„ê·¹ê³¼ ê°™ì€ ì”í˜¹í•œ ì„¸ê³„ ì†ì— ìš°ë¦¬ëŠ” ì›ƒìŒì§“ê³  ìˆì–´. ì´ ì„¸ê³„ë¥¼ ë¹„ì›ƒëŠ” ê³ ê²°í•œ ìˆ˜í˜¸ìë“¤ì—ê²Œ ì§€ì¼œë‚¼ ê²ƒë“¤ì€ ë‘ë ¤ì› ë˜ ìì‹ ë¿ì´ë‹ˆê¹Œ! ìˆ¨ì„ ë©ˆì¶˜ ì‚¬ëŒë“¤ ì†ì—ì„œ ì²­ëª…í•˜ê²Œ ìš¸ë ¤í¼ì§€ëŠ” Ariaâ€¦ ë³‘ë“  ë§ˆìŒìœ¼ë¡œ ê°€ë“ì°¬ ì´ ì„¸ê³„ì— í­ê²©ì„ ë‚ ë ¤! ì§€ê¸ˆ ì›ƒê³  ìˆëŠ” ë„ˆë„ Sociopath, ì•Œì–ì•„? ìš°ë¦¬ëŠ” ì´ ê³³ì— ìƒëª…ì´ ê¹ƒë“  í¬ì„±ì„ ë˜ì ¸! ì—í”½ì„¸ë¸ OST ã€ŒPromiseã€ê¹Šì€ ì ˆë§ ì†ì— ê°‡í˜€ ì“°ëŸ¬ì ¸ ê°€ëŠ” ë‚˜ì˜ ë‘ ì†ì„ê°€ë§Œíˆ ì¡ì•„ ì¤€ ë”°ìŠ¤í•œ ë„ˆì˜ ì˜¨ê¸°ë¥¼ ì•„ì§ë‚œ ê¸°ì–µí•´Now I can hear youëˆˆì„ ê°ìœ¼ë©´ì† ëì—ì„œ ë„ ëŠê»´Now I can hear youI can find youì´ì   ì•Œ ìˆ˜ ìˆì–´ ì•½ì†í•´ Promise I promise ìŠì§€ ì•Šì„ê²Œìš°ë¦¬ í•¨ê»˜ ë‚˜ëˆˆ ì•½ì†ë“¤ì„Promise I promise ì–´ë‘ ì„ ì§€ë‚œN ë²ˆì§¸ í•˜ëŠ˜ ì•„ë˜ ì´ê³³ì—ëì´ ë³´ì´ì§€ ì•Šì•„ë„ ë•Œë¡  ì§€ì³ì„œ ì£¼ì € ì•‰ì•„ë„ì•½ì†í•´ í¬ë§ì„ ìš°ë¦¬ ë§ˆìŒì— ê°„ì ˆíˆ ëª¨ì•„ë¹›ì„ í–¥í•´Now I can hear youëˆˆì„ ê°ìœ¼ë©´ì† ëì—ì„œ ë„ ëŠê»´Now I can hear youI can find youì´ì   ì•Œ ìˆ˜ ìˆì–´ì•½ì†í•´ Promise I promise ìŠì§€ ì•Šì„ê²Œìš°ë¦¬ í•¨ê»˜ ë‚˜ëˆˆ ì•½ì†ë“¤ì„Promise I promise ì–´ë‘ ì„ ì§€ë‚œN ë²ˆì§¸ í•˜ëŠ˜ ì•„ë˜ ì´ê³³ì— ë‹¤ì‹œ ì‹œì‘ëœ ì‹œê°„ ë‹¤ì‹œ ê±¸ì–´ê°€ëŠ” ê¸¸í•¨ê»˜í•˜ê¸°ì— ë‚˜ëŠ” ë¹›ë‚  ìˆ˜ ìˆì–´ë©€ë¦¬ ì¡°ê¸ˆì”© ë³´ì—¬ ë”°ëœ»í•œ íŒŒë€ ë¹›ì´ìš°ë¦¬ ë§ˆìŒì„ ì—¬ê¸°ì— ëª¨ì•„ ì‹œì‘í•´ í•œë²ˆ ë” ì•½ì†í•´ Promise I promise ìŠì§€ ì•Šì„ê²Œìš°ë¦¬ í•¨ê»˜ ë‚˜ëˆˆ ì•½ì†ë“¤ì„Promise I promise ì–´ë‘ ì„ ì§€ë‚œN ë²ˆì§¸ í•˜ëŠ˜ ì•„ë˜ ì•½ì†ë“¤ì„ Promise I promise ì–´ë‘ ì„ ì§€ë‚œN ë²ˆì§¸ í•˜ëŠ˜ ì•„ë˜ ì´ê³³ì— Mel Spectrogram ë¶„ì„ ì¤‘ì¸ ìŒì•…ì…ë‹ˆë‹¤. GPUì™€ ìŠ¤í† ë¦¬ì§€ ì§€ì›ì¢€ ë¶€íƒë“œë¦½ë‹ˆë‹¤.","link":"/2020/10/26/fpost3/"},{"title":"training_pc","text":"ë”¥ëŸ¬ë‹ ê³µë¶€ í›„ê¸° ë¹„ì •í˜• ë°ì´í„°ë¥¼ ë‹¤ë£°ë ¤ë©´ GPUëŠ” í•„ìˆ˜ë‹¤. Why GPU ?? CPUë³´ë‹¤ ë” ë¹ ë¥¸ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥ í–‰ë ¬ê³± ê³„ì‚°ì´ CPUë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„ ê³„ì‚° ê·¸ë˜í”„ ë¹Œë“œ, ì²˜ë¦¬ ì†ë„ê°€ ë¹ ë¦„. ë¹„ì •í˜• ë°ì´í„° ì²˜ë¦¬ì—” GPUê°€ í•„ìˆ˜ ìŒì„± ë”¥ëŸ¬ë‹, ë‚˜ë„ í•´ë³¼ê¹Œ?ìŒì„± ë”¥ëŸ¬ë‹ ìŒì„± ë”¥ëŸ¬ë‹ì€ ê²°ì½” ì‰½ì§€ ì•Šë‹¤. ë”¥ëŸ¬ë‹ê³„ì˜ ë³´ìŠ¤ê¸‰.ì‹ í˜¸ì²˜ë¦¬ ë°°ì›Œì•¼ ê·¸ë‚˜ë§ˆ ìˆ˜ì›”í•˜ë‹¤.ì´ˆë°˜ Feature Extraction ê²½í—˜ì„ ìŒ“ëŠ”ê²ƒì„ ê¶Œì¥í•œë‹¤.RNNê³„ì—´ì˜ LSTMìœ¼ë¡œ ì‹œì‘. í•˜ì§€ë§Œ,Attention, Transformer ë‚ ì½”ë”© ë“± ë…¼ë¬¸ êµ¬í˜„ ê²½í—˜ì´ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.Mel Spectrogramì„ í•œë‹¤ í•´ë„ ì›ë¦¬ë¥¼ ì˜ ì•Œì•„ì•¼ ë‚˜ì¤‘ì— ëª¨ë¸ë§ê³¼ ë°ì´í„° ë¶„í•´ê°€ ì‰½ë‹¤. ë”¥ëŸ¬ë‹ ì…ë¬¸ ë°©ë²•ë”¥ëŸ¬ë‹ ì…ë¬¸ í•˜ë ¤ë©´ ìˆ˜í•™ì€ í•„ìˆ˜. ì •ë§ ì¤‘ìš”. ë…¼ë¬¸ êµ¬í˜„ ì‹œë„í•´ë³´ê¸°1ë…¼ë¬¸ êµ¬í˜„ ì‹œë„í•´ë³´ê¸°2 ì–´ë–¤ ë…¼ë¬¸ì—ëŠ” íŒŒë¼ë¯¸í„° í•˜ë‚˜ë„ ì—†ëŠ” ê²ƒë„ ìˆë‹¤.ë ˆì´ì–´ êµ¬ì¡°ë„ë§Œ ìˆê³  íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê²ƒì€ ì§„ì§œ êµ¬í˜„ë‚œì´ë„ ë³´ìŠ¤ê¸‰. ë”¥ëŸ¬ë‹ ìê²©ì¦ ì·¨ë“ í›„ê¸°ì´ê²ƒì´ ë°”ë¡œ ë”¥ëŸ¬ë‹ ìê²©ì¦!! ë¬¸ì œ ìœ í˜• Category 1: Basic model Category 2: Model from learning dataset Category 3: Image classificationConvolutional Neural Network with real-world image dataset Category 4: Natural language processing (NLP)NLP Text Classification with real-world text dataset Category 5: Time series, sequences and predictionsSequence Model with real-world numeric dataset ì´ ì‹œí—˜ì€ ì‘ì‹œìê°€ TensorFlow 2.xë¥¼ í†µí•´ ëª¨ë¸ì„ ë¹Œë“œí•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹(ML) ë° ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ ì›ì¹™ TensorFlow 2.xì—ì„œ ML ëª¨ë¸ ê°œë°œí•˜ê¸° ì‹¬ì¸µì‹ ê²½ë§ ë° í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì„ í†µí•œ ì´ë¯¸ì§€ ì¸ì‹, ê°ì²´ íƒì§€, í…ìŠ¤íŠ¸ ì¸ì‹ ì•Œê³ ë¦¬ì¦˜ ë¹Œë“œ ì»´í“¨í„°ê°€ ì •ë³´ë¥¼ â€˜ë³´ëŠ”â€™ ë°©ì‹ê³¼ í”Œë¡¯ ì†ì‹¤ ë° ì •í™•ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë‹¤ë¥¸ í¬ê¸° ë° í˜•íƒœì˜ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ í•©ì„±ê³±ì—ì„œ ì´ë¯¸ì§€ì˜ ê²½ë¡œë¥¼ ì‹œê°í™” ê³¼ì í•©ì„ ì˜ˆë°©í•˜ê¸° ìœ„í•œ í™•ì¥ ë° ë“œë¡­ì•„ì›ƒê³¼ ê°™ì€ ì „ëµ íƒìƒ‰ TensorFlowë¥¼ ì´ìš©í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‹ ê²½ë§ ì ìš© Google ê³µì‹ í˜ì´ì§€ ë°œì·Œ í•©ê²© ì ìˆ˜ / ê·œì¹™ í—ˆìš©ëœ ì¸í„°ë„· ë¸Œë¼ìš°ì§•ì€ Tensorflow document, ì´ 100ì  ë§Œì ì— 90ì  ì»¤íŠ¸ë¼ì¸ ë‚œì´ë„ëŠ” category 5ê°€ ê°€ì¥ ë†’ìŒ ì‹œí—˜ ì‹œê°„ : 5ì‹œê°„ (ì»´í“¨í„° ë”¥ëŸ¬ë‹ í›ˆë ¨ì‹œê°„ í¬í•¨) ë”°ë©´ ì¢‹ì€ ì  ê°œë°œì ë„¤íŠ¸ì›Œí¬ì— join í•  ìˆ˜ê°€ ìˆì–´ìš”~ ìê²©ì¦ ë„¤íŠ¸ì›Œí¬ ì°¸ê³ ë¬¸ì„œ ê·¸ëŸ¼ 20000~!","link":"/2020/11/16/training-pc/"},{"title":"ë”¥ëŸ¬ë‹ ì…ë¬¸ê³¼ ì¤€ë¹„","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°Tensor ì´í•´í•˜ê¸° ì°¨ì› 0ì°¨ì›(ìƒìˆ˜) : Scalarê°’ 1ì°¨ì›(ë¦¬ìŠ¤íŠ¸ ì”Œìš´ ìƒìˆ˜), 2ì°¨ì›(2d), 3ì°¨ì›(3d), 4ì°¨ì›(4-d), nì°¨ì›(n-d) : Tensor Numpyë¡œ Tensor í‘œí˜„ê³¼ ì‘ìš©ì´ ê°€ëŠ¥ 123456import numpy as nparr = np.array([[3, 6, 9], [2, 4, 8]])print(arr.dtype) # dtype('float64')print(arr.shape) # (2, 3)print(arr.size) # 2 * 3 = 6 ì°¨ì› ëŠ˜ë¦¬ê¸°ì™€ ì¤„ì´ê¸° reshape, -1 í™œìš© 123arr.reshape(-1) # 1ì°¨ì›ìœ¼ë¡œ í¼ì¹˜ê¸°arr.reshape(-1, 3) # ì²«ë²ˆì§¸ ì°¨ì›ì€ ì•Œì•„ì„œ, ë‘ë²ˆì§¸ ì°¨ì›ì€ shape 3 Ravel() : arrì˜ ì°¨ì›ì„ 1ë¡œ ë°”ê¿ˆ(==&gt; Flatten) 123arr = np.array([[1, 2, 3], [4, 5, 6]]) # (2, 3)arr.ravel()arr.shape #(6, ) np.expand_dims() : ê°’ì„ ìœ ì§€í•˜ê³  ì°¨ì›ë§Œ ëŠ˜ë¦´ë•Œ 12arr = np.expand_dims(arr, -1) #(6, 1)arr.shape numpy arrayë¥¼ ë¹ ë¥´ê²Œ ì±„ìš°ëŠ” ë°©ë²•! 1234567891011121314# 0ìœ¼ë¡œ ì±„ìš°ê¸°arr2 = np.zeros([3, 4]) # 3 * 4ì˜ 0ì´ ì±„ì›Œì§„ ë°°ì—´one2 = np.ones([3, 4]) # 3 * 4ì˜ 1ë¡œ ì±„ì›Œì§„ ë°°ì—´five2 = np.ones([3, 4]) * 5 # 1ë¡œ ì±„ìš´ ê°’ì— 5ë¥¼ ë‹¤ ê³±í•¨arr2 = np.arange(n, m) # n ~ m-1ê¹Œì§€ì˜ ìˆ˜ë¡œ ë°°ì—´ ì±„ìš°ê¸°# array([n ~ m-1])arr = np.arange(5, 11).reshape(2, -1) # 5 ~ 10 : 6ê°œì˜ ìˆ«ì, (2, 3)arr # array([5, 6, 7] # [8, 9, 10]) ëª¨ì–‘ì´ ë§ì§€ ì•Šìœ¼ë©´ Errorâ€¦5, 6, 7, 8, 9ëŠ” 5ê°œì˜ ìˆ«ì5 * 1 ë§Œ ê°€ëŠ¥í•œ. Index &amp; slicing 123456789101112131415# ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ &amp; ìŠ¬ë¼ì´ì‹±nums = [2, 3, 4, 5, 6]nums[:-1] # ë§ˆì§€ë§‰ ìˆ«ì ì „ê¹Œì§€ í‘œì‹œnums[::-1] # ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ìˆ«ìë¥¼ ê±°ê¾¸ë¡œ í‘œí˜„nums = [[1, 2, 3], 4, 5, 6, 7]print(nums[0][1]) = 2 # ì²«ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ì¸ë±ìŠ¤ê°€ 1ì¸ ìˆ«ìarr = np.array([5, 6, 7], [8, 9, 10])print(arr[1, 2]) # 10 --&gt; ì¸ë±ì‹± [í–‰, ì—´]print(arr[1:, 1:]) # [[9, 10]] Boolean Indexing1234data = np.random.randn(3, 3)print(data&lt;=0) # False, Trueë¡œ ë‚˜ì˜´data[data &lt;=0] = 1 # 0 ì´í•˜ì¸ ê²ƒì„ 1ë¡œ ì±„ìš°ë‹¤","link":"/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/"},{"title":"ë”¥ëŸ¬ë‹ ì…ë¬¸ê³¼ ì¤€ë¹„2","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-2Broadcastë‘ê°œì˜ í–‰ë ¬ shapeê°€ ì„œë¡œ ë‹¬ë¼ë„í•œìª½ì˜ ì°¨ì›ì´ ê°™ê±°ë‚˜, ì—°ì‚°í•˜ëŠ” ê°’ì´ í•œ ê°œì¼ë•Œshapeì— ë§ê²Œ ë³µì‚¬í•´ì„œ ì—°ì‚°í•¨ 123456789101112131415161718192021arr = np.arange(6).reshape(-1, 3)# [[0, 1, 2], # [3, 4, 5]]arr + 3# [[3, 4, 5],# [6, 7, 8]]arr * 3# [[0, 3, 6],# [9, 12 15]arr + np.array([1, 2, 3])# [[1, 3, 5],# [4, 6, 8]]np.add(arr, 1)# ëª¨ë“  ì›ì†Œì— 1ì„ ë”í•¨np.multiply(arr, 3)# ëª¨ë“  ì›ì†Œì— 3ì„ ê³±í•¨ argmax, argmin ë°°ì—´ì˜ í° ê°’ì´ë‚˜ ì‘ì€ ê°’ì˜ index return 1234arr = np.array([1, 4, 6, 54, 3, 2])np.argmax(arr) # 54np.argmin(arr) # 1np.unique(arr) # ìœ ì¼í•œ ê°’ ì¶œë ¥","link":"/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„3","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-3ì°¨ì›ìˆ˜ ëŠ˜ë¦¬ê¸°, ì¤„ì´ê¸°(TF2.x)1234567x = tf.expand_dims(x, 1)x.shape # (x.shape, 1)x[..., tf.newaxis].shape # (x.shape, 1)np.squeeze(x[0]).shape # x.shape ì°¨ì› ì¤„ì´ê¸° TF2.x LayersConvolution filters : layerì—ì„œ ì¶œë ¥ë ë•Œ ëª‡ê°œì˜ filter kernel_size : filter(weight) ì˜ ì‚¬ì´ì¦ˆ strides : ëª‡ ê°œì˜ pixelë§Œí¼ skipí•˜ë©´ì„œ sliding window í•  ê²ƒì¸ì§€ padding : same, zero activation : í™œì„±í™” í•¨ìˆ˜(Linear functionì€ ì¸µì„ ìŒ“ëŠ” ì˜ë¯¸ê°€ ì—†ë‹¤) to be Continuedâ€¦","link":"/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„4","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-4ì¸ê³µì‹ ê²½ë§ê³¼ ì†ì‹¤í•¨ìˆ˜ ì¸ê³µì‹ ê²½ë§ì˜ ê¸°ë³¸ êµ¬ì¡° ë‡Œì˜ í•™ìŠµë°©ë²•ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í•œ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê¸°ë³¸ êµ¬ì¡° : y = Wx+b\\(x_i\\) : ì…ë ¥, \\(w_i\\): ê°€ì¤‘ì¹˜, b : bias, f: í™œì„±í™”í•¨ìˆ˜u : ê²°í•©(Net), z: ì¶œë ¥ ë‰´ëŸ°ì—ëŠ” ì„ í˜• ê²°í•©ê³¼ í™œì„±í™” í•¨ìˆ˜ ê¸°ëŠ¥ì´ ë“¤ì–´ìˆìŒ ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ë¨ ê° ë…¸ë“œì˜ ë‰´ëŸ° ì¶œë ¥ì€ ì§ì ‘ ì „ë‹¬ë˜ëŠ” ì •ë³´ì—ë§Œ ì˜ì¡´í•  ë¿ ë‹¤ë¥¸ ë…¸ë“œì™€ëŠ” ë¬´ê´€ ê·¸ë˜ì„œ? ë³‘ë ¬ì²˜ë¦¬ê°€ ê°€ëŠ¥í•¨. ì†ì‹¤ í•¨ìˆ˜(Loss or Cost function) ì‹ ê²½ë§ì˜ ì¶œë ¥ê°’ê³¼ ì‹¤ì œ ê²°ê³¼ê°’ì˜ ì°¨ì´ë¥¼ ì •ì˜í•˜ëŠ” í•¨ìˆ˜ ì‹ ê²½ë§ í•™ìŠµëª©í‘œëŠ” ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì›€ì§ì—¬ì•¼ í•¨ SGD, Adam ë“±ì˜ í•™ìŠµ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì†ì‹¤ í•¨ìˆ˜ íšŒê·€(Regression)ì œê³± ì˜¤ì°¨(MSE) ì‚¬ìš©, ìµœê·¼ì—ëŠ” rmse, maeì˜ ì¥ì ì´ ìˆëŠ” Huber Loss ì‚¬ìš©í•˜ëŠ” ì¶”ì„¸ Huber Loss?MAE + MSE -&gt; for Time Series Data!! ë¶„ë¥˜(Classification)í™œì„±í™” í•¨ìˆ˜ : softmax, ì†ì‹¤í•¨ìˆ˜ : cross-entropy ì•Œê³ ë¦¬ì¦˜ê³¼ ì—­ì „íŒŒ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê²½ì‚¬ í•˜ê°•ë²•: ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ S(\\theta) ê°’ì„ ìµœì í™” gradient(ê¸°ìš¸ê¸°)ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì¼ì • í¬ê¸°ë§Œí¼ ì´ë™í•˜ëŠ” ê²ƒì„ ë°˜ë³µí•˜ì—¬ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” \\thetaì˜ ê°’ì„ ì°¾ìŒ \\[\\theta = \\theta - \\eta \\nabla_\\theta S(\\theta)\\] ì´ ë–„ \\eta ëŠ” ë¯¸ë¦¬ ì •í•´ì§„ learning rate(step size) ì´ê³  ë³´í†µ 1e-3 ~ 1e-4 ì •ë„ë¥¼ ì‚¬ìš© ì—­ì „íŒŒ ê³„ì‚° ê·¸ë˜í”„ ë…¸ë“œëŠ” ì—°ì‚°ì„, ì—£ì§€ëŠ” ë°ì´í„°ì˜ íë¦„ë°©í–¥ sigmoid í•¨ìˆ˜ ì—­ì „íŒŒ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•(Chain Rule) í–‰ë ¬ì—°ì‚°ê³¼ ì—­ì „íŒŒ 1 ì´ì§„ë¶„ë¥˜ 2-layer NN ì—­ì „íŒŒ to be continuedâ€¦","link":"/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„6","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-6ğŸ’¥Remind!! ë”¥ëŸ¬ë‹ì— ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  ì„ í˜• í•¨ìˆ˜ë¡œëŠ” XORê³¼ ê°™ì€ non-linearí•œ ë¬¸ì œëŠ” í•´ê²°ì´ ì•ˆë¨;; ê·¸ëŸ¬ë©´ Hidden Layerë¥¼ ëŠ˜ë¦¬ë©´ ë˜ì§€ ì•Šì„ê¹Œ? $$f(ax+by) = af(x) + bf(y)$$ ë¼ëŠ” íŠ¹ì§• ë•Œë¬¸ì— N-layer ê¹Šì´ë¥¼ ì•„ë¬´ë¦¬ ìŒ“ì•„ë„ 1-Layerë¡œ ë™ì‘í•¨. ìµœì í™”(Opt) ì•Œê³ ë¦¬ì¦˜ ê²½ì‚¬í•˜ê°•ë²•(GD)$$\\theta = \\theta - \\eta \\nabla_\\theta S(\\theta)$$ Networkì˜ parameter=$$\\theta $$ ë¡œ í• ë•Œ ì†ì‹¤í•¨ìˆ˜ $$J(\\theta)$$ì˜ ê°’ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê¸°ìš¸ê¸°$$\\nabla J(\\theta)$$ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•GDì—ì„œëŠ” Gradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì¼ì • í¬ê¸°(lr)ë§Œí¼ ì´ë™í•˜ëŠ” ê²ƒì„ ë°˜ë³µí•˜ì—¬ loss functionì˜ ê°’ì„ ìµœì†Œí™” í•˜ëŠ” $$\\theta$$ì˜ ê°’ì„ ì°¾ìŒ, lr $$\\eta$$ ëŠ” ë³´í†µ 1e-3 ~ 1e-4 ì‚¬ì´ì—ì„œ ì‚¬ìš©í•¨.ë„ˆë¬´ í¬ë©´ global minimumì„ ì§€ë‚˜ì¹˜ê³  ë„ˆë¬´ ì‘ìœ¼ë©´ Local Minimumì— ë¹ ì§. í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ì „ì²´ Training setì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ batch Gradient Descent, ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´mini-batchì— ëŒ€í•´ì„œë§Œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í™•ë¥ ì  GDë¥¼ ì‚¬ìš©í•¨.ê°™ì€ ì‹œê°„ì— ë” ë§ì€ stepë¥¼ ê°ˆ ìˆ˜ ìˆìŒ, ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•  ê²½ìš° batchì˜ ê²°ê³¼ì™€ ë¹„ìŠ·í•¨ GD vs SGDGD : í™•ì‹¤í•œë° ë„ˆë¬´ ëŠë¦¼ | SGD : ì¡°ê¸ˆ í—¤ë©”ì§€ë§Œ ë¹ ë¦„ Momentum : í˜„ì¬ Gradientë¥¼ í†µí•´ ì´ë™í•˜ëŠ” ë°©í–¥ê³¼ ë³„ê°œë¡œ ê³¼ê±°ì˜ ì´ë™ë°©ì‹ì„ ê¸°ì–µí•˜ë©´ì„œ ì¼ì¢…ì˜ ê´€ì„±ì„ ì£¼ëŠ” ë°©ì‹ AdaGrad(Adaptive Gradient) ë§ì´ ë³€í™”í–ˆë˜ ë³€ìˆ˜ë“¤ì€ step sizeë¥¼ ì‘ê²Œ í•˜ëŠ” ê²ƒìì£¼ ë“±ì¥í•˜ê±°ë‚˜ ë³€í™”ë¥¼ ë§ì´ í•œ ë³€ìˆ˜ë“¤ì€ optimumì— ê°€ê¹Œì´ ìˆì„ í™•ë¥ ì´ ë†’ê¸° ë•Œë¬¸ì— ì‘ì€ í¬ê¸°ë¡œ ì´ë™í•˜ë©´ì„œ ë¯¸ì„¸ì¡°ì ˆ ì ê²Œ ë³€í™”í•œ ë³€ìˆ˜ë“¤ì€ ë§ì´ ì´ë™í•´ì•¼í•  í™•ë¥ ì´ ë†’ê¸° ë•Œë¬¸ì— ë¨¼ì € ë¹ ë¥´ê²Œ lossê°’ì„ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë™í•˜ëŠ” ë°©ì‹í•™ìŠµì„ ê³„ì† ì§„í–‰í•˜ë©´ step sizeê°€ ë„ˆë¬´ ì¤„ì–´ë“œëŠ” ë‹¨ì ì´ ìˆìŒ. RMSPropí•©ì„ ì§€ìˆ˜í‰ê· ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ Adagradì˜ ë‹¨ì ì„ í•´ê²°Gê°€ ë¬´í•œì • ì»¤ì§€ì§€ëŠ” ì•Šìœ¼ë©´ì„œ ìµœê·¼ ë³€í™”ëŸ‰ì˜ ë³€ìˆ˜ê°„ ìƒëŒ€ì ì¸ í¬ê¸° ì°¨ì´ëŠ” ìœ ì§€í•  ìˆ˜ ìˆìŒ. AdamMomentum + RMSProp ì§€ê¸ˆê¹Œì§€ ê³„ì‚°í•´ì˜¨ ê¸°ìš¸ê¸°ì˜ ì§€ìˆ˜í‰ê· ì„ ì €ì¥ rmspropê³¼ ìœ ì‚¬í•˜ê²Œ Gradientì˜ ì œê³±ê°’ì˜ ì§€ìˆ˜í‰ê· ì„ ì €ì¥ Overfitting(ê³¼ì í•©) Training Setì˜ ì§€ì—½ì ì¸ íŠ¹ì„±ê¹Œì§€ ë°˜ì˜í•´ Variance Highë¡œ Trainingë˜ì–´ì„œ Training Setì„ ì•”ê¸°í•´ë²„ë¦¬ëŠ” í˜„ìƒ Test Setì„ ì˜ ì˜ˆì¸¡í•˜ì§€ ëª»í•¨ ì£¼ë¡œ í‘œí˜„ë ¥ì´ ë†’ì€ ëª¨ë¸, ì¦‰ íŒŒë¼ë¯¸í„°ê°€ ë§ì€ ëª¨ë¸ì— ë°œìƒ ì •ê·œí™”(Regularization) ì†ì‹¤í•¨ìˆ˜ì— ê°€ì¤‘ì¹˜ì˜ í¬ê¸°ë¥¼ í¬í•¨ ê°€ì¤‘ì¹˜ê°€ ì‘ì•„ì§€ë„ë¡ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì€ Outlier(Noise)ì˜ ì˜í–¥ì„ ì ê²Œ ë°›ìŒ L2 ì •ê·œí™” Rigde Regression L1 ì •ê·œí™”Sparse Modelì— ì•Œë§ìŒ.. ì‘ì€ ê°€ì¤‘ì¹˜ë“¤ì´ ê±°ì˜ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ì—¬ ëª‡ê°œì˜ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë“¤ë§Œ ë‚¨ìŒ. Lasso Regression ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ì ì´ ìˆê¸° ë•Œë¬¸ì— Gradient-Base Learningì—ëŠ” ì£¼ì˜.. DropOutê° ë ˆì´ì–´ì˜ ì¼ì • ë¹„ìœ¨ë¡œ ë‰´ëŸ°ì˜ ì¶œë ¥ ê°’ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë‚˜ë¨¸ì§€ ë‰´ëŸ°ë“¤ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ê³¼ì í•©ì„ íš¨ê³¼ì ìœ¼ë¡œ ì˜ˆë°© ê°€ëŠ¥(Network ë‚´ë¶€ì˜ Ensemble í•™ìŠµìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ) ì—­ì „íŒŒëŠ” ReLUì²˜ëŸ¼ ë™ì‘Forward Propagationë•Œ ì‹œê·¸ë„ì„ í†µê³¼ì‹œí‚¨ ë‰´ëŸ°ì€ Backwardë•Œë„ í†µê³¼ì‹œí‚´dropëœ ë‰´ëŸ°ì€ Backward Propagationë•Œë„ ì‹œê·¸ë„ ì°¨ë‹¨ ë°˜ë©´, TESTë•ŒëŠ” ëª¨ë“  ë‰´ëŸ°ì— ì‹ í˜¸ë¥¼ ì „ë‹¬í•¨ Batch Normalizationí•™ìŠµí•˜ëŠ” ì´ì „ ì¸µì˜ íŒŒë¼ë¯¸í„° ë³€í™”ë¡œ í˜„ì¬ì¸µì˜ ì…ë ¥ ë¶„í¬ê°€ ë°”ë€ŒëŠ” í˜„ìƒì„ ë‚´ë¶€ ê³µë¶„ì‚° ë³€í™”(Internal Covariate Shift)ì´ì „ ì¸µì˜ ì‘ì€ íŒŒë¼ë¯¸í„° ë³€í™”ê°€ ì¦í­ë˜ì–´ ë’· ë ˆì´ì–´ì— í° ì˜í–¥ì„ ë°›ìŒ.ê·¸ë˜ì„œâ€¦ BN(2015) Gradient Vanishing, Explodingì„ ë°©ì§€í•˜ëŠ” ëŒ€í‘œì ì¸ ë°©ë²• ì§ì ‘ì ì¸ ë°©ë²•ì„. Training ê³¼ì • ìì²´ë¥¼ ì•ˆì •í™”ì‹œì¼œ í•™ìŠµì†ë„ë¥¼ ê°€ì†í™” í‰ê· ê³¼ ë¶„ì‚°ì„ ì¡°ì ˆí•˜ëŠ” ê³¼ì •ì´ NN ì•ˆì— í¬í•¨ ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì´ í•µì‹¬ì  Trainingí• ë•Œê° Mini Batchë§ˆë‹¤ $$\\gamma$$ ì™€ $$\\beta$$ë¥¼ êµ¬í•˜ê³  ì €ì¥í•´ ë‘  Testí• ë•Œêµ¬í–ˆë˜ $$\\gamma$$ ì™€ $$\\beta$$ì˜ í‰ê· ì„ ì‚¬ìš© Data Augmentationì¼ì¢…ì˜ Regularizationì‘ì—…, ë°ì´í„°ê°€ ì ì„ ë•Œ ì‚¬ìš©í•˜ë©´ ë§¤ìš° íš¨ê³¼ì ì¦‰ ë°ì´í„° ë³€í˜•","link":"/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„5","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-5ì„ í˜•ëŒ€ìˆ˜ ë°°ì›Œë³´ê¸°(í–‰ë ¬ì„ ì•„ë¬´ë¦¬ ê³±í•˜ê³  ë”í•´ë„ ì„ ëª¨ì–‘)Scala : í¬ê¸°ë§Œ ì¡´ì¬í•˜ëŠ” ì–‘Vector : ì†ë„, ìœ„ì¹˜ì´ë™, í˜, ê³µê°„ë’¤í‹€ë¦¼ê³¼ ê°™ì´ í¬ê¸°ì™€ ë°©í–¥ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì–‘ Norm ? nì°¨ì› ë²¡í„° $$\\vec{x} = (x_1, x_2, \\cdots x_n)$$Norm $$\\lVert x \\rVert = \\sqrt{x_1^1 + x_2^2 + \\cdots + x_n^2}$$ â€œì›ì  Oì—ì„œ ì \\(x_1, x_2, \\cdots, x_n\\) ê¹Œì§€ì˜ ê±°ë¦¬â€ ë‚´ì  ? Inner product, Dot productí–‰ë ¬ë¼ë¦¬ ê³±í•  ë•ŒëŠ” ì°¨ì›ì„ ì£¼ì˜í•œë‹¤. A(m, n) * B(n, m) ë§Œ ê°€ëŠ¥ Transpose: ì „ì¹˜í–‰ë ¬(í–‰ê³¼ ì—´ì„ ë’¤ë°”ê¿ˆ) A.T numpy ì—°ì‚°(Element-wise operation) np.dot(x, y) (aka ë‚´ì , dot-product)ì™€ x * y(element-wise)ëŠ” ì„œë¡œ ë‹¤ë¦„. numpy ë¹„êµ, ë…¼ë¦¬ì—°ì‚°(element-wise operation)numpy Reductions argmax() : ìµœëŒ€ê°’ìˆëŠ” ì¸ë±ìŠ¤ë¥¼ ë¦¬í„´, argmin() : ìµœì†Œê°’ì˜ ì¸ë±ìŠ¤ ë¦¬í„´ np.all, np.any? ALL : Arrayë‚´ ëª¨ë“  ê°’ì´ TRUEì¸ê°€? any : Arrayë‚´ ê°’ì´ í•˜ë‚˜ë¼ë„ TRUEì¸ê°€? np.mean, np.median, np.std ë“± í†µê³„í•¨ìˆ˜ ì‚¬ìš© ê°€ëŠ¥ë”¥ëŸ¬ë‹ì— ëŒ€í•œ í™˜ìƒ ë³µì¡í•œ ë¬¸ì œë„ ì¸µì„ ê¹Šê³  ë„“ê²Œ ìŒ“ìœ¼ë©´ í•´ê²°ëœë‹¤ â€“&gt; Gradient Vanhshing, Initialize fault ìœ¼í•˜í•˜í•°ã…‹ã…‹ã…‹ $$Sigmoid(z) = \\frac{1} {1 + e^{-z}}$$Sigmoid ë„í•¨ìˆ˜ì˜ ìµœëŒ€ê°’ì€ 1/4 â€¦ â€“&gt; ê·¸ë˜ì„œ Gradient Vanishing ë‚˜ëŠ”ê±°ì„ ã…‡ã…‡ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì´ˆê¸°í™”ì˜ ì¤‘ìš”ì„±$$t = wx+b$$ ì—ì„œ wê°€ 100, bê°€ 50ì´ë¼ë©´ xê°€ 0.01ì´ë”ë¼ë„ tëŠ” 51ì´ ë¨ì—­ì „íŒŒë•Œ sigmoid í•¨ìˆ˜ í†µê³¼ì‹œí‚¤ë©´ $$\\sigmaâ€™ (51)$$ ë¦¬í„´ë¨í•˜ì§€ë§Œ tê°€ 5ë§Œ ë„˜ì–´ë„ $$\\sigma (t)$$ ëŠ” 0ì— ìˆ˜ë ´ â€“&gt; ì´ê²ƒì´ ë°”ë¡œ Gradient Vanishingâ€¦ ê·¸ë˜ì„œ ì…ë ¥ì¸µì˜ ê°€ì¤‘ì¹˜wë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ë¦¬ì…‹!Forward Propagationë•Œ ë‘ë²ˆì§¸ ì¸µ ë‰´ëŸ°ì— ëª¨ë‘ ê°™ì€ ê°’ì´ ì „ë‹¬ë¨Backward Propagationë•Œ ë‘ì§¸ ì¸µ ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ ë˜‘ê°™ì´ ì—…ë°ì´íŠ¸ ==&gt; ì‹ ê²½ë§ í‘œí˜„ë ¥ ì œí•œ BiasëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ”ê²Œ ì¼ë°˜ì ìœ¼ë¡œ íš¨ìœ¨ì  ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” 2 í‘œì¤€ ì •ê·œë¶„í¬ë¥¼ ì´ìš©í•œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”Sigmoidí•¨ìˆ˜ì˜ ì¶œë ¥ê°’ì´ ê·¹ë‹¨ì ìœ¼ë¡œ(0 or 1)ì— ì¹˜ìš°ì¹˜ëŠ” í˜„ìƒ â€“&gt; Gradient Vanishing í‘œì¤€í¸ì°¨ë¥¼ 0.01ë¡œ í•˜ëŠ” ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°í™”ê°€ì¤‘ì¹˜ê°€ ëª¨ì—¬ ìˆìŒ =&gt; ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ì–´ëŠì •ë„ ì™„í™”ë¨ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” 3 Xavierì´ˆê¸°í™” ë°©ë²•(2010) 1w = np.random.randn(n_input, n_output) / (n_input) ** 0.5 Sigmoidì™€ ê°™ì€ Sì í•¨ìˆ˜ì˜ ê²½ìš° ì¶œë ¥ê°’ë“¤ì´ ì •ê·œë¶„í¬ í˜•íƒœì´ì–´ì•¼ ì•ˆì •ì  í•™ìŠµ ê°€ëŠ¥ Sigmoid functionê³¼ Xavier Initë°©ë²•ì„ ì‚¬ìš©í–ˆì„ ê²½ìš° ê·¸ë˜í”„ ReLU ê³„ì—´ í•¨ìˆ˜ì—ëŠ” ì ì ˆí•˜ì§€ ì•ŠìŒlayerë¥¼ ê±°ì³ê°ˆ ìˆ˜ë¡ 0ì— ìˆ˜ë ´(converge) ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” 4He ì´ˆê¸°í™” ë°©ë²•(2015) 1w = np.random.randn(n_input, n_output) / (n_input / 2) ** 0.5 RELU + He init â€“&gt; 10 layerë¥¼ ê±°ì³ë„ í‘œì¤€í¸ì°¨ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ Summary ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ëŠ” ë„ˆë¬´ë‚˜ ì¤‘ìš”í•¨ tanhì˜ ê²½ìš° Xavier Init ë°©ë²•ì´ íš¨ìœ¨ì  ReLUê³„ì—´ í•¨ìˆ˜ì—ëŠ” He Init ë°©ë²•ì´ íš¨ìœ¨ì  ìµœê·¼ì—” ëŒ€ë¶€ë¶„ He Initë¥¼ ì£¼ë¡œ ì‚¬ìš©","link":"/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/"},{"title":"R ë„ì ì—¬ë³´ê¸°","text":"R ì‹œì‘í•´ë³´ê¸°R ë³€ìˆ˜ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137v1 &lt;-c(0, 1, 2, 3, NA) # ë²¡í„°v2 &lt;- 'a' # ë™ì  ë³€ìˆ˜ì— ë¬¸ì assignv2 = 'a' # ì •ì  ë³€ìˆ˜rm(v1, v2) # ë³€ìˆ˜ ì§€ìš°ê¸°v1 &lt;- c(0, 1, 2, 3)v2 &lt;- c(0, 1, 3, NA) # Errorv3 &lt;- c(0, 1, 3, NULL) # okmean(v1)mean(v2) # Errormean(v3) # okTRUE | TRUE # TRUETRUE | FALSE # TRUE!TRUE # ë’¤ì§‘ê¸° ì—°ì‚°!FALSE# ë²”ì£¼í˜• ë³€ìˆ˜key &lt;- factor('m', c('m', 'f'))nlevels(key)levels(key)[1] # 1ë ˆë²¨ ì¶”ì¶œlevels(key)[2] # ì¢…ë¥˜ ì¶”ì¶œlevels(key) &lt;- c('m', 'f')#vectorx &lt;- c(1,2,3,4,5)xy &lt;- c(1,2,3,4,&quot;a&quot;)ynames(x) &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;)xx[1]x[2]x[-1]x[-2]x[1:3]x[3:5]x[&quot;a&quot;]names(x)[2]length(x)nrow(x)NROW(x)seq(1,10) # rangeseq(1:10)seq(1,10,1)seq(1,10,3)NROW(seq(1,10,3))rep(1:10, times=2) #ë°˜ë³µrep(1:10, each=2)rep(1:10, each=2, times=2)#listx &lt;- list(name=&quot;jo&quot;, he=&quot;999&quot;)xx$namex$hex[1]x[2]x[[1]] # ì¸ë±ì‹±x[[2]]y &lt;- list(name=&quot;k&quot;, he=c(1,2,3))yy[[1]]y[[2]]z &lt;- list(c(&quot;a&quot;,&quot;b&quot;), c(1,2,3,4))zz[[1]][2]z[[2]][-1]z[[2]][2:4]length(z[[1]])length(z[[2]])#matrixmatrix(c(1,2,3,4,5,6,7,8,9,10), nrow=2)matrix(c(1,2,3,4,5,6,7,8,9,10), nrow=5)m &lt;- matrix(1:9, nrow=3, dimnames=list(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;)))mrownames(m)colnames(m)m[1:2,]m[,1:2]m[1,1]m[3,3]m[-1,]m[,-2]m[-1,-2]m[-1,-3]m[m[,3]&gt;8,]#m[m[2:3]2]m[m[,3]&gt;=8,c(1,2)]m[c(2,3),c(1,2)]m[m[,3]&gt;=8,m[1,]&lt;5]mm[c(2,3),c(1,3)]#arrayarray(1:12, dim=c(3,4))x &lt;- array(1:12, dim=c(2,2,3))xx[,,2]m[1,]m1 &lt;- matrix(c(1,2,3,4,5,6,7,8,9), nrow=5)m1name &lt;- c(&quot;kim&quot;, &quot;lee&quot;, &quot;park&quot;)age &lt;- c(10,20,30)gender &lt;-factor(c(&quot;M&quot;,&quot;F&quot;,&quot;M&quot;))df &lt;- data.frame(name, age, gender)dfstr(df)df[df$age &gt;= 20 &amp; gender == &quot;F&quot;, c(&quot;name&quot;,&quot;age&quot;)]df[df$age &gt;= 20 &amp; gender == &quot;F&quot;, &quot;name&quot;]df[df[,2]&gt;=20 &amp; df[,3] == &quot;F&quot;, c(1,2)]","link":"/2020/11/29/R-%EB%81%84%EC%A0%81%EC%97%AC%EB%B3%B4%EA%B8%B0/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„7","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-7í•©ì„±ê³±(Convolution) Convolution? ì •ì˜ í•©ì„±ê³± ì—°ì‚°ì€ ë‘ í•¨ìˆ˜ f, g ê°€ìš´ë° í•˜ë‚˜ì˜ í•¨ìˆ˜ë¥¼ ë°˜ì „(reverse), ì „ì´(shift)ì‹œí‚¨ ë‹¤ìŒ, ë‹¤ë¥¸ í•˜ë‚˜ì˜ í•¨ìˆ˜ì™€ ê³±í•œ ê²°ê³¼ë¥¼ ì ë¶„í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì´ë¥¼ ìˆ˜í•™ ê¸°í˜¸ë¡œ í‘œì‹œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. {\\displaystyle (fg)(t)=\\int _{-\\infty }^{\\infty }f(\\tau )g(t-\\tau ),d\\tau }(fg)(t)=\\int _^{\\infty }f(\\tau )g(t-\\tau ),d\\tau ë˜í•œ g í•¨ìˆ˜ ëŒ€ì‹ ì— f í•¨ìˆ˜ë¥¼ ë°˜ì „, ì „ì´ ì‹œí‚¤ëŠ” ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œí•  ìˆ˜ë„ ìˆë‹¤. ì´ ë‘ ì—°ì‚°ì€ í˜•íƒœëŠ” ë‹¤ë¥´ì§€ë§Œ ê°™ì€ ê²°ê³¼ê°’ì„ ê°–ëŠ”ë‹¤. {\\displaystyle (fg)(t)=\\int _{-\\infty }^{\\infty }f(t-\\tau )g(\\tau ),d\\tau }(fg)(t)=\\int _^{\\infty }f(t-\\tau )g(\\tau ),d\\tau ìœ„ì˜ ì ë¶„ì—ì„œ ì ë¶„ êµ¬ê°„ì€ í•¨ìˆ˜ fì™€ gê°€ ì •ì˜ëœ ë²”ìœ„ì— ë”°ë¼ì„œ ë‹¬ë¼ì§„ë‹¤. ë˜í•œ ë‘ í™•ë¥  ë³€ìˆ˜ Xì™€ Yê°€ ìˆì„ ë•Œ ê°ê°ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ë¥¼ fì™€ gë¼ê³  í•˜ë©´, X+Yì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ëŠ” {\\displaystyle fg,}fg,ë¡œ í‘œì‹œí•  ìˆ˜ ìˆë‹¤. ë¬´ì—‡ì¸ì§€ ëª¨ë¥´ê² ì£ ? ì‰½ê²Œ ë§í•˜ìë©´ ê¸°ì¡´ MLPì—ì„œëŠ” ì´ë¯¸ì§€ê°€ ì‚´ì§ì´ë¼ë„ íšŒì „ì´ ë˜ê±°ë‚˜ ìœ„ì¹˜ ì´ë™ì´ ìˆë‹¤ë©´ ì‹ ê²½ë§ ìì²´ë¥¼ ë‹¤ì‹œ í•™ìŠµí•´ì•¼ í•˜ì§€ë§Œ CNNì€ ì´ë¯¸ì§€ì˜ ë³€í™”ê°€ ìˆì–´ë„ ì¬í•™ìŠµ ì—†ì–´ë„ ê°€ëŠ¥í•¨. ëª¨ë“  pixelì„ ë¹„êµí•  ê²Œ ì ˆëŒ€ ì•„ë‹˜. Feature ì¶”ì¶œì— ì¤‘ì ì„ ë‘ . $$C_in x C_out$$ ë²ˆì˜ í•©ì„±ê³± ì—°ì‚° biasëŠ” í•˜ë‚˜ì˜ ë²¡í„° Filter(kernel)ì˜ í¬ê¸°ì— ë”°ë¼ ì˜ìƒì˜ í¬ê¸°ê°€ ì¤„ì–´ë“œëŠ” ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ paddingì„ ì“´ë‹¤. í¬ê¸°ê°€ (2N + 1)ì¸ ì»¤ë„ì— ìƒí•˜ì¢Œìš°ì— Nê°œ Zero paddingì„ í•´ì£¼ë©´ ëœë‹¤. Sliding Window ë°©ì‹ìœ¼ë¡œ ì»¤ë„ì´ ì´ë™ë˜ëŠ”ë° ê·¸ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ë ¤ë©´ Strideë¥¼ ì“´ë‹¤. ë„ˆë¬´ í¬ë©´ ì¶œë ¥ Feature Mapì´ ê³¼ë„í•˜ê²Œ ì¤„ì–´ë“œëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤. ë³´ë‹¤ íš¨ìœ¨ì ì¸ Conv ì—°ì‚°ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” 1x1 Convë¥¼ ë„£ëŠ”ë‹¤ ì—°ì‚°ëŸ‰, íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ BottleNeck êµ¬ì¡°ë¥¼ í™œìš©í•œë‹¤. í•˜í•„ 1x1 ?? 3x3 filter í•œê°œì™€ 1x1 + 3x3 parameter ë¹„êµ ê·¸ë˜ë„ ëª¨ë¥´ê² ë‹¤ë©´?? 12345678910111213141516import numpy as npnp.random((3, 3)).shape == (np.random((3, 1)) * np.random((1, 3)).shape)&gt;&gt; True# keras# k - kernel_size(ex. 3, 5, 7...)# n_filter - number of filters/channels í•„í„° ê°¯ìˆ˜conv1_1 = Conv(n_filters, (1, k))(input_1)conv1_2 = Conv(n_filters, (k, 1))(conv1_1)# ì™œ ë³‘ëª©?conv2 = Conv2D(96, (1, 1), ...)(conv1) # ì¤„ì˜€ë‹¤ê°€(receptive FieldëŠ” ê·¸ëŒ€ë¡œ, Feature mapì„ ë¯¸ë¦¬ ì¤„ì„.)conv3 = Conv2D(96, (3, 3), ...)(conv2) conv4 = Conv2D(128, (1, 1), ...)(conv3) # ë‹¤ì‹œ ëŠ˜ë¦¼ CNN ë§Œë“¤ì—ˆëŠ”ë° ë„ˆë¬´ ëŠë¦¬ë„¤? ì–´ë–»ê²Œ í•˜ë©´ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆì„ê¹Œâ€¦ Conv filterë¥¼ ë” ë„“ê²Œ ì“´ë‹¤. â€“&gt; GPU ì—°ì‚°ì´ ì‰¬ì›Œì§„ë‹¤ 12345# ì´ë ‡ê²Œ ë˜ì–´ìˆëŠ” ê±¸conv = Conv2D(96, (3, 3), ...)(conv)conv = Conv2D(96, (3, 3), ...)(conv)# ì•„ë˜ì²˜ëŸ¼ ë°”ê¾¼ë‹¤.conv = Conv2D(128, (3, 3), ...)(conv) GPUëŠ” ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— í•„í„° ê°¯ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ë”ìš± ë¹¨ë¼ì§„ë‹¤. ì‰½ê²Œ ë§í•˜ë©´ 96ê°œì”© ë‘ë²ˆë³´ë‹¤ 128ê°œì”© í•œë²ˆì´ ë” ë¹ ë¥´ë‹¤. ì„¤ëª…. 96 // 3 = 32 2- layerì„ 1- layerë¡œ ë°”ê¿€ë• 2^2 * 32 = 128 ë˜ ë‹¤ë¥¸ ë°©ë²•ê° ì±„ë„ì—ì„œ ë³„ë„ì˜ 2d convë¥¼ í•˜ëŠ” ë°©ë²•in_channels * channel_multipliter ì¤‘ê°„ì±„ë„ì€ ì—°ê²°ë˜ê³  1x1 convë¡œ out_channelsì— ë§¤í•‘ 1234# Kerasfrom keras.layers import SeparableConv2Dnet = SeparableConv2D(32, (3, 3))(net)# it's almost 1:1 similar to the simple Keras Conv2D layer ì¶œì²˜ :source1source2","link":"/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"DL","slug":"DL","link":"/tags/DL/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"HEXO","slug":"HEXO","link":"/tags/HEXO/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"Tensorflow2","slug":"Tensorflow2","link":"/tags/Tensorflow2/"},{"name":"TF2","slug":"TF2","link":"/tags/TF2/"},{"name":"Music EDA","slug":"Music-EDA","link":"/tags/Music-EDA/"},{"name":"Script","slug":"Script","link":"/tags/Script/"},{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"coding","slug":"coding","link":"/tags/coding/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"GitPage","slug":"GitPage","link":"/categories/GitPage/"},{"name":"ilsang","slug":"ilsang","link":"/categories/ilsang/"},{"name":"Hexo","slug":"GitPage/Hexo","link":"/categories/GitPage/Hexo/"},{"name":"Music","slug":"ilsang/Music","link":"/categories/ilsang/Music/"},{"name":"Code","slug":"Code","link":"/categories/Code/"},{"name":"R","slug":"Code/R","link":"/categories/Code/R/"}]}