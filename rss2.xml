<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ikarus&#39;s BLOG</title>
    <link>https://ikarus-999.github.io/</link>
    
    <image>
      <url>https://www.gravatar.com/avatar/5a9f87e79678fa1d3ad8a772986a89a0</url>
      <title>ikarus&#39;s BLOG</title>
      <link>https://ikarus-999.github.io/</link>
    </image>
    
    <atom:link href="https://ikarus-999.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>데이터쟁이 이카루스 블로그입니다</description>
    <pubDate>Sun, 03 Jan 2021 07:56:26 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>소소한 연구아닌 연구</title>
      <link>https://ikarus-999.github.io/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/</link>
      <guid>https://ikarus-999.github.io/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/</guid>
      <pubDate>Sun, 03 Jan 2021 04:05:29 GMT</pubDate>
      
      <description>넘사벽 난이도인 트랜스포머 모델링
 시계열 데이터를 병렬 처리한다는 점에서는 가장 좋지만
그만큼 모델링 하기가 까다롭다.

 model input : (Batch, Sequence, feature_num)

 Transformer는 input / output을 잘못 설계하다간 sequence 정보가 날아갈 수도 있고
그냥 데이터 자체가 8:45 가 될수가 있다.

 입력 데이터를 Seqneuce로 Packing 해주어야 학습이 가능한 데이터가 된다.

 Sequence Packing 방법은 파이썬 문법으로 비교적 쉽게 되지만…

1</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="넘사벽-난이도인-트랜스포머-모델링"><a href="#넘사벽-난이도인-트랜스포머-모델링" class="headerlink" title="넘사벽 난이도인 트랜스포머 모델링"></a>넘사벽 난이도인 트랜스포머 모델링</h2><p>  시계열 데이터를 병렬 처리한다는 점에서는 가장 좋지만<br>  그만큼 모델링 하기가 까다롭다.</p><p>  model input : (Batch, Sequence, feature_num)</p><p>  Transformer는 input / output을 잘못 설계하다간 sequence 정보가 날아갈 수도 있고<br>  그냥 데이터 자체가 8:45 가 될수가 있다.</p><p>  입력 데이터를 Seqneuce로 Packing 해주어야 학습이 가능한 데이터가 된다.</p><p>  Sequence Packing 방법은 파이썬 문법으로 비교적 쉽게 되지만…</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df_train = train.values</span><br><span class="line">train_seq = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(seq_len, <span class="built_in">len</span>(df_train)+<span class="number">1</span>):</span><br><span class="line">  train_seq.append(df_train[i - seq_len: i])</span><br><span class="line">train_seq = np.array(train_seq)</span><br></pre></td></tr></table></figure><p>  Sequence unpacking 은 numpy 연산을 써야 비교적 쉬움..<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = np.vstack(r <span class="keyword">for</span> r <span class="keyword">in</span> train_seq)</span><br><span class="line">out = out.unique(out, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>  <img src="/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/%ED%8F%AC%ED%8A%B8%ED%8F%B4%EB%A6%AC%EC%98%A41.PNG" alt="packing &amp; unpacking"></p><p>  <img src="/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/%ED%8F%AC%ED%8A%B8%ED%8F%B4%EB%A6%AC%EC%98%A42.PNG" alt="unpacking2"></p><h3 id="다변량-회귀에서-얻은-교훈-1"><a href="#다변량-회귀에서-얻은-교훈-1" class="headerlink" title="다변량 회귀에서 얻은 교훈 1"></a>다변량 회귀에서 얻은 교훈 1</h3><p>  입력 데이터를 받아서 Time Embedding…</p><p>  시간별 Positional Encoding 방법을 소개하지.</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Time2Vector</span>(<span class="params">Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, seq_len, **kwargs</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(Time2Vector, self).__init__()</span><br><span class="line">      self.seq_len = seq_len</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;shape (batch, seq_len) 형태로 가중치와 Bias 초기화 &#x27;&#x27;&#x27;</span></span><br><span class="line">      self.weights_linear = self.add_weight(name=<span class="string">&#x27;weight_linear&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.bias_linear = self.add_weight(name=<span class="string">&#x27;bias_linear&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.weights_periodic = self.add_weight(name=<span class="string">&#x27;weight_periodic&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.bias_periodic = self.add_weight(name=<span class="string">&#x27;bias_periodic&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(self.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;주기성, 선형 시간별 특징을 계산&#x27;&#x27;&#x27;</span></span><br><span class="line">      x = tf.math.reduce_mean(x[:,:,:], axis=<span class="number">-1</span>) <span class="comment"># 입력 Feature 차원 슬라이싱</span></span><br><span class="line">      time_linear = self.weights_linear * x + self.bias_linear <span class="comment"># 선형 시간 특징</span></span><br><span class="line">      time_linear = tf.expand_dims(time_linear, axis=<span class="number">-1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line"></span><br><span class="line">      time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)</span><br><span class="line">      time_periodic = tf.expand_dims(time_periodic, axis=<span class="number">-1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line">      <span class="keyword">return</span> tf.concat([time_linear, time_periodic], axis=<span class="number">-1</span>) <span class="comment"># shape = (batch, seq_len, 2)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="트랜스포머의-특징"><a href="#트랜스포머의-특징" class="headerlink" title="트랜스포머의 특징"></a>트랜스포머의 특징</h3><p>  시계열 데이터 분류, 회귀 문제에서는<br>  Decoder가 빠져있는 Self Attention을 사용한다.  (Fine Tunning)</p><p>  Decoder는 챗봇이나 Auto-Encoder같은 Encoder-Decoder 구조에 사용되는 경향이 있다.</p><p>  참고 논문 : <a href="https://openreview.net/forum?id=lE1AB4stmX">https://openreview.net/forum?id=lE1AB4stmX</a></p><p>  논문에서는 n_layer_num, padding_mask 를 쓰는 특징이 있다. 이 파라미터까지 쓸 수 있으면 좋겠지만<br>  GPU 메모리가 8기가밖에 안되서 논문 파라미터보다는 작게 설정을 해야 원활이 가능하다.<br>  그리고 dataset을 window 하면 GPU RAM 사용량을 더 낮출 수 있다.</p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/Try/">Try</category>
      
      <category domain="https://ikarus-999.github.io/tags/Dev/">Dev</category>
      
      
      <comments>https://ikarus-999.github.io/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>치열한-데이콘후기</title>
      <link>https://ikarus-999.github.io/2021/01/02/%EC%B9%98%EC%97%B4%ED%95%9C-%EB%8D%B0%EC%9D%B4%EC%BD%98%ED%9B%84%EA%B8%B0/</link>
      <guid>https://ikarus-999.github.io/2021/01/02/%EC%B9%98%EC%97%B4%ED%95%9C-%EB%8D%B0%EC%9D%B4%EC%BD%98%ED%9B%84%EA%B8%B0/</guid>
      <pubDate>Sat, 02 Jan 2021 12:56:29 GMT</pubDate>
      
      <description>데이콘 최근 대회 후기 - 데이콘 어나더 버전
 흥미롭지만 quantile loss… 정말 만만치 않다.
loss function 수식구현은 쉽지만 loss를 줄이는 방법이 관건이다.
진짜 4.1에서 줄어들지 않는 loss… 과연 어떻게 하면 줄일수 있을까…

다변량 회귀 왜 이렇게 어려울까요? - 데이콘 대회에서 얻은 엄청난 교훈
 loss 계산할때 다른 target 값과 x_train값이 아닌 

 다른 Element 끼리 계산되어서 오히려 loss가 폭증하기도 한다.

 Wandb로 시각화 안했으면 큰일났었을듯 싶었다. 

 </description>
      
      
      
      <content:encoded><![CDATA[<h2 id="데이콘-최근-대회-후기-데이콘-어나더-버전"><a href="#데이콘-최근-대회-후기-데이콘-어나더-버전" class="headerlink" title="데이콘 최근 대회 후기 - 데이콘 어나더 버전"></a>데이콘 최근 대회 후기 - 데이콘 어나더 버전</h2><p>  흥미롭지만 quantile loss… 정말 만만치 않다.<br>  loss function 수식구현은 쉽지만 loss를 줄이는 방법이 관건이다.<br>  진짜 4.1에서 줄어들지 않는 loss… 과연 어떻게 하면 줄일수 있을까…</p><h3 id="다변량-회귀-왜-이렇게-어려울까요-데이콘-대회에서-얻은-엄청난-교훈"><a href="#다변량-회귀-왜-이렇게-어려울까요-데이콘-대회에서-얻은-엄청난-교훈" class="headerlink" title="다변량 회귀 왜 이렇게 어려울까요? - 데이콘 대회에서 얻은 엄청난 교훈"></a>다변량 회귀 왜 이렇게 어려울까요? - 데이콘 대회에서 얻은 엄청난 교훈</h3><p>  loss 계산할때 다른 target 값과 x_train값이 아닌   </p><p>  다른 Element 끼리 계산되어서 오히려 loss가 폭증하기도 한다.</p><p>  Wandb로 시각화 안했으면 큰일났었을듯 싶었다. </p><p>  Loss 시각화 툴로 보니 초반에서 수렴하지 않는 문제나 아예 u자로 불규칙적으로 튀는 현상도 발생….</p><p>  <img src="/2021/01/02/%EC%B9%98%EC%97%B4%ED%95%9C-%EB%8D%B0%EC%9D%B4%EC%BD%98%ED%9B%84%EA%B8%B0/%EC%8B%A0%EA%B8%B0%ED%95%9C%EA%B7%B8%EB%9E%98%ED%94%84.PNG" alt="loss가 줄어드는 것처럼 보이지만 실제는 ..."></p><p>  분명 window dataset으로 부하분산을 하면 데이터 처리가 수월할 것 같지만…</p><p>  채점을 해보니 극악의 Loss가 나오기도 한다.</p><p>  이 떄에는 window_size, buffer size를 잘 조절해야 한다.</p><p>  window_size를 너무 낮추면 자칫하면 GPU 사용률은 한자리에 머물고 시간은 오래 걸린다. </p><p>  하지만 GPU RAM 사용량은 줄어드는 이점은 있다.</p><p>  window dataset을 하다보면 배치 크기가 안 맞아서 예측이 안될때가 있는데 이럴땐…<br>  <img src="/2021/01/02/%EC%B9%98%EC%97%B4%ED%95%9C-%EB%8D%B0%EC%9D%B4%EC%BD%98%ED%9B%84%EA%B8%B0/%ED%9D%90%EC%9D%B5.PNG" alt="배치 크기가 안맞을 떄는 이렇게 padding을 쓴다."></p><p>  Shuffle Buffer size 조절은 필수, batch Size에 주의하여 조절한다.<br>  그렇지 않으면(BATCH SIZE가 굉장히 높다면)<br>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f2570221d30&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes&#x3D;True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https:&#x2F;&#x2F;www.tensorflow.org&#x2F;tutorials&#x2F;customization&#x2F;performance#python_or_tensor_args and https:&#x2F;&#x2F;www.tensorflow.org&#x2F;api_docs&#x2F;python&#x2F;tf&#x2F;function for  more details. </span><br></pre></td></tr></table></figure></p><p>  이런 경고가 나온다.. -&gt; batch size를 반드시 조절하거나 batch padding…</p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/Kaggle-Dacon/">Kaggle, Dacon</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/Dacon/">Dacon</category>
      
      <category domain="https://ikarus-999.github.io/tags/Try/">Try</category>
      
      
      <comments>https://ikarus-999.github.io/2021/01/02/%EC%B9%98%EC%97%B4%ED%95%9C-%EB%8D%B0%EC%9D%B4%EC%BD%98%ED%9B%84%EA%B8%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>어나더Dacon도전기_2</title>
      <link>https://ikarus-999.github.io/2020/12/23/%EC%96%B4%EB%82%98%EB%8D%94Dacon%EB%8F%84%EC%A0%84%EA%B8%B0-2/</link>
      <guid>https://ikarus-999.github.io/2020/12/23/%EC%96%B4%EB%82%98%EB%8D%94Dacon%EB%8F%84%EC%A0%84%EA%B8%B0-2/</guid>
      <pubDate>Tue, 22 Dec 2020 15:13:20 GMT</pubDate>
      
      <description>데이콘 또다른 대회 후기 - 데이콘 어나더 버전🌠🌠
 역시 흥미로운 데이터 세계 🐱
Regression, 특히 CNN, LSTM 모델은 Epoch를 늘리면 더욱 정확도가 향상되는 특징이 있다!! 

 너무 많이 늘리면 Overfitting이 날 수 있으니 LR 스케줄러 등 다양한 방법을 시도해봐야 한다.

CNN-LSTM 다변량 회귀 모델링 후기🌠
 처음에는 왜 이렇게 loss가 줄지 않는지 🎚 의문이 들었다.🌠

 
헐… 4.5 라니;;;🔌 20Epoch 가지고는 아예 학습이 안되나보다.

 그러면 Epoch🕜를 늘</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="데이콘-또다른-대회-후기-데이콘-어나더-버전🌠🌠"><a href="#데이콘-또다른-대회-후기-데이콘-어나더-버전🌠🌠" class="headerlink" title="데이콘 또다른 대회 후기 - 데이콘 어나더 버전🌠🌠"></a>데이콘 또다른 대회 후기 - 데이콘 어나더 버전🌠🌠</h2><p>  역시 흥미로운 데이터 세계 🐱<br>  Regression, 특히 CNN, LSTM 모델은 Epoch를 늘리면 더욱 정확도가 향상되는 특징이 있다!!  </p><p>  너무 많이 늘리면 Overfitting이 날 수 있으니 LR 스케줄러 등 다양한 방법을 시도해봐야 한다.</p><h3 id="CNN-LSTM-다변량-회귀-모델링-후기🌠"><a href="#CNN-LSTM-다변량-회귀-모델링-후기🌠" class="headerlink" title="CNN-LSTM 다변량 회귀 모델링 후기🌠"></a>CNN-LSTM 다변량 회귀 모델링 후기🌠</h3><p>  처음에는 왜 이렇게 loss가 줄지 않는지 🎚 의문이 들었다.🌠</p><p>  <img src="/2020/12/23/%EC%96%B4%EB%82%98%EB%8D%94Dacon%EB%8F%84%EC%A0%84%EA%B8%B0-2/Multi_var_reg.PNG" alt="loss가 전혀 줄지 않음"><br>  헐… 4.5 라니;;;🔌 20Epoch 가지고는 아예 학습이 안되나보다.</p><p>  그러면 Epoch🕜를 늘리는데 그냥 늘리면 안된다💽</p><p>  자칫 잘못하면 오버피팅이 발생할 수 있기 때문이다.🕜</p><p>  그리고 시계열 문제는 시간, 구간당 평균이나 여러 통계적 변수를 만들어야 점수가 더 잘나오는 특징이 있다.⏩</p><p>  <a href="https://github.com/sachinruk/KerasQuantileModel/blob/master/Keras%20Quantile%20Model.ipynb">참고문서1</a></p><p>  역시나 예측이 맞았다. Epoch을 약간(?) 겁나게 늘리고 optimizer나 다른 학습율 스케줄러, sgd에 decay, momentum 을 주어야 하나보다.</p><p>  그리고 다변량 회귀이기 때문에 validation loss 계산이 약간 신중해야 정확한 loss값이 계산이 가능하다.</p><p>  test dataset에 element-wise 방식으로 계산되는거라 test dataset shape 그대로 predict가 출력된다.</p><p>  Reshape, Squeeze 를 써서 shape을 맞춰주어야 한다.</p><p>  Quantile Regression 문제는 그냥 Regression에 Quantile별로 값을 추출해야 하는 약간의(?) 난제가 존재한다.</p><p>  Pytorch는 Pythonic🐍 하지만 어떤 면에서는 TF2.x 보다 더 까다로운 것 같다.</p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/Kaggle-Dacon/">Kaggle, Dacon</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/Dacon/">Dacon</category>
      
      <category domain="https://ikarus-999.github.io/tags/Try/">Try</category>
      
      
      <comments>https://ikarus-999.github.io/2020/12/23/%EC%96%B4%EB%82%98%EB%8D%94Dacon%EB%8F%84%EC%A0%84%EA%B8%B0-2/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>인공지능 경진대회 후기</title>
      <link>https://ikarus-999.github.io/2020/12/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0/</link>
      <guid>https://ikarus-999.github.io/2020/12/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0/</guid>
      <pubDate>Tue, 08 Dec 2020 15:16:14 GMT</pubDate>
      
      <description>인공지능 문제해결 경진대회 참가후기 🔔
 정말 치열했던 경진대회였다. 

 처음에는 이미지 멀티라벨 다중 분류 문제가 나왔었고 

 그 이후 본선대회에서는 NLP, OCR, GAN 등 여러 도메인 문제가 출현했었다 

 베이스라인 코드 그런건 사실상 없ㅋ음ㅋ..🔐

 그냥 데이터를 말 그대로 해체 분석을 했어야 했다.

 아무리 Data Clensing을 해도 그대로인 loss와 score… 8:45🕛

 결국 아예 리모델링, 데이터 Cleanize 한 뒤에 학습을 다시 돌렸다.

 천신만고 끝에…

 

 

 
참고로 팀명</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="인공지능-문제해결-경진대회-참가후기-🔔"><a href="#인공지능-문제해결-경진대회-참가후기-🔔" class="headerlink" title="인공지능 문제해결 경진대회 참가후기 🔔"></a>인공지능 문제해결 경진대회 참가후기 🔔</h2><p>  정말 치열했던 경진대회였다. </p><p>  처음에는 이미지 멀티라벨 다중 분류 문제가 나왔었고  </p><p>  그 이후 본선대회에서는 NLP, OCR, GAN 등 여러 도메인 문제가 출현했었다  </p><p>  베이스라인 코드 그런건 사실상 없ㅋ음ㅋ..🔐</p><p>  그냥 데이터를 말 그대로 해체 분석을 했어야 했다.</p><p>  아무리 Data Clensing을 해도 그대로인 loss와 score… 8:45🕛</p><p>  결국 아예 리모델링, 데이터 Cleanize 한 뒤에 학습을 다시 돌렸다.</p><p>  천신만고 끝에…</p><p>  <img src="/2020/12/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0/2222.PNG" alt="최종순위...ㄷㄷㄷ"></p><p>  <img src="/2020/12/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0/3333.PNG" alt="최종순위 ㅎㄷㄷ"></p><p>  <img src="/2020/12/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0/4444.PNG" alt="최종순위 ㄷㄷㄷ;;"><br>    참고로 팀명은 <del>풍선띄우기</del> ;;</p><p>  최종 명단에 발표되기까지 엄청난 긴장감이…🌡📈</p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/Competition/">Competition</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/Try/">Try</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      <category domain="https://ikarus-999.github.io/tags/competition/">competition</category>
      
      
      <comments>https://ikarus-999.github.io/2020/12/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Dacon도전기</title>
      <link>https://ikarus-999.github.io/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/</link>
      <guid>https://ikarus-999.github.io/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/</guid>
      <pubDate>Sun, 06 Dec 2020 12:48:20 GMT</pubDate>
      
      <description>데이콘 도전 후기
 흥미롭지만 쉽지는 않다.
말 그대로 새로운 파생변수, 모델을 많이 만들면 점수가 오르긴 한다.

 데이콘_연습문제

 연습용이긴 한데 BERT모델 폭격으로 양민학살 이 벌써부터 시작되었…ㅎㄷㄷ

 버트모델 안쓰고도 순위 올릴수는 있다. 

 지금부터 그 방법을 소개합니다.

데이콘 순위를 올릴수 있는 방법 - 창의적인 modeling, K-cross Validation, parameter searching
 

 VDCNN은 GPU가 잘 버텨주면
0.86까지는 마구마구 올릴수 있음. 

 이것이 바로 VDCNN이</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="데이콘-도전-후기"><a href="#데이콘-도전-후기" class="headerlink" title="데이콘 도전 후기"></a>데이콘 도전 후기</h2><p>  흥미롭지만 쉽지는 않다.<br>  말 그대로 새로운 파생변수, 모델을 많이 만들면 점수가 오르긴 한다.</p><p>  <a href="https://dacon.io/competitions/open/235597/leaderboard/">데이콘_연습문제</a></p><p>  연습용이긴 한데 BERT모델 폭격으로 <del>양민학살</del> 이 벌써부터 시작되었…ㅎㄷㄷ</p><p>  버트모델 안쓰고도 순위 올릴수는 있다. </p><p>  지금부터 그 방법을 소개합니다.</p><h3 id="데이콘-순위를-올릴수-있는-방법-창의적인-modeling-K-cross-Validation-parameter-searching"><a href="#데이콘-순위를-올릴수-있는-방법-창의적인-modeling-K-cross-Validation-parameter-searching" class="headerlink" title="데이콘 순위를 올릴수 있는 방법 - 창의적인 modeling, K-cross Validation, parameter searching"></a>데이콘 순위를 올릴수 있는 방법 - 창의적인 modeling, K-cross Validation, parameter searching</h3><p>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/vdcnn_1.PNG" alt="이것이 바로 VDCNN">  </p><p>  VDCNN은 GPU가 잘 버텨주면<br>  0.86까지는 마구마구 올릴수 있음.  </p><p>  이것이 바로 VDCNN이당!<br>  <a href="https://github.com/ikarus-999/VDCNN_kor"><img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/vdcnn_5.PNG" alt="VDCNN code"></a></p><p>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/vdcnn_9.PNG" alt="VDCNN 학습장면"></p><p>  학습 모델을 그림으로 보기</p><p>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/output_36_0.png" alt="VDCNN 그림1"></p><p>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/Dacon01.PNG" alt="데이콘 결과"><br>  여기에 여러 가지 모델 Ensemble 해보면 0.88은 가능해보일 것 같네요.</p><blockquote><p>추가로 모델 Ensemble 을 시도<br>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/output_43_0.png" alt="추가했던 모델1"><br>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/output_50_0.png" alt="추가했던 모델2"><br>  <img src="/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/output_57_0.png" alt="추가했던 모델3"></p></blockquote><blockquote><p>추가적으로 트랜스포머 모델링..(2020-12-09 추가…)<br>  <a href="https://github.com/ikarus-999/VDCNN_kor/blob/master/TF2_kor_2.0.ipynb">Transformer</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/Kaggle-Dacon/">Kaggle, Dacon</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/Dacon/">Dacon</category>
      
      <category domain="https://ikarus-999.github.io/tags/Try/">Try</category>
      
      
      <comments>https://ikarus-999.github.io/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>kaggle_try</title>
      <link>https://ikarus-999.github.io/2020/12/06/kaggle-try/</link>
      <guid>https://ikarus-999.github.io/2020/12/06/kaggle-try/</guid>
      <pubDate>Sun, 06 Dec 2020 06:56:21 GMT</pubDate>
      
      <description>남들이 잘 안하는 데이터셋으로 캐글 도전 후기
 데이터셋 링크 :
https://www.kaggle.com/c/LANL-Earthquake-Prediction

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16


from tqdm import tqdm
import pandas as pd
import numpy as np

import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras import metrics
from tens</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="남들이-잘-안하는-데이터셋으로-캐글-도전-후기"><a href="#남들이-잘-안하는-데이터셋으로-캐글-도전-후기" class="headerlink" title="남들이 잘 안하는 데이터셋으로 캐글 도전 후기"></a>남들이 잘 안하는 데이터셋으로 캐글 도전 후기</h2><p>  데이터셋 링크 :<br>  <a href="https://www.kaggle.com/c/LANL-Earthquake-Prediction">https://www.kaggle.com/c/LANL-Earthquake-Prediction</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.initializers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> random, sys</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pd.set_option(<span class="string">&#x27;precision&#x27;</span>, <span class="number">30</span>)</span><br><span class="line">np.set_printoptions(precision = <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">47</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">train_df = pd.read_csv(<span class="string">&#x27;./train.csv&#x27;</span>, dtype=&#123;<span class="string">&#x27;acoustic_data&#x27;</span>: np.int8, <span class="string">&#x27;time_to_failure&#x27;</span>: np.float32&#125;)</span><br></pre></td></tr></table></figure><pre><code>CPU times: user 1min 30s, sys: 19.1 s, total: 1min 49sWall time: 1min 49s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train = train_df[<span class="string">&#x27;acoustic_data&#x27;</span>].values</span><br><span class="line">y_train = train_df[<span class="string">&#x27;time_to_failure&#x27;</span>].values</span><br></pre></td></tr></table></figure><h3 id="Training-data에서-완전한-데이터-구간-찾기-0에-근접한-failure-time-찾기"><a href="#Training-data에서-완전한-데이터-구간-찾기-0에-근접한-failure-time-찾기" class="headerlink" title="Training data에서 완전한 데이터 구간 찾기(0에 근접한  failure time 찾기)"></a>Training data에서 완전한 데이터 구간 찾기(0에 근접한  failure time 찾기)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ends_mask = np.less(y_train[:<span class="number">-1</span>], y_train[<span class="number">1</span>:])</span><br><span class="line">segment_ends = np.nonzero(ends_mask)</span><br><span class="line"></span><br><span class="line">train_segments = []</span><br><span class="line">start = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> end <span class="keyword">in</span> segment_ends[<span class="number">0</span>]:</span><br><span class="line">    train_segments.append((start, end))</span><br><span class="line">    start = end</span><br><span class="line">    </span><br><span class="line">print(train_segments)</span><br></pre></td></tr></table></figure><pre><code>[(0, 5656573), (5656573, 50085877), (50085877, 104677355), (104677355, 138772452), (138772452, 187641819), (187641819, 218652629), (218652629, 245829584), (245829584, 307838916), (307838916, 338276286), (338276286, 375377847), (375377847, 419368879), (419368879, 461811622), (461811622, 495800224), (495800224, 528777114), (528777114, 585568143), (585568143, 621985672)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.set_title(<span class="string">&#x27;segment size&#x27;</span>)</span><br><span class="line">ax.bar(np.arange(<span class="built_in">len</span>(train_segments)), [ s[<span class="number">1</span>] - s[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> train_segments])</span><br></pre></td></tr></table></figure><pre><code>&lt;BarContainer object of 16 artists&gt;</code></pre><p><img src="/2020/12/06/kaggle-try/output_6_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EarthQuakeRandom</span>(<span class="params">tf.keras.utils.Sequence</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x, y, x_mean, x_std, segments, ts_length, batch_size, steps_per_epoch</span>):</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        self.segments = segments</span><br><span class="line">        self.ts_length = ts_length</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.steps_per_epoch = steps_per_epoch</span><br><span class="line">        self.segments_size = np.array([s[<span class="number">1</span>] - s[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> segments])</span><br><span class="line">        self.segments_p = self.segments_size / self.segments_size.<span class="built_in">sum</span>()</span><br><span class="line">        self.x_mean = x_mean</span><br><span class="line">        self.x_std = x_std</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.batch_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_ts_length</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.ts_length</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_segments</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.segments</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_segments_p</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.segments_p</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_segments_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.segments_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.steps_per_epoch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        segment_index = np.random.choice(<span class="built_in">range</span>(<span class="built_in">len</span>(self.segments)), p=self.segments_p)</span><br><span class="line">        segment = self.segments[segment_index]</span><br><span class="line">        end_indexes = np.random.randint(segment[<span class="number">0</span>] + self.ts_length, segment[<span class="number">1</span>], size=self.batch_size)</span><br><span class="line"></span><br><span class="line">        x_batch = np.empty((self.batch_size, self.ts_length))</span><br><span class="line">        y_batch = np.empty(self.batch_size, )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, end <span class="keyword">in</span> <span class="built_in">enumerate</span>(end_indexes):</span><br><span class="line">            x_batch[i, :] = self.x[end - self.ts_length: end]</span><br><span class="line">            y_batch[i] = self.y[end - <span class="number">1</span>]</span><br><span class="line">            </span><br><span class="line">        <span class="comment">#x_batch = (x_batch - self.x_mean)/self.x_std</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.expand_dims(x_batch, axis=<span class="number">2</span>), y_batch</span><br></pre></td></tr></table></figure><h3 id="train-validation-나누기-segments"><a href="#train-validation-나누기-segments" class="headerlink" title="train / validation 나누기(segments)"></a>train / validation 나누기(segments)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t_segments = [train_segments[i] <span class="keyword">for</span> i <span class="keyword">in</span> [ <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]]</span><br><span class="line">v_segments = [train_segments[i] <span class="keyword">for</span> i <span class="keyword">in</span> [ <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br></pre></td></tr></table></figure><h3 id="training-data에-대해서만-평균과-표준편차-계산"><a href="#training-data에-대해서만-평균과-표준편차-계산" class="headerlink" title="training data에 대해서만 평균과 표준편차 계산"></a>training data에 대해서만 평균과 표준편차 계산</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x_sum = <span class="number">0.</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> t_segments:</span><br><span class="line">    x_sum += X_train[s[<span class="number">0</span>]:s[<span class="number">1</span>]].<span class="built_in">sum</span>()</span><br><span class="line">    count += (s[<span class="number">1</span>] - s[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">X_train_mean = x_sum/count</span><br><span class="line"></span><br><span class="line">x2_sum = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> t_segments:</span><br><span class="line">    x2_sum += np.power(X_train[s[<span class="number">0</span>]:s[<span class="number">1</span>]] - X_train_mean, <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">X_train_std =  np.sqrt(x2_sum/count)</span><br><span class="line"></span><br><span class="line">print(X_train_mean, X_train_std)</span><br></pre></td></tr></table></figure><pre><code>4.472289301190891 6.189013535612676</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_gen = EarthQuakeRandom(</span><br><span class="line">    x = X_train, </span><br><span class="line">    y = y_train,</span><br><span class="line">    x_mean = X_train_mean, </span><br><span class="line">    x_std = X_train_std,</span><br><span class="line">    segments = t_segments,</span><br><span class="line">    ts_length = <span class="number">150000</span>,</span><br><span class="line">    batch_size = <span class="number">64</span>,</span><br><span class="line">    steps_per_epoch = <span class="number">400</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">valid_gen = EarthQuakeRandom(</span><br><span class="line">    x = X_train, </span><br><span class="line">    y = y_train,</span><br><span class="line">    x_mean = X_train_mean, </span><br><span class="line">    x_std = X_train_std,</span><br><span class="line">    segments = v_segments,</span><br><span class="line">    ts_length = <span class="number">150000</span>,</span><br><span class="line">    batch_size = <span class="number">64</span>,</span><br><span class="line">    steps_per_epoch = <span class="number">400</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.kaggle.com/shujian/transformer-with-lstm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> dataloader <span class="keyword">import</span> TokenList, pad_to_longest</span><br><span class="line">    <span class="comment"># for transformer</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">embed_size = <span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNormalization</span>(<span class="params">Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, eps=<span class="number">1e-6</span>, **kwargs</span>):</span></span><br><span class="line">        self.eps = eps</span><br><span class="line">        <span class="built_in">super</span>(LayerNormalization, self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        self.gamma = self.add_weight(name=<span class="string">&#x27;gamma&#x27;</span>, shape=input_shape[<span class="number">-1</span>:],</span><br><span class="line">                                     initializer=Ones(), trainable=<span class="literal">True</span>)</span><br><span class="line">        self.beta = self.add_weight(name=<span class="string">&#x27;beta&#x27;</span>, shape=input_shape[<span class="number">-1</span>:],</span><br><span class="line">                                    initializer=Zeros(), trainable=<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">super</span>(LayerNormalization, self).build(input_shape)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = K.mean(x, axis=<span class="number">-1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        std = K.std(x, axis=<span class="number">-1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.gamma * (x - mean) / (std + self.eps) + self.beta</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        <span class="keyword">return</span> input_shape</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, attn_dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        self.temper = np.sqrt(d_model)</span><br><span class="line">        self.dropout = Dropout(attn_dropout)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, q, k, v, mask</span>):</span></span><br><span class="line">        attn = Lambda(<span class="keyword">lambda</span> x:K.batch_dot(x[<span class="number">0</span>],x[<span class="number">1</span>],axes=[<span class="number">2</span>,<span class="number">2</span>])/self.temper)([q, k])</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mmask = Lambda(<span class="keyword">lambda</span> x:(<span class="number">-1e+10</span>)*(<span class="number">1</span>-x))(mask)</span><br><span class="line">            attn = Add()([attn, mmask])</span><br><span class="line">        attn = Activation(<span class="string">&#x27;softmax&#x27;</span>)(attn)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line">        output = Lambda(<span class="keyword">lambda</span> x:K.batch_dot(x[<span class="number">0</span>], x[<span class="number">1</span>]))([attn, v])</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>():</span></span><br><span class="line">    <span class="comment"># mode 0 - big martixes, faster; mode 1 - more clear implementation</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_head, d_model, d_k, d_v, dropout, mode=<span class="number">0</span>, use_norm=<span class="literal">True</span></span>):</span></span><br><span class="line">        self.mode = mode</span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.d_k = d_k</span><br><span class="line">        self.d_v = d_v</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="number">0</span>:</span><br><span class="line">            self.qs_layer = Dense(n_head*d_k, use_bias=<span class="literal">False</span>)</span><br><span class="line">            self.ks_layer = Dense(n_head*d_k, use_bias=<span class="literal">False</span>)</span><br><span class="line">            self.vs_layer = Dense(n_head*d_v, use_bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="number">1</span>:</span><br><span class="line">            self.qs_layers = []</span><br><span class="line">            self.ks_layers = []</span><br><span class="line">            self.vs_layers = []</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_head):</span><br><span class="line">                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=<span class="literal">False</span>)))</span><br><span class="line">                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=<span class="literal">False</span>)))</span><br><span class="line">                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=<span class="literal">False</span>)))</span><br><span class="line">        self.attention = ScaledDotProductAttention(d_model)</span><br><span class="line">        self.layer_norm = LayerNormalization() <span class="keyword">if</span> use_norm <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.w_o = TimeDistributed(Dense(d_model))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        d_k, d_v = self.d_k, self.d_v</span><br><span class="line">        n_head = self.n_head</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.mode == <span class="number">0</span>:</span><br><span class="line">            qs = self.qs_layer(q)  <span class="comment"># [batch_size, len_q, n_head*d_k]</span></span><br><span class="line">            ks = self.ks_layer(k)</span><br><span class="line">            vs = self.vs_layer(v)</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">reshape1</span>(<span class="params">x</span>):</span></span><br><span class="line">                s = tf.shape(x)   <span class="comment"># [batch_size, len_q, n_head * d_k]</span></span><br><span class="line">                x = tf.reshape(x, [s[<span class="number">0</span>], s[<span class="number">1</span>], n_head, d_k])</span><br><span class="line">                x = tf.transpose(x, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>])  </span><br><span class="line">                x = tf.reshape(x, [<span class="number">-1</span>, s[<span class="number">1</span>], d_k])  <span class="comment"># [n_head * batch_size, len_q, d_k]</span></span><br><span class="line">                <span class="keyword">return</span> x</span><br><span class="line">            qs = Lambda(reshape1)(qs)</span><br><span class="line">            ks = Lambda(reshape1)(ks)</span><br><span class="line">            vs = Lambda(reshape1)(vs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                mask = Lambda(<span class="keyword">lambda</span> x:K.repeat_elements(x, n_head, <span class="number">0</span>))(mask)</span><br><span class="line">            head, attn = self.attention(qs, ks, vs, mask=mask)  </span><br><span class="line">                </span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">reshape2</span>(<span class="params">x</span>):</span></span><br><span class="line">                s = tf.shape(x)   <span class="comment"># [n_head * batch_size, len_v, d_v]</span></span><br><span class="line">                x = tf.reshape(x, [n_head, <span class="number">-1</span>, s[<span class="number">1</span>], s[<span class="number">2</span>]]) </span><br><span class="line">                x = tf.transpose(x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">                x = tf.reshape(x, [<span class="number">-1</span>, s[<span class="number">1</span>], n_head*d_v])  <span class="comment"># [batch_size, len_v, n_head * d_v]</span></span><br><span class="line">                <span class="keyword">return</span> x</span><br><span class="line">            head = Lambda(reshape2)(head)</span><br><span class="line">        <span class="keyword">elif</span> self.mode == <span class="number">1</span>:</span><br><span class="line">            heads = []; attns = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_head):</span><br><span class="line">                qs = self.qs_layers[i](q)   </span><br><span class="line">                ks = self.ks_layers[i](k) </span><br><span class="line">                vs = self.vs_layers[i](v) </span><br><span class="line">                head, attn = self.attention(qs, ks, vs, mask)</span><br><span class="line">                heads.append(head); attns.append(attn)</span><br><span class="line">            head = Concatenate()(heads) <span class="keyword">if</span> n_head &gt; <span class="number">1</span> <span class="keyword">else</span> heads[<span class="number">0</span>]</span><br><span class="line">            attn = Concatenate()(attns) <span class="keyword">if</span> n_head &gt; <span class="number">1</span> <span class="keyword">else</span> attns[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        outputs = self.w_o(head)</span><br><span class="line">        outputs = Dropout(self.dropout)(outputs)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.layer_norm: <span class="keyword">return</span> outputs, attn</span><br><span class="line">        <span class="comment"># outputs = Add()([outputs, q]) # sl: fix</span></span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(outputs), attn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_hid, d_inner_hid, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        self.w_1 = Conv1D(d_inner_hid, <span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.w_2 = Conv1D(d_hid, <span class="number">1</span>)</span><br><span class="line">        self.layer_norm = LayerNormalization()</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        output = self.w_1(x) </span><br><span class="line">        output = self.w_2(output)</span><br><span class="line">        output = self.dropout(output)</span><br><span class="line">        output = Add()([output, x])</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(output)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, enc_input, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)</span><br><span class="line">        output = self.pos_ffn_layer(output)</span><br><span class="line">        <span class="keyword">return</span> output, slf_attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetPosEncodingMatrix</span>(<span class="params">max_len, d_emb</span>):</span></span><br><span class="line">    pos_enc = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * (j // <span class="number">2</span>) / d_emb) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(d_emb)] </span><br><span class="line">        <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(d_emb) </span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_len)</span><br><span class="line">            ])</span><br><span class="line">    pos_enc[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(pos_enc[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>]) <span class="comment"># dim 2i</span></span><br><span class="line">    pos_enc[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(pos_enc[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>]) <span class="comment"># dim 2i+1</span></span><br><span class="line">    <span class="keyword">return</span> pos_enc</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetPadMask</span>(<span class="params">q, k</span>):</span></span><br><span class="line">    ones = K.expand_dims(K.ones_like(q, <span class="string">&#x27;float32&#x27;</span>), <span class="number">-1</span>)</span><br><span class="line">    mask = K.cast(K.expand_dims(K.not_equal(k, <span class="number">0</span>), <span class="number">1</span>), <span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    mask = K.batch_dot(ones, mask, axes=[<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetSubMask</span>(<span class="params">s</span>):</span></span><br><span class="line">    len_s = tf.shape(s)[<span class="number">1</span>]</span><br><span class="line">    bs = tf.shape(s)[:<span class="number">1</span>]</span><br><span class="line">    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><pre><code>No module named &#39;dataloader&#39;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CnnTransformerModel</span>():</span></span><br><span class="line">    i = Input(shape = (<span class="number">150000</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    x = BatchNormalization(axis=<span class="number">-1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>, beta_initializer=<span class="string">&#x27;zeros&#x27;</span>, gamma_initializer=<span class="string">&#x27;ones&#x27;</span>, moving_mean_initializer=<span class="string">&#x27;zeros&#x27;</span>, moving_variance_initializer=<span class="string">&#x27;ones&#x27;</span>, beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>, gamma_constraint=<span class="literal">None</span>)(i)</span><br><span class="line"></span><br><span class="line">    x = Convolution1D( <span class="number">8</span>, kernel_size = <span class="number">10</span>, strides = <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    </span><br><span class="line">    x = BatchNormalization(axis=<span class="number">-1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>, beta_initializer=<span class="string">&#x27;zeros&#x27;</span>, gamma_initializer=<span class="string">&#x27;ones&#x27;</span>, moving_mean_initializer=<span class="string">&#x27;zeros&#x27;</span>, moving_variance_initializer=<span class="string">&#x27;ones&#x27;</span>, beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>, gamma_constraint=<span class="literal">None</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Convolution1D(<span class="number">16</span>, kernel_size = <span class="number">10</span>, strides = <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    </span><br><span class="line">    x = BatchNormalization(axis=<span class="number">-1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>, beta_initializer=<span class="string">&#x27;zeros&#x27;</span>, gamma_initializer=<span class="string">&#x27;ones&#x27;</span>, moving_mean_initializer=<span class="string">&#x27;zeros&#x27;</span>, moving_variance_initializer=<span class="string">&#x27;ones&#x27;</span>, beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>, gamma_constraint=<span class="literal">None</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Convolution1D(<span class="number">32</span>, kernel_size = <span class="number">10</span>, strides = <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    </span><br><span class="line">    x = BatchNormalization(axis=<span class="number">-1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>, beta_initializer=<span class="string">&#x27;zeros&#x27;</span>, gamma_initializer=<span class="string">&#x27;ones&#x27;</span>, moving_mean_initializer=<span class="string">&#x27;zeros&#x27;</span>, moving_variance_initializer=<span class="string">&#x27;ones&#x27;</span>, beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>, gamma_constraint=<span class="literal">None</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Convolution1D(<span class="number">64</span>, kernel_size = <span class="number">10</span>, strides = <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line"></span><br><span class="line">    x = Bidirectional(LSTM(<span class="number">128</span>, return_sequences = <span class="literal">True</span>, return_state = <span class="literal">False</span>))(x)</span><br><span class="line">    </span><br><span class="line">    x = Bidirectional(LSTM(<span class="number">64</span>, return_sequences = <span class="literal">True</span>, return_state = <span class="literal">False</span>))(x)</span><br><span class="line">    </span><br><span class="line">    x, slf_attn = MultiHeadAttention(n_head=<span class="number">5</span>, d_model=<span class="number">300</span>, d_k=<span class="number">64</span>, d_v=<span class="number">64</span>, dropout=<span class="number">0.3</span>)(x, x, x)</span><br><span class="line">    </span><br><span class="line">    avg_pool = GlobalAveragePooling1D()(x)</span><br><span class="line">    </span><br><span class="line">    avg_pool = Dense(<span class="number">60</span>,activation = <span class="string">&#x27;relu&#x27;</span>)(avg_pool)</span><br><span class="line">    </span><br><span class="line">    y = Dense(<span class="number">1</span>,activation = <span class="string">&#x27;relu&#x27;</span>)(avg_pool)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Model(inputs = [i], outputs = [y])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = CnnTransformerModel()</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_error&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>,metrics = [<span class="string">&#x27;mean_absolute_error&#x27;</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;__________________________________________________________________________________________________Layer (type)                    Output Shape         Param #     Connected to                     ==================================================================================================input_1 (InputLayer)            [(None, 150000, 1)]  0                                            __________________________________________________________________________________________________batch_normalization (BatchNorma (None, 150000, 1)    4           input_1[0][0]                    __________________________________________________________________________________________________conv1d (Conv1D)                 (None, 15000, 8)     88          batch_normalization[0][0]        __________________________________________________________________________________________________batch_normalization_1 (BatchNor (None, 15000, 8)     32          conv1d[0][0]                     __________________________________________________________________________________________________conv1d_1 (Conv1D)               (None, 1500, 16)     1296        batch_normalization_1[0][0]      __________________________________________________________________________________________________batch_normalization_2 (BatchNor (None, 1500, 16)     64          conv1d_1[0][0]                   __________________________________________________________________________________________________conv1d_2 (Conv1D)               (None, 150, 32)      5152        batch_normalization_2[0][0]      __________________________________________________________________________________________________batch_normalization_3 (BatchNor (None, 150, 32)      128         conv1d_2[0][0]                   __________________________________________________________________________________________________conv1d_3 (Conv1D)               (None, 15, 64)       20544       batch_normalization_3[0][0]      __________________________________________________________________________________________________bidirectional (Bidirectional)   (None, 15, 256)      197632      conv1d_3[0][0]                   __________________________________________________________________________________________________bidirectional_1 (Bidirectional) (None, 15, 128)      164352      bidirectional[0][0]              __________________________________________________________________________________________________dense (Dense)                   (None, 15, 320)      40960       bidirectional_1[0][0]            __________________________________________________________________________________________________dense_1 (Dense)                 (None, 15, 320)      40960       bidirectional_1[0][0]            __________________________________________________________________________________________________lambda (Lambda)                 (None, None, 64)     0           dense[0][0]                      __________________________________________________________________________________________________lambda_1 (Lambda)               (None, None, 64)     0           dense_1[0][0]                    __________________________________________________________________________________________________lambda_3 (Lambda)               (None, None, None)   0           lambda[0][0]                                                                                      lambda_1[0][0]                   __________________________________________________________________________________________________activation (Activation)         (None, None, None)   0           lambda_3[0][0]                   __________________________________________________________________________________________________dense_2 (Dense)                 (None, 15, 320)      40960       bidirectional_1[0][0]            __________________________________________________________________________________________________dropout (Dropout)               (None, None, None)   0           activation[0][0]                 __________________________________________________________________________________________________lambda_2 (Lambda)               (None, None, 64)     0           dense_2[0][0]                    __________________________________________________________________________________________________lambda_4 (Lambda)               (None, None, 64)     0           dropout[0][0]                                                                                     lambda_2[0][0]                   __________________________________________________________________________________________________lambda_5 (Lambda)               (None, None, 320)    0           lambda_4[0][0]                   __________________________________________________________________________________________________time_distributed (TimeDistribut (None, None, 300)    96300       lambda_5[0][0]                   __________________________________________________________________________________________________dropout_1 (Dropout)             (None, None, 300)    0           time_distributed[0][0]           __________________________________________________________________________________________________layer_normalization (LayerNorma (None, None, 300)    600         dropout_1[0][0]                  __________________________________________________________________________________________________global_average_pooling1d (Globa (None, 300)          0           layer_normalization[0][0]        __________________________________________________________________________________________________dense_4 (Dense)                 (None, 60)           18060       global_average_pooling1d[0][0]   __________________________________________________________________________________________________dense_5 (Dense)                 (None, 1)            61          dense_4[0][0]                    ==================================================================================================Total params: 627,193Trainable params: 627,079Non-trainable params: 114__________________________________________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from IPython.display import SVG</span></span><br><span class="line"><span class="comment"># from keras.utils.vis_utils import model_to_dot</span></span><br><span class="line"><span class="comment"># SVG(model_to_dot(model,show_shapes = True).create(prog=&#x27;dot&#x27;, format=&#x27;svg&#x27;))</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start_time = time.time()</span><br><span class="line">hist = model.fit(</span><br><span class="line">    train_gen,</span><br><span class="line">    epochs = <span class="number">25</span>, </span><br><span class="line">    verbose = <span class="number">1</span>,validation_data= valid_gen</span><br><span class="line">)</span><br><span class="line">print(<span class="string">&quot;--- %s seconds ---&quot;</span> % (time.time() - start_time))</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/25400/400 [==============================] - 92s 230ms/step - loss: 8.1259 - mean_absolute_error: 2.1690 - val_loss: 8.6962 - val_mean_absolute_error: 2.2990Epoch 2/25400/400 [==============================] - 93s 232ms/step - loss: 6.8734 - mean_absolute_error: 1.9745 - val_loss: 10.5941 - val_mean_absolute_error: 2.5199Epoch 3/25400/400 [==============================] - 92s 231ms/step - loss: 6.2273 - mean_absolute_error: 1.8582 - val_loss: 8.3263 - val_mean_absolute_error: 2.1892Epoch 4/25400/400 [==============================] - 93s 232ms/step - loss: 6.3129 - mean_absolute_error: 1.8750 - val_loss: 7.2582 - val_mean_absolute_error: 2.1842Epoch 5/25400/400 [==============================] - 93s 232ms/step - loss: 6.1699 - mean_absolute_error: 1.8555 - val_loss: 10.6914 - val_mean_absolute_error: 2.5315Epoch 6/25400/400 [==============================] - 93s 232ms/step - loss: 6.3945 - mean_absolute_error: 1.8601 - val_loss: 17.6278 - val_mean_absolute_error: 3.4488Epoch 7/25400/400 [==============================] - 91s 228ms/step - loss: 5.9358 - mean_absolute_error: 1.7905 - val_loss: 7.1590 - val_mean_absolute_error: 2.0332Epoch 8/25400/400 [==============================] - 91s 227ms/step - loss: 5.8317 - mean_absolute_error: 1.7861 - val_loss: 7.4188 - val_mean_absolute_error: 2.0401Epoch 9/25400/400 [==============================] - 90s 226ms/step - loss: 6.0664 - mean_absolute_error: 1.8154 - val_loss: 11.4744 - val_mean_absolute_error: 2.6900Epoch 10/25400/400 [==============================] - 91s 228ms/step - loss: 5.6385 - mean_absolute_error: 1.7225 - val_loss: 6.3991 - val_mean_absolute_error: 1.9165Epoch 11/25400/400 [==============================] - 91s 228ms/step - loss: 5.7589 - mean_absolute_error: 1.7666 - val_loss: 14.0191 - val_mean_absolute_error: 2.8988Epoch 12/25400/400 [==============================] - 92s 229ms/step - loss: 5.3819 - mean_absolute_error: 1.6724 - val_loss: 7.1035 - val_mean_absolute_error: 2.1311Epoch 13/25400/400 [==============================] - 91s 227ms/step - loss: 5.5961 - mean_absolute_error: 1.7425 - val_loss: 14.4304 - val_mean_absolute_error: 2.9596Epoch 14/25400/400 [==============================] - 91s 227ms/step - loss: 5.3910 - mean_absolute_error: 1.6667 - val_loss: 7.3332 - val_mean_absolute_error: 2.1058Epoch 15/25400/400 [==============================] - 91s 228ms/step - loss: 5.3278 - mean_absolute_error: 1.6476 - val_loss: 8.5243 - val_mean_absolute_error: 2.1570Epoch 16/25400/400 [==============================] - 91s 227ms/step - loss: 5.1909 - mean_absolute_error: 1.6277 - val_loss: 10.1045 - val_mean_absolute_error: 2.4271Epoch 17/25400/400 [==============================] - 91s 228ms/step - loss: 5.5173 - mean_absolute_error: 1.6810 - val_loss: 7.0249 - val_mean_absolute_error: 2.0507Epoch 18/25400/400 [==============================] - 91s 228ms/step - loss: 4.8198 - mean_absolute_error: 1.5651 - val_loss: 9.4390 - val_mean_absolute_error: 2.4027Epoch 19/25400/400 [==============================] - 91s 228ms/step - loss: 5.0561 - mean_absolute_error: 1.6042 - val_loss: 6.4782 - val_mean_absolute_error: 1.9549Epoch 20/25400/400 [==============================] - 91s 227ms/step - loss: 4.9979 - mean_absolute_error: 1.5990 - val_loss: 7.7401 - val_mean_absolute_error: 2.0900Epoch 21/25400/400 [==============================] - 91s 227ms/step - loss: 5.0527 - mean_absolute_error: 1.6058 - val_loss: 12.2271 - val_mean_absolute_error: 2.7062Epoch 22/25400/400 [==============================] - 91s 227ms/step - loss: 4.1005 - mean_absolute_error: 1.4261 - val_loss: 7.2351 - val_mean_absolute_error: 2.0184Epoch 23/25400/400 [==============================] - 91s 227ms/step - loss: 5.1156 - mean_absolute_error: 1.6122 - val_loss: 6.6283 - val_mean_absolute_error: 1.9421Epoch 24/25400/400 [==============================] - 91s 228ms/step - loss: 5.0533 - mean_absolute_error: 1.6155 - val_loss: 14.7405 - val_mean_absolute_error: 3.0259Epoch 25/25400/400 [==============================] - 91s 228ms/step - loss: 5.0501 - mean_absolute_error: 1.6213 - val_loss: 12.0591 - val_mean_absolute_error: 2.6012--- 2297.5507295131683 seconds ---</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(hist.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(hist.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Model MSE / Loss&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MSE/Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Test&#x27;</span>], loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/06/kaggle-try/output_18_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(hist.history[<span class="string">&#x27;mean_absolute_error&#x27;</span>])</span><br><span class="line">plt.plot(hist.history[<span class="string">&#x27;val_mean_absolute_error&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Model Mean Absolute Error&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MAE&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train&#x27;</span>, <span class="string">&#x27;Test&#x27;</span>], loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/06/kaggle-try/output_19_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">del</span> train_gen</span><br><span class="line"><span class="keyword">del</span> valid_gen</span><br><span class="line"><span class="keyword">del</span> X_train</span><br><span class="line"><span class="keyword">del</span> y_train</span><br><span class="line"><span class="keyword">del</span> train_df</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure><pre><code>6378</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_weights(<span class="string">&#x27;./trained_model.h5&#x27;</span>, overwrite=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir, makedirs</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> isfile, join, basename, splitext, isfile, exists</span><br></pre></td></tr></table></figure><h2 id="Test-data-Normalize"><a href="#Test-data-Normalize" class="headerlink" title="Test data Normalize"></a>Test data Normalize</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_test</span>(<span class="params">ts_length = <span class="number">150000</span></span>):</span></span><br><span class="line">    base_dir = <span class="string">&#x27;./test/&#x27;</span></span><br><span class="line">    test_files = [f <span class="keyword">for</span> f <span class="keyword">in</span> listdir(base_dir) <span class="keyword">if</span> isfile(join(base_dir, f))]</span><br><span class="line"></span><br><span class="line">    ts = np.empty([<span class="built_in">len</span>(test_files), ts_length])</span><br><span class="line">    ids = []</span><br><span class="line">    </span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> tqdm(test_files):</span><br><span class="line">        ids.append(splitext(f)[<span class="number">0</span>])</span><br><span class="line">        t_df = pd.read_csv(base_dir + f, dtype=&#123;<span class="string">&quot;acoustic_data&quot;</span>: np.int8&#125;)</span><br><span class="line">        ts[i, :] = t_df[<span class="string">&#x27;acoustic_data&#x27;</span>].values</span><br><span class="line">        i = i + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ts, ids</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data, test_ids = load_test()</span><br></pre></td></tr></table></figure><pre><code>100%|██████████| 2624/2624 [00:27&lt;00:00, 97.07it/s]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_test = test_data</span><br><span class="line">X_test = np.expand_dims(X_test, <span class="number">2</span>)</span><br><span class="line">X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(2624, 150000, 1)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">submission_df = pd.DataFrame(&#123;<span class="string">&#x27;seg_id&#x27;</span>: test_ids, <span class="string">&#x27;time_to_failure&#x27;</span>: y_pred[:, <span class="number">0</span>]&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">submission_df.to_csv(<span class="string">&quot;submission.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="후기"><a href="#후기" class="headerlink" title="후기"></a>후기</h2><p>  <img src="/2020/12/06/kaggle-try/kaggle_1.PNG" alt="greem"></p><blockquote><p>시계열 데이터셋에다 도란스포머 해보니 할만 하지만 GPU가 버티질 못함..</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/Kaggle-Dacon/">Kaggle, Dacon</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/Try/">Try</category>
      
      <category domain="https://ikarus-999.github.io/tags/Kaggle/">Kaggle</category>
      
      
      <comments>https://ikarus-999.github.io/2020/12/06/kaggle-try/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>참고리스트</title>
      <link>https://ikarus-999.github.io/2020/12/06/%EC%B0%B8%EA%B3%A0%EB%A6%AC%EC%8A%A4%ED%8A%B8/</link>
      <guid>https://ikarus-999.github.io/2020/12/06/%EC%B0%B8%EA%B3%A0%EB%A6%AC%EC%8A%A4%ED%8A%B8/</guid>
      <pubDate>Sun, 06 Dec 2020 01:08:33 GMT</pubDate>
      
      <description>텐서플로우 개발에 참고할 만한 문서들(References)
Tensorflow Tutorial
 텐서플로우 튜토리얼

시계열데이터 스터디자료 모음(Transformer)
 참고논문1
둘러볼만한 깃헙
둘러볼만한 깃헙2
깃헙 링크2

음성 자연어처리 스터디자료 모음
 TacoTron Tensorflow
오디오 딥러닝 참고자료 best
VGGish
kaggle Dataset

계속 업데이트 예정 입니다.</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="텐서플로우-개발에-참고할-만한-문서들-References"><a href="#텐서플로우-개발에-참고할-만한-문서들-References" class="headerlink" title="텐서플로우 개발에 참고할 만한 문서들(References)"></a>텐서플로우 개발에 참고할 만한 문서들(References)</h2><h3 id="Tensorflow-Tutorial"><a href="#Tensorflow-Tutorial" class="headerlink" title="Tensorflow Tutorial"></a>Tensorflow Tutorial</h3><p>  <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials">텐서플로우 튜토리얼</a></p><h3 id="시계열데이터-스터디자료-모음-Transformer"><a href="#시계열데이터-스터디자료-모음-Transformer" class="headerlink" title="시계열데이터 스터디자료 모음(Transformer)"></a>시계열데이터 스터디자료 모음(Transformer)</h3><p>  <a href="https://paperswithcode.com/task/time-series-forecasting">참고논문1</a><br>  <a href="https://github.com/LongxingTan/Time-series-prediction">둘러볼만한 깃헙</a><br>  <a href="https://github.com/allen-chiang/Time-Series-Transformer">둘러볼만한 깃헙2</a><br>  <a href="https://github.com/fengyang95/Awesome-Deep-Learning-Based-Time-Series-Forecasting">깃헙 링크2</a></p><h3 id="음성-자연어처리-스터디자료-모음"><a href="#음성-자연어처리-스터디자료-모음" class="headerlink" title="음성 자연어처리 스터디자료 모음"></a>음성 자연어처리 스터디자료 모음</h3><p>  <a href="https://github.com/carpedm20/multi-speaker-tacotron-tensorflow">TacoTron Tensorflow</a><br>  <a href="https://github.com/musikalkemist/DeepLearningForAudioWithPython">오디오 딥러닝 참고자료 best</a><br>  <a href="https://github.com/luuil/Tensorflow-Audio-Classification">VGGish</a><br>  <a href="https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification">kaggle Dataset</a></p><blockquote><p>계속 업데이트 예정 입니다.</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/Papers/">Papers</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/Paper/">Paper</category>
      
      
      <comments>https://ikarus-999.github.io/2020/12/06/%EC%B0%B8%EA%B3%A0%EB%A6%AC%EC%8A%A4%ED%8A%B8/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문하기-9</title>
      <link>https://ikarus-999.github.io/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/</link>
      <guid>https://ikarus-999.github.io/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/</guid>
      <pubDate>Thu, 03 Dec 2020 09:41:05 GMT</pubDate>
      
      <description>지난번에 이어서 오디오 딥러닝 2번째
2. Sound Representation
위에서 Sampling된 discrete한 데이터를 이제 우리는 표현이 가능합니다. 그렇다면 어떤 요소를 기반으로 저희가 데이터를 표현해야할까요?, 첫번째는 시간의 흐름에 따라, 공기의 파동의 크기로 보는 Time-domain Representation 방법이 있습니다. 두번째는 시간에 따라서 frequency의 변화를 보는 Time-Frequency representation이 있습니다. 

2.1. Time domain - Waveform
Wavef</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="지난번에-이어서-오디오-딥러닝-2번째"><a href="#지난번에-이어서-오디오-딥러닝-2번째" class="headerlink" title="지난번에 이어서 오디오 딥러닝 2번째"></a>지난번에 이어서 오디오 딥러닝 2번째</h2><h2 id="2-Sound-Representation"><a href="#2-Sound-Representation" class="headerlink" title="2. Sound Representation"></a>2. Sound Representation</h2><p>위에서 Sampling된 discrete한 데이터를 이제 우리는 표현이 가능합니다. 그렇다면 어떤 요소를 기반으로 저희가 데이터를 표현해야할까요?, 첫번째는 시간의 흐름에 따라, 공기의 파동의 크기로 보는 Time-domain Representation 방법이 있습니다. 두번째는 시간에 따라서 frequency의 변화를 보는 Time-Frequency representation이 있습니다. </p><h3 id="2-1-Time-domain-Waveform"><a href="#2-1-Time-domain-Waveform" class="headerlink" title="2.1. Time domain - Waveform"></a>2.1. Time domain - Waveform</h3><p>Waveform의 경우에는 오디오의 자연적이 표현입니다. 시간이 x축으로 그리고 amplitude가 y축으로 표현이 됩니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> librosa.display</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">14</span>,<span class="number">5</span>))</span><br><span class="line">librosa.display.waveplot(y[<span class="number">0</span>:<span class="number">10000</span>], sr=sr)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.collections.PolyCollection at 0x7fa325708d50&gt;</code></pre><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_21_1.png" alt="png"></p><h3 id="정현파-Sinusoid"><a href="#정현파-Sinusoid" class="headerlink" title="정현파 (Sinusoid)"></a>정현파 (Sinusoid)</h3><p>모든 신호는 주파수(frequency)와 크기(magnitude), 위상(phase)이 다른 정현파(sinusolida signal)의 조합으로 나타낼 수 있다. 퓨리에 변환은 조합된 정현파의 합(하모니) 신호에서 그 신호를 구성하는 정현파들을 각각 분리해내는 방법입니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sr = <span class="number">16000</span> <span class="comment"># sample rate</span></span><br><span class="line">T = <span class="number">2.0</span>    <span class="comment"># seconds</span></span><br><span class="line">t = np.linspace(<span class="number">0</span>, T, <span class="built_in">int</span>(T*sr), endpoint=<span class="literal">False</span>) <span class="comment"># time variable</span></span><br><span class="line">x = <span class="number">0.5</span>*np.sin(<span class="number">2</span>*np.pi*<span class="number">440</span>*t)                <span class="comment"># pure sine wave at 440 Hz</span></span><br><span class="line"><span class="comment"># y = 0.5*numpy.sin(2*numpy.pi*400*t)</span></span><br><span class="line"></span><br><span class="line">ipd.Audio(x, rate=sr) <span class="comment"># load a NumPy array</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">librosa.display.waveplot(x[:<span class="number">50</span>], sr=sr)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.collections.PolyCollection at 0x7fa327a01550&gt;</code></pre><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_24_1.png" alt="png"></p><h3 id="푸리에-변환-Fourier-transform"><a href="#푸리에-변환-Fourier-transform" class="headerlink" title="푸리에 변환 (Fourier transform)"></a>푸리에 변환 (Fourier transform)</h3><p>푸리에 변환(Fourier transform)을 직관적으로 설명하면 푸리에 변환은 임의의 입력 신호를 다양한 주파수를 갖는 주기함수(복수 지수함수)들의 합으로 분해하여 표현하는 것 입니다. 그리고 각 주기함수들의 진폭을 구하는 과정을 퓨리에 변환이라고 합니다.</p><ul><li>주기(period): 파동이 한번 진동하는데 걸리는 시간, 또는 그 길이, 일반적으로 sin함수의 주기는 \(2\pi /w\)입니다</li><li>주파수(frequency): 1초동안의 진동횟수입니다.</li></ul><p>퓨리에 변환의 식을 살펴봅시다.</p><p>$$y(t)=\sum_{k=-\infty}^\infty A_k , \exp \left( i\cdot 2\pi\frac{k}{T} t \right)$$</p><p>이 식을 하나식 해석해봅시다.<br>\(k\)는 \(-\infty\) ~ \(\infty\)의 범위를 가지고 움직입니다.<br>이것은 주기함수들의 갯수입니다. 어떠한 신호가 다른 주기함수들의 합으로 표현되는데, 그 주기함수는 무한대의 범위에 있군요.</p><p>그렇다면 \(A_k\)은 그 사인함수의 진폭이라고 합니다. 이 식은 시간에 대한 입력신호 \(y_{t}\)가  \(\exp \left( i\cdot 2\pi\frac{k}{T} t \right)\) 와 진폭(\(A_k\))의 선형결합으로 표현됨을 말하고 있군요.</p><p>위 그림을 본다면 조금 더 명확히 알수 있을 것 같습니다. 붉은색 라인이 입력신호 \(y_{t}\) 입니다. 일반적으로 우리가 다루게 되는 데이터인 음악이나 목소리 같은 데이터 역시 complex tone입니다. 여려개의 주파수영역이 합쳐진 것이죠. 이러한 여러개의 주파수 영역을 분리하자!가 주요한 아이디어입니다. 파란색 주기함수들을 보신다면 여러개의 주기함수들을 찾으실 수 있습니다. 그 주기함수들은 고유의 주파수(frequency)와 강도(amplitude)를 가지고 있고 그것이 파란색의 라인들로 표현되어 있습니다.</p><p>진폭에 대한 수식은 다음과 같습니다.<br>$$A_k = \frac{1}{T} \int_{-\frac{T}{2}}^\frac{T}{2} f(t) , \exp \left( -i\cdot 2\pi \frac{k}{T} t \right) , dt$$<br>여기서 하나의 의문점이 드실것 같습니다. 주기함수의 합으로 표현된다고 했는데 저희가 보고 있는것은 \(\exp \left( i\cdot 2\pi\frac{k}{T} t \right)\) 지수함수의 형태이기 때문입니다.</p><p>지수함수와 주기함수 사이의 연관관계는 무엇일까요? 그 관계를 찾은 것이 바로 오일러 공식입니다.</p><p>$$e^{i\theta} = \cos{\theta} + i\sin{\theta}$$</p><p>이 식을 위 식처럼 표현한다면 다음과 같습니다<br>$$\exp \left( i\cdot 2\pi\frac{k}{T} t \right) = \cos\left({2\pi\frac{k}{T}}\right) + i\sin\left({2\pi\frac{k}{T}}\right)$$</p><p>여기서 \(\cos{2\pi\frac{k}{T}}\), \(i\sin{2\pi\frac{k}{T}}\) 함수는 주기와 주파수를 가지는 주기함수입니다. </p><p>즉 퓨리에 변환은 입력 singal이 어떤것인지 상관없이 sin, cos과 같은 주기함수들의 합으로 항상 분해 가능하다는 것입니다. </p><h3 id="Fourier-Transform의-Orthogonal"><a href="#Fourier-Transform의-Orthogonal" class="headerlink" title="Fourier Transform의 Orthogonal"></a>Fourier Transform의 Orthogonal</h3><p>$$y(t)=\sum_{k=-\infty}^\infty A_k , \exp \left( i\cdot 2\pi\frac{k}{T} t \right)$$</p><p>어떠한 주기함수를 우리는 cos과 sin함수로 표현하게 되었습니다. 여기서 한가지 재밌는 점은, 이 함수들이 직교하는 함수(orthogonal)라는 점이다.<br>$${ \exp \left(i\cdot 2\pi\frac{k}{T} t\right) } = orthogonal$$</p><p>벡터의 직교는 해당 벡터를 통해 평면의 모든 좌표를 표현할수 있었다. 함수의 내적은 적분으로 표현할 수 있는데, 만약 구간 [a,b]에서 직교하는 함수는 구간 [a,b]의 모든 함수를 표현할수 있습니다.</p><p>위 케이스에서는 cos, sin 함수가 사실상 우리 입력신호에 대해서 기저가 되어주는 함수라고 생각할 수 있습니다.</p><h3 id="DFT-Discrete-Fourier-Transform"><a href="#DFT-Discrete-Fourier-Transform" class="headerlink" title="DFT (Discrete Fourier Transform)"></a>DFT (Discrete Fourier Transform)</h3><p>한가지 의문점이 듭니다. 바로, 우리가 sampling으로 들어온 데이터는 바로 시간의 간격에 따른 소리의 amplitude의 discrete한 데이터이기 때문이다. 그렇다면 위 푸리에 변환 식을 Discrete한 영역으로 생각해봅시다.</p><p>만약에 우리가 수집한 데이터 \(y_{n}\)에서, 이산 시계열 데이터가 주기 \(N\)으로 반복한다고 할때, DFT는 주파수와 진폭이 다른 \(N\)개의 사인 함수의 합으로 표현이 가능합니다.<br>$$y_n = \frac{1}{N} \sum_{k=0}^{N-1} Y_k \cdot \exp \left( i\cdot 2\pi\frac{k}{N} n \right)$$</p><p>위 식을 보면 k의 range가 0부터 \(N-1\)로 변화했음을 알 수 있다. 이때 Spectrum \(Y_{k}\)를 원래의 시계열 데이터에 대한 퓨리에 변환값이라고 하죠.</p><p>$$Y_k = \sum_{n=0}^{N-1} y_n\cdot \exp \left( -i\cdot 2\pi\frac{k}{N} n \right)$$</p><ul><li>\(y_{n}\) : input signal</li><li>\(n\) : Discrete time index</li><li>\(k\) : discrete frequency index</li><li>\(Y_{k}\) : k번째 frequeny에 대한 Spectrum의 값</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DFT</span>(<span class="params">x</span>):</span></span><br><span class="line">    N = <span class="built_in">len</span>(x)</span><br><span class="line">    X = np.array([])</span><br><span class="line">    nv = np.arange(N)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        s = np.exp(<span class="number">1j</span>*<span class="number">2</span>*np.pi*k/N*nv)</span><br><span class="line">        X = np.append(X, <span class="built_in">sum</span>(x*np.conjugate(s)))</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h3 id="STFT-Short-Time-Fourier-Transform"><a href="#STFT-Short-Time-Fourier-Transform" class="headerlink" title="STFT (Short-Time Fourier Transform)"></a>STFT (Short-Time Fourier Transform)</h3><p>FFT는 시간에 흐름에 따라 신호의 수파수가 변했을때, 어느 시간대에 주파수가 변하는지 모르게 됩니다. 이러한 한계를 극복하기 위해서, STFT는 시간의 길이를 나눠서 이제 퓨리에 변환을 하게 됩니다. 즉 FFT를 했을때는 Time domina에 대한 정보가 날아가게 되는 것이죠.</p><p>주파수의 특성이 시간에 따라 달라지는 사운드를 분석하는 방법입니다. 일반적으로 우리가 사용하는 signal 데이터에 적합하다. 시계열 데이터를 일정한 시간 구간 (window size)로 나누고, 각 구간에 대해서 스펙트럼을 구하는 데이터이다. 이는 Time-frequency 2차원 데이터로 표현이 됩니다.</p><p>$$X(l,k) = \sum_{n=0}^{N-1} w(n) x(n+lH)\exp^{\frac{-2\pi k n}{N}}$$</p><ul><li><p>\(N\) : FFT size</p><ul><li>Window를 얼마나 많은 주파수 밴드로 나누는가 입니다.</li></ul></li><li><p>Duration</p><ul><li>샘플링 레이트를 window로 나눈 값입니다.</li><li>$$T= window/SR$$</li><li>T(Window) = 5T(Signal), duration은 신호주기보다 5배 이상 길게 잡아야한다.</li><li>440Hz 신호의 window size는 5*(1/440)이 됩니다.</li></ul></li><li><p>\(w(n)\) : Window function</p><ul><li>일반적으로 Hann window가 쓰입니다.</li></ul></li><li><p>\(n\) : Window size</p><ul><li>Window 함수에 들어가는 Sample의 양입니다.</li><li>작을수록 Low-frequency resolution을 가지게 되고, high-time resolution을 가집니다.</li><li>길수록 High-frequency, low time resolution을 가집니다.</li></ul></li><li><p>\(H\) : Hop size</p><ul><li>윈도우가 겹치는 사이즈입니다. 일반적으로는 1/4정도를 겹치게 합니다.</li></ul></li></ul><p>STFT의 결과는 즉 시간의 흐름(Window)에 따른 Frequency영역별 Amplitude를 반환합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sr = <span class="number">16000</span> <span class="comment"># sample rate</span></span><br><span class="line">T = <span class="number">2.0</span>    <span class="comment"># seconds</span></span><br><span class="line">t = np.linspace(<span class="number">0</span>, T, <span class="built_in">int</span>(T * sr), endpoint=<span class="literal">False</span>) <span class="comment"># time variable</span></span><br><span class="line">x = <span class="number">0.5</span> * np.sin(<span class="number">2</span> * np.pi * <span class="number">440</span> * t)                <span class="comment"># pure sine wave at 440 Hz</span></span><br><span class="line"><span class="comment"># y = 0.5*numpy.sin(2*numpy.pi*400*t)</span></span><br><span class="line"></span><br><span class="line">ipd.Audio(x, rate=sr) <span class="comment"># load a NumPy array</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="built_in">len</span>(y))</span><br><span class="line">D = librosa.stft(y)</span><br><span class="line">print(D.shape, D)</span><br><span class="line"></span><br><span class="line"><span class="comment"># phase 에 대한 정보를 날린다.</span></span><br><span class="line">D_mag = np.<span class="built_in">abs</span>(D)</span><br><span class="line">print(D_mag)</span><br><span class="line">print(D_mag.shape)</span><br><span class="line"></span><br><span class="line">magnitude, phase = librosa.magphase(D)</span><br><span class="line"></span><br><span class="line">print(magnitude)</span><br><span class="line">print(magnitude.shape)</span><br><span class="line"></span><br><span class="line">print(magnitude-D_mag)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>48944(1025, 96) [[-2.1494275e-01+0.0000000e+00j -2.0992082e-01+0.0000000e+00j  -2.0418610e-01+0.0000000e+00j ... -1.9438802e-01+0.0000000e+00j  -1.9518623e-01+0.0000000e+00j -2.3163199e-01+0.0000000e+00j] [ 9.3493842e-02+6.7762636e-21j  1.2481287e-01+4.8880498e-03j   7.3961377e-02+1.3274251e-03j ...  7.7925511e-02-1.7781712e-02j   9.6285135e-02+1.7115690e-02j  1.1564651e-01-5.2810002e-02j] [ 1.9829417e-02+8.2818238e-19j -3.1706840e-02+1.5587136e-02j   5.7078212e-02-2.0519590e-02j ...  2.3265863e-02+7.6752454e-02j   3.0044108e-03-6.0352467e-02j  5.4616658e-03+6.8522707e-02j] ... [-5.3125373e-03-1.2618632e-18j  3.4157380e-03-1.7295172e-03j  -1.8859134e-03-3.5993013e-04j ... -7.6227036e-04-9.3025468e-05j  -1.8814437e-04-8.4138475e-05j  4.7763987e-04-5.3400453e-04j] [ 2.1248308e-03+1.5585406e-19j -1.4035926e-03+8.0862024e-05j   2.4144542e-03+3.4830419e-04j ... -2.3595782e-04+1.1687888e-03j   1.1331354e-04+1.2911476e-04j  2.8909228e-04+3.3650018e-04j] [-8.1756472e-04+0.0000000e+00j -9.3529455e-04+0.0000000e+00j  -1.4104146e-03+0.0000000e+00j ...  1.3452002e-03+0.0000000e+00j  -9.2299597e-06+0.0000000e+00j -4.9439305e-04+0.0000000e+00j]][[2.1494275e-01 2.0992082e-01 2.0418610e-01 ... 1.9438802e-01  1.9518623e-01 2.3163199e-01] [9.3493842e-02 1.2490856e-01 7.3973291e-02 ... 7.9928555e-02  9.7794548e-02 1.2713383e-01] [1.9829417e-02 3.5331041e-02 6.0654562e-02 ... 8.0201246e-02  6.0427204e-02 6.8740025e-02] ... [5.3125373e-03 3.8286415e-03 1.9199529e-03 ... 7.6792570e-04  2.0610093e-04 7.1645004e-04] [2.1248308e-03 1.4059199e-03 2.4394479e-03 ... 1.1923688e-03  1.7178644e-04 4.4362905e-04] [8.1756472e-04 9.3529455e-04 1.4104146e-03 ... 1.3452002e-03  9.2299597e-06 4.9439305e-04]](1025, 96)[[2.1494275e-01 2.0992082e-01 2.0418610e-01 ... 1.9438802e-01  1.9518623e-01 2.3163199e-01] [9.3493842e-02 1.2490856e-01 7.3973291e-02 ... 7.9928555e-02  9.7794548e-02 1.2713383e-01] [1.9829417e-02 3.5331041e-02 6.0654562e-02 ... 8.0201246e-02  6.0427204e-02 6.8740025e-02] ... [5.3125373e-03 3.8286415e-03 1.9199529e-03 ... 7.6792570e-04  2.0610093e-04 7.1645004e-04] [2.1248308e-03 1.4059199e-03 2.4394479e-03 ... 1.1923688e-03  1.7178644e-04 4.4362905e-04] [8.1756472e-04 9.3529455e-04 1.4104146e-03 ... 1.3452002e-03  9.2299597e-06 4.9439305e-04]](1025, 96)[[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">S = librosa.core.stft(audio_np, n_fft=<span class="number">1024</span>, hop_length=<span class="number">512</span>, win_length=<span class="number">1024</span>)</span><br><span class="line">D = np.<span class="built_in">abs</span>(S)**<span class="number">2</span></span><br><span class="line">log_S = librosa.power_to_db(S, ref=np.<span class="built_in">max</span>) <span class="comment">#소리의 단위를 db로 바꿈 </span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">librosa.display.specshow(log_S, sr=<span class="number">16000</span>, x_axis=<span class="string">&#x27;time&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="Window-function"><a href="#Window-function" class="headerlink" title="Window function?"></a>Window function?</h3><p>위에서 Window function과 Window size라는 이야기가 나오고 있습니다. 윈도우 Function과 Size는 왜 쓰는 것이며 어떨때 쓰는 것일까요?</p><p>Window function의 주된 기능은 main-lobe의 width와 side-lobe의 레벨의 Trade-off 를 제어해 준다는 장점이 있습니다. 그리고 깁스 현상을 막아주는 고마운 친구이기도 하죠. 지금나온 main-lobe, side-bloe, 깁스현상은 무엇일까요?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">frame_audio</span>(<span class="params">audio, FFT_size=<span class="number">1024</span>, hop_size=<span class="number">20</span>, sample_rate = <span class="number">22050</span></span>):</span></span><br><span class="line">    audio = np.pad(audio, <span class="built_in">int</span>(FFT_size/<span class="number">2</span>), mode=<span class="string">&#x27;reflect&#x27;</span>)</span><br><span class="line">    frame_len = np.<span class="built_in">round</span>(sample_rate*hop_size / <span class="number">1000</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">    frame_num = <span class="built_in">int</span>((<span class="built_in">len</span>(audio) - FFT_size) / frame_len) + <span class="number">1</span></span><br><span class="line">    frames = np.zeros((frame_num, FFT_size))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(frame_num):</span><br><span class="line">        frames[n] = audio[n*frame_len:n*frame_len+FFT_size]</span><br><span class="line">    <span class="keyword">return</span> frames</span><br><span class="line"></span><br><span class="line">audio_framed = frame_audio(audio_np)</span><br><span class="line">print(<span class="string">&quot;Framed audio shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(audio_framed.shape))</span><br></pre></td></tr></table></figure><pre><code>Framed audio shape: (469, 1024)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> signal</span><br><span class="line"></span><br><span class="line">window = signal.get_window(<span class="string">&quot;hann&quot;</span>, <span class="number">1024</span>, fftbins=<span class="literal">True</span>)</span><br><span class="line">audio_win = audio_framed * window</span><br><span class="line">ind = <span class="number">2</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.plot(window)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">plt.plot(audio_framed[ind])</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">plt.plot(audio_win[ind])</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_49_0.png" alt="png"></p><p>플롯을 보게 된다면 windowing을 적용하기전 plot은 끝부분이 다 다르지만, windowing을 지나고 나서 나오는 plot은 끝이 0 으로 일치한다는 특성을 볼 수 있습니다.</p><h3 id="Window-size"><a href="#Window-size" class="headerlink" title="Window size?"></a>Window size?</h3><p>윈도우 사이즈는 일반적으로 time과 frequency의 resolutions을 제어해 줍니다.</p><ul><li>short-window : 낮은 frequency resolution, 높은 time-resolution을 가지게 됩니다.</li><li>Long-window : 높은 frequency resolution을 가지며, 낮은 time-resolution을 가지게 됩니다.</li></ul><h3 id="Spectrogram"><a href="#Spectrogram" class="headerlink" title="Spectrogram"></a>Spectrogram</h3><p>Spectrogram을 추출하는 방법을 고민해봅시다.일반적으로 프로세스는 입력신호에 대해서 window function을 통과하여 window size만큼 sampling 된 data를 받아서 Discrete Fourier Transform을 거치게 됩니다. DFT를 거친 신호들은 Frequency와 Amplitude의 영역을 가지는 Spectrum이 됩니다. 이후 이를 90도로 회전시켜서, time domain으로 stack하게 됩니다.</p><p>Spectrogram은 Frequency Scale에 대해서 Scaling이 이루어집니다. 주파수 영역에 Scaling을 하는 이유는, 인간의 주파수를 인식하는 방식과 연관이 있습니다. </p><p>일반적으로 사람은, 인접한 주파수를 크게 구별하지 못합니다. 그 이유는 우리의 인지기관이 categorical한 구분을 하기 때문입니다. 때문에 우리는 주파수들의 Bin의 그룹을 만들고 이들을 합하는 방식으로, 주파수 영역에서 얼마만큼의 에너지가 있는지를 찾아볼 것입니다. 일반적으로는 인간이 적은 주파수에 더 풍부한 정보를 사용하기때문에, 주파수가 올라갈수록 필터의 폭이 높아지면서 고주파는 거의 고려를 안하게 됩니다.</p><p>따라서 아래 frequency scale은 어떤 방식을 통해 저주파수대 영역을 고려할 것이가에 대한 고민이 남아 있습니다.</p><h3 id="Linear-frequency-scale"><a href="#Linear-frequency-scale" class="headerlink" title="Linear frequency scale"></a>Linear frequency scale</h3><p>일반적으로 single tone(순음)들의 배음 구조를 파악하기 좋습니다. 하지만 분포가 저주파수 영역에 기울어져(skewed) 있습니다.</p><h3 id="Mel-Scale"><a href="#Mel-Scale" class="headerlink" title="Mel Scale"></a>Mel Scale</h3><p>멜 스펙트럼은 주파수 단위를 다음 공식에 따라 멜 단위로 바꾼 것을 의미합니다.</p><p>$$m = 2595 \log_{10}\left(1 + \frac{f}{700}\right)$$<br>일반적으로는 mel-scaled bin을 FFT size보다 조금더 작게 만드는게 일반적입니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># STFT</span></span><br><span class="line">S = librosa.core.stft(audio_np, n_fft=<span class="number">1024</span>, hop_length=<span class="number">512</span>, win_length=<span class="number">1024</span>)</span><br><span class="line"><span class="comment"># phase 에 대한 정보를 날린다.</span></span><br><span class="line">D = np.<span class="built_in">abs</span>(S)**<span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mel spectrogram (512 --&gt; 40)</span></span><br><span class="line">mel_basis = librosa.filters.mel(sr, <span class="number">1024</span>, n_mels=<span class="number">40</span>)</span><br><span class="line">mel_S = np.dot(mel_basis, D)</span><br><span class="line">mel_S.shape</span><br></pre></td></tr></table></figure><pre><code>(40, 404)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> librosa.display</span><br><span class="line"></span><br><span class="line">S = librosa.feature.melspectrogram(audio_np, sr=sr, n_mels = <span class="number">128</span>)</span><br><span class="line">log_S = librosa.power_to_db(S, ref=np.<span class="built_in">max</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">librosa.display.specshow(log_S, sr=sr, x_axis=<span class="string">&#x27;time&#x27;</span>, y_axis=<span class="string">&#x27;mel&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Mel power sepctrogram&#x27;</span>)</span><br><span class="line">plt.colorbar(<span class="built_in">format</span>=<span class="string">&#x27;%+02.0f dB&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_54_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> librosa.display</span><br><span class="line"></span><br><span class="line">S = librosa.feature.melspectrogram(audio_np, sr=sr, n_mels = <span class="number">256</span>)</span><br><span class="line">log_S = librosa.power_to_db(S, ref=np.<span class="built_in">max</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">librosa.display.specshow(log_S, sr=sr, x_axis=<span class="string">&#x27;time&#x27;</span>, y_axis=<span class="string">&#x27;mel&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Mel power sepctrogram&#x27;</span>)</span><br><span class="line">plt.colorbar(<span class="built_in">format</span>=<span class="string">&#x27;%+02.0f dB&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_55_0.png" alt="png"></p><h3 id="Bark-scale"><a href="#Bark-scale" class="headerlink" title="Bark scale"></a>Bark scale</h3><p>귀가 인식하는 주파수의 영역은 대략 20Hz~2000Hz 로 가정합니다. 하지만 주파수에 대한 사람의 인식은 비선형적입니다. 귀와 뇌의 가청대역을 24개의 대역으로 나눈것을 Bark라고 합니다! Bark scale은 500Hz이하에서는 100Hz의 대역폭을 가지며, 500Hz 이상에서는 각 대역의 중심수파수의 대략 20%에 해당하는 대역폭을 가지게 됩니다.</p><p><code>20, 100, 200, 300, 400, 510, 630, 770, 920, 1080, 1270, 1480, 1720, 2000, 2320, 2700, 3150, 3700, 4400, 5300, 6400, 7700, 9500, 12000, 15500 ( Hz )</code></p><h3 id="Log-compression"><a href="#Log-compression" class="headerlink" title="Log compression"></a>Log compression</h3><p>\(10 * log10(\frac{S}{ref})\)<br>의 단위로 신호를 스케일링 합니다. 이는 spectrogram을 데시벨 유닛으로 전환해 줍니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#log compression</span></span><br><span class="line">log_mel_S = librosa.power_to_db(mel_S)</span><br><span class="line">log_mel_S.shape</span><br></pre></td></tr></table></figure><pre><code>(40, 404)</code></pre><h3 id="Discrete-cosine-transform-DCT"><a href="#Discrete-cosine-transform-DCT" class="headerlink" title="Discrete cosine transform (DCT)"></a>Discrete cosine transform (DCT)</h3><p>DCT는 n개의 데이터를 n개의 코사인 함수의 합으로 표현하여 데이터의 양을 줄이는 방식입니다. </p><ul><li>저 주파수에 에너지가 집중되고 고 주파수 영역에 에너지가 감소합니다.</li></ul><p>Filter Bank는 모두 Overlapping 되어 있기 때문에 Filter Bank 에너지들 사이에 상관관계가 존재하기 때문이다. DCT는 에너지들 사이에 이러한 상관관계를 분리 해주는 역활을 해줍니다.</p><p>하지만 여기서 26개 DCT Coefficient 들 중 12만 남겨야 하는데, 그 이유는 DCT Coefficient 가 많으면, Filter Bank 에너지의 빠른 변화를 나타내게 되고, 이것은 음성인식의 성능을 낮추게 됩니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mfcc (DCT)</span></span><br><span class="line">mfcc = librosa.feature.mfcc(S=log_mel_S, n_mfcc=<span class="number">13</span>)</span><br><span class="line">mfcc = mfcc.astype(np.float32)    <span class="comment"># to save the memory (64 to 32 bits)</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">librosa.display.specshow(mfcc)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3a8975cc88&gt;</code></pre><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_61_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=<span class="number">13</span>)</span><br><span class="line">delta2_mfcc = librosa.feature.delta(mfcc, order=<span class="number">2</span>)</span><br><span class="line">print(delta2_mfcc.shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">librosa.display.specshow(delta2_mfcc)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;MFCC coeffs&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Time&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;MFCC&#x27;</span>)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><pre><code>(13, 148)</code></pre><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_62_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_pitch</span>(<span class="params">data, sr</span>):</span></span><br><span class="line">    y_pitch = data.copy()</span><br><span class="line">    bins_per_octave = <span class="number">12</span></span><br><span class="line">    pitch_pm = <span class="number">2</span></span><br><span class="line">    pitch_change = pitch_pm * <span class="number">2</span> * (np.random.uniform())</span><br><span class="line">    y_pitch = librosa.effects.pitch_shift(y_pitch.astype(<span class="string">&#x27;float64&#x27;</span>), sr, n_steps=pitch_change,</span><br><span class="line">                                          bins_per_octave=bins_per_octave)</span><br><span class="line">    <span class="keyword">return</span> y_pitch</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">waveform_aug</span>(<span class="params">waveform,sr</span>):</span></span><br><span class="line">  y = change_pitch(waveform, sr)</span><br><span class="line">  fig = plt.figure(figsize = (<span class="number">14</span>,<span class="number">5</span>))</span><br><span class="line">  librosa.display.waveplot(y, sr=sr)</span><br><span class="line">  ipd.display(ipd.Audio(data=y, rate=sr))</span><br><span class="line">  <span class="keyword">return</span> y, sr</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ipd.display(ipd.Audio(data=audio_np, rate=sr))</span><br><span class="line">y, sr = waveform_aug(audio_np, <span class="number">16000</span>)</span><br></pre></td></tr></table></figure><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_66_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">S = librosa.feature.melspectrogram(audio_np, sr=sr, n_mels = <span class="number">128</span>)</span><br><span class="line">log_S = librosa.power_to_db(S, ref=np.<span class="built_in">max</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">librosa.display.specshow(log_S, sr=sr, x_axis=<span class="string">&#x27;time&#x27;</span>, y_axis=<span class="string">&#x27;mel&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Mel power sepctrogram&#x27;</span>)</span><br><span class="line">plt.colorbar(<span class="built_in">format</span>=<span class="string">&#x27;%+02.0f dB&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_67_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.uniform(low=<span class="number">1.5</span>, high=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>2.6892527992385458</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_aug</span>(<span class="params">data</span>):</span></span><br><span class="line">    y_aug = data.copy()</span><br><span class="line">    dyn_change = np.random.uniform(low=<span class="number">1.5</span>, high=<span class="number">3</span>)</span><br><span class="line">    y_aug = y_aug * dyn_change</span><br><span class="line">    <span class="keyword">return</span> y_aug</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_noise</span>(<span class="params">data</span>):</span></span><br><span class="line">    noise = np.random.randn(<span class="built_in">len</span>(data))</span><br><span class="line">    data_noise = data + <span class="number">0.005</span> * noise</span><br><span class="line">    <span class="keyword">return</span> data_noise</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hpss</span>(<span class="params">data</span>):</span></span><br><span class="line">    y_harmonic, y_percussive = librosa.effects.hpss(data.astype(<span class="string">&#x27;float64&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> y_harmonic, y_percussive</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.roll(data, <span class="number">1600</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stretch</span>(<span class="params">data, rate=<span class="number">1</span></span>):</span></span><br><span class="line">    input_length = <span class="built_in">len</span>(data)</span><br><span class="line">    streching = librosa.effects.time_stretch(data, rate)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(streching) &gt; input_length:</span><br><span class="line">        streching = streching[:input_length]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        streching = np.pad(streching, (<span class="number">0</span>, <span class="built_in">max</span>(<span class="number">0</span>, input_length - <span class="built_in">len</span>(streching))), <span class="string">&quot;constant&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> streching</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_pitch_and_speed</span>(<span class="params">data</span>):</span></span><br><span class="line">    y_pitch_speed = data.copy()</span><br><span class="line">    <span class="comment"># you can change low and high here</span></span><br><span class="line">    length_change = np.random.uniform(low=<span class="number">0.8</span>, high=<span class="number">1</span>)</span><br><span class="line">    speed_fac = <span class="number">1.0</span> / length_change</span><br><span class="line">    tmp = np.interp(np.arange(<span class="number">0</span>, <span class="built_in">len</span>(y_pitch_speed), speed_fac), np.arange(<span class="number">0</span>, <span class="built_in">len</span>(y_pitch_speed)), y_pitch_speed)</span><br><span class="line">    minlen = <span class="built_in">min</span>(y_pitch_speed.shape[<span class="number">0</span>], tmp.shape[<span class="number">0</span>])</span><br><span class="line">    y_pitch_speed *= <span class="number">0</span></span><br><span class="line">    y_pitch_speed[<span class="number">0</span>:minlen] = tmp[<span class="number">0</span>:minlen]</span><br><span class="line">    <span class="keyword">return</span> y_pitch_speed</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data_noise = add_noise(audio_np)</span><br><span class="line">data_roll = shift(audio_np)</span><br><span class="line">data_stretch = stretch(audio_np)</span><br><span class="line">pitch_speed = change_pitch_and_speed(audio_np)</span><br><span class="line">value = value_aug(audio_np)</span><br><span class="line">y_harmonic, y_percussive = hpss(audio_np)</span><br><span class="line">y_shift = shift(audio_np)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipd.Audio(data_noise, rate=fs)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">librosa.display.specshow(librosa.amplitude_to_db(magnitude,</span><br><span class="line">                                                  ref=np.<span class="built_in">max</span>),</span><br><span class="line">                          y_axis=<span class="string">&#x27;log&#x27;</span>, x_axis=<span class="string">&#x27;time&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Power spectrogram&#x27;</span>)</span><br><span class="line">plt.colorbar(<span class="built_in">format</span>=<span class="string">&#x27;%+2.0f dB&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_32_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mel_s = librosa.feature.melspectrogram(y=y, sr=sr)</span><br><span class="line">print(mel_s.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">librosa.display.specshow(librosa.amplitude_to_db(mel_s,</span><br><span class="line">                                                  ref=np.<span class="built_in">max</span>),</span><br><span class="line">                          y_axis=<span class="string">&#x27;log&#x27;</span>, x_axis=<span class="string">&#x27;time&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Power spectrogram&#x27;</span>)</span><br><span class="line">plt.colorbar(<span class="built_in">format</span>=<span class="string">&#x27;%+2.0f dB&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>(128, 96)</code></pre><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_33_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=<span class="number">40</span>)</span><br><span class="line"></span><br><span class="line">print(mfccs.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">librosa.display.specshow(mfccs, x_axis=<span class="string">&#x27;time&#x27;</span>)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.title(<span class="string">&#x27;MFCC&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>(40, 96)</code></pre><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_34_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">librosa.display.specshow(librosa.amplitude_to_db(mfccs,</span><br><span class="line">                                                  ref=np.<span class="built_in">max</span>),</span><br><span class="line">                          y_axis=<span class="string">&#x27;log&#x27;</span>, x_axis=<span class="string">&#x27;time&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Power spectrogram&#x27;</span>)</span><br><span class="line">plt.colorbar(<span class="built_in">format</span>=<span class="string">&#x27;%+2.0f dB&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/output_35_0.png" alt="png"></p><h4 id="들리는-소리-배경잡음-목소리"><a href="#들리는-소리-배경잡음-목소리" class="headerlink" title="들리는 소리 = 배경잡음 + 목소리"></a>들리는 소리 = 배경잡음 + 목소리</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filename_wav = <span class="string">&quot;./wav/voice.wav&quot;</span></span><br><span class="line">filename_noise = <span class="string">&quot;./wav/cafe_noise.wav&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> IPython.display <span class="keyword">as</span> ipd</span><br><span class="line">ipd.Audio(filename_wav)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> IPython.display <span class="keyword">as</span> ipd</span><br><span class="line">ipd.Audio(filename_noise)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data_wav, sr_wav = librosa.load(filename_wav, mono=<span class="literal">True</span>, sr=<span class="number">16000</span>)</span><br><span class="line">data_noise, sr_noise = librosa.load(filename_noise, mono=<span class="literal">True</span>, sr=<span class="number">16000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(data_wav.shape)</span><br><span class="line">print(data_noise.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 전체적으로 들리는 소리는 소리의 합( 배경음 + 목소리 )</span></span><br><span class="line">data_wav_noise = data_wav[:] + data_noise[:<span class="built_in">len</span>(data_wav)]</span><br><span class="line"></span><br><span class="line">pos=<span class="number">10</span></span><br><span class="line">print(<span class="string">&quot;wav: &#123;:.8f&#125;, noise &#123;:.8f&#125;, wav+noise: &#123;:.8f&#125;&quot;</span>.<span class="built_in">format</span>(data_wav[pos], data_noise[pos], data_wav_noise[pos]))</span><br></pre></td></tr></table></figure><pre><code>(48944,)(1044712,)wav: -0.00009155, noise -0.00003052, wav + noise: -0.00012207</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipd.Audio(data_wav_noise, rate=<span class="number">16000</span>)</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      <category domain="https://ikarus-999.github.io/tags/Audio/">Audio</category>
      
      
      <comments>https://ikarus-999.github.io/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문하기-8</title>
      <link>https://ikarus-999.github.io/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-8/</link>
      <guid>https://ikarus-999.github.io/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-8/</guid>
      <pubDate>Thu, 03 Dec 2020 08:02:23 GMT</pubDate>
      
      <description>오디오 딥러닝 해보기
Reference
 * Digital Signal Processing Lecture https://github.com/spatialaudio/digital-signal-processing-lecture 
   
   
 * Python for Signal Processing (unipingco) https://github.com/unpingco/Python-for-Signal-Processing 
   
   
 * Audio for Deep Learning (남기현님) https://tykimos.github</description>
      
      
      
      <content:encoded><![CDATA[<h3 id="오디오-딥러닝-해보기"><a href="#오디오-딥러닝-해보기" class="headerlink" title="오디오 딥러닝 해보기"></a>오디오 딥러닝 해보기</h3><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><p>Digital Signal Processing Lecture <a href="https://github.com/spatialaudio/digital-signal-processing-lecture">https://github.com/spatialaudio/digital-signal-processing-lecture</a> </p></li><li><p>Python for Signal Processing (unipingco) <a href="https://github.com/unpingco/Python-for-Signal-Processing">https://github.com/unpingco/Python-for-Signal-Processing</a> </p></li><li><p>Audio for Deep Learning (남기현님) <a href="https://tykimos.github.io/2019/07/04/ISS_2nd_Deep_Learning_Conference_All_Together/">https://tykimos.github.io/2019/07/04/ISS_2nd_Deep_Learning_Conference_All_Together/</a> </p></li><li><p>오디오 전처리 작업을 위한 연습 (박수철님) <a href="https://github.com/scpark20/audio-preprocessing-practice">https://github.com/scpark20/audio-preprocessing-practice</a> </p></li><li><p>Musical Applications of Machine Learning <a href="https://mac.kaist.ac.kr/~juhan/gct634/">https://mac.kaist.ac.kr/~juhan/gct634/</a> </p></li><li><p>Awesome audio study materials for Korean (최근우님) <a href="https://github.com/keunwoochoi/awesome-audio-study-materials-for-korean">https://github.com/keunwoochoi/awesome-audio-study-materials-for-korean</a></p></li><li><p>T Academy(출처) <a href="https://tacademy.skplanet.com/live/player/onlineLectureDetail.action?seq=178">https://tacademy.skplanet.com/live/player/onlineLectureDetail.action?seq=178</a></p></li></ul><h2 id="1-Digital-Signal-Processing"><a href="#1-Digital-Signal-Processing" class="headerlink" title="1. Digital Signal Processing"></a>1. Digital Signal Processing</h2><p>소리 signal를 어떠한 데이터 타입으로 표현하며, 소리와 관련된 task를 해결하는데 있습니다. 그렇다면 소리는 어떠한 데이터를 가지고 있을까요?</p><h3 id="Sound"><a href="#Sound" class="headerlink" title="Sound?"></a>Sound?</h3><p>소리는 일반적으로 진동으로 인한 공기의 압축으로 생성됩니다. 그렇다면 압축이 얼마나 됬느냐에 따라서 표현되것이 바로 Wave(파동)인데요. 파동은 진동하며 공간/매질을 전파해 나가는 현상입니다. 질량의 이동은 없지만 에너지/운동량의 운반은 존재합니다.</p><p>Wave에서 저희가 얻을수 있는 정보는 크게 3가지가 있습니다.</p><ul><li>Phase(Degress of displacement) : 위상</li><li>Amplitude(Intensity) : 진폭</li><li>Frequency : 주파수</li></ul><h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h3><p>샘플링은 무엇일까요?? 아날로그 정보를 잘게 쪼개서 discrete한 디지털 정보로 표현해야합니다. 우리는 무한하게 쪼개서 저장할수 없으니, 어떤 기준을 가지고 아날로그 정보를 쪼개서 대표값을 취하게 됩니다.</p><p><code>Convert into a sqeuence of binary values via Sampling and Quantization</code></p><h3 id="1-1-Time-domain"><a href="#1-1-Time-domain" class="headerlink" title="1.1. Time domain"></a>1.1. Time domain</h3><p>시간을 기준으로 아날로그 시그널을 쪼개게 되는 것을 의미합니다. Sampling을 통하여 컴퓨터는 소리 sequence를 binary value로 받아드리게 됩니다.</p><p><strong>Sampling rate : 얼마나 잘게 쪼갤 것인가?</strong><br><br>잘개 쪼갤수록 원본 데이터와 거이 가까워지기 떄문에 좋지만 Data의 양이 증가하게 됩니다. 만약 너무 크게 쪼개게 된다면, 원본 데이터로 reconstruct하기 힘들어 질 것입니다.</p><p><strong>Sampling theorem</strong><br><br>샘플링 레이트가 최대 frequency의 2배 보다 커져야 한다는 것입니다.<br>$ f_{s} &gt; 2f_{m} $ 여기서 $f_{s}$는 sampling rate, 그리고 $f_{m}$은 maximum frequency를 말합니다.</p><ul><li>Nyqusit frequency = $f_{s}/2$, sampling rate의 절반입니다.</li></ul><p>일반적으로 Sampling은 인간의 청각 영역에 맞게 형성이 됩니다.</p><ul><li>Audio CD : 44.1 kHz(44100 sample/second)</li><li>Speech communication : 8 kHz(8000 sample/second)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># library load</span></span><br><span class="line"><span class="keyword">import</span> soundfile <span class="keyword">as</span> sf</span><br><span class="line"><span class="keyword">import</span> librosa</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">&quot;./wav/voice.wav&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파일 로드 방법 1</span></span><br><span class="line">y, sr = sf.read(filename, dtype=<span class="string">&#x27;int16&#x27;</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Sample Rate: &quot;</span>, sr)</span><br><span class="line">print(<span class="string">&quot;DATA: &quot;</span>, <span class="built_in">type</span>(y), y.shape, <span class="built_in">len</span>(y), y)</span><br><span class="line"></span><br><span class="line">dur = <span class="built_in">len</span>(y) / sr</span><br><span class="line">print(<span class="string">&quot;dur : &quot;</span>, dur)</span><br></pre></td></tr></table></figure><pre><code>Sample Rate:  16000DATA:  &lt;class &#39;numpy.ndarray&#39;&gt; (48944,) 48944 [ -9   1  -5 ... -20 -16 -24]dur :  3.059</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파일 로드 방법 2</span></span><br><span class="line">y, sr = librosa.load(filename, mono=<span class="literal">True</span>, sr=<span class="number">16000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Sample Rate: &quot;</span>, sr)</span><br><span class="line">print(<span class="string">&quot;DATA: &quot;</span>, <span class="built_in">type</span>(y), y.shape, y)</span><br><span class="line">dur = <span class="built_in">len</span>(y) / sr</span><br><span class="line">print(<span class="string">&quot;dur : &quot;</span>, dur)</span><br></pre></td></tr></table></figure><pre><code>Sample Rate:  16000DATA:  &lt;class &#39;numpy.ndarray&#39;&gt; (48944,) [-2.7465820e-04  3.0517578e-05 -1.5258789e-04 ... -6.1035156e-04 -4.8828125e-04 -7.3242188e-04]dur :  3.059</code></pre><h3 id="Resampling"><a href="#Resampling" class="headerlink" title="Resampling"></a>Resampling</h3><p>샘플링된 데이터를 다시금 더 높은 sampling rate 혹은 더 낮은 sampling rate로 다시 샘플링할수 있습니다. 이때는 일반적으로 interpolation(보간)을 할때는 low-pass filter를 사용합니다.(Windowed sinc function)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> IPython.display <span class="keyword">as</span> ipd</span><br><span class="line">y_8k = librosa.resample(y, sr, <span class="number">8000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipd.Audio(y_8k, rate=<span class="number">8000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(y_8k)</span><br></pre></td></tr></table></figure><pre><code>24472</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># duration</span></span><br><span class="line"><span class="built_in">len</span>(y_8k)/<span class="number">8000</span></span><br></pre></td></tr></table></figure><pre><code>3.059</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_2k = librosa.resample(y, sr, <span class="number">4000</span>)</span><br><span class="line">ipd.Audio(y_2k, rate=<span class="number">2000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(y_2k)</span><br></pre></td></tr></table></figure><pre><code>12236</code></pre><h3 id="Nomalization-amp-Quantization"><a href="#Nomalization-amp-Quantization" class="headerlink" title="Nomalization &amp; Quantization"></a>Nomalization &amp; Quantization</h3><p>시간의 기준이 아닌 실제 amplitude의 real valued 를 기준으로 시그널의 값을 조절합니다. Amplitude를 이산적인 구간으로 나누고, signal 데이터의 Amplitude를 반올림하게 됩니다.</p><p>그렇다면 이산적인 구간은 어떻게 나눌수 있을까요?, bit의 비트에 의해서 결정됩니다. </p><ul><li>B bit의 Quantization : $-2^{B-1}$ ~ $2^{B-1}-1$</li><li>Audio CD의 Quantization (16 bits) : $-2^{15}$ ~ $2^{15}-1$</li><li>위 값들은 보통 -1.0 ~ 1.0  영역으로 scaling되기도 합니다.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalize</span></span><br><span class="line">normed_wav = y / <span class="built_in">max</span>(np.<span class="built_in">abs</span>(y))</span><br><span class="line">ipd.Audio(normed_wav, rate=sr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#quantization 하면 음질은 떨어지지만 light한 자료형이 된다.</span></span><br><span class="line">Bit = <span class="number">8</span></span><br><span class="line">max_value = <span class="number">2</span> ** (Bit<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">quantized_8_wav = normed_wav * max_value</span><br><span class="line">quantized_8_wav = np.<span class="built_in">round</span>(quantized_8_wav).astype(<span class="built_in">int</span>)</span><br><span class="line">quantized_8_wav = np.clip(quantized_8_wav, -max_value, max_value<span class="number">-1</span>)</span><br><span class="line">ipd.Audio(quantized_8_wav, rate=sr)</span><br></pre></td></tr></table></figure><h3 id="mu-law-encoding"><a href="#mu-law-encoding" class="headerlink" title="mu-law encoding"></a>mu-law encoding</h3><p>사람의 귀는 소리의 amplitude에 대해 log적으로 반응합니다. 즉, 작은소리의 차이는 잘잡아내는데 반해 소리가 커질수록 그 차이를 잘 느끼지 못합니다. 이러한 특성을 wave값을 표현하는데 반영해서 작은값에는 높은 분별력(high resolution)을, 큰값끼리는 낮은 분별력(low resolution)을 갖도록 합니다</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mu_law</span>(<span class="params">x, mu=<span class="number">255</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.sign(x) * np.log(<span class="number">1</span> + mu * np.<span class="built_in">abs</span>(x)) / np.log(<span class="number">1</span> + mu)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1000</span>)</span><br><span class="line">x_mu = mu_law(x)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=[<span class="number">6</span>, <span class="number">4</span>])</span><br><span class="line">plt.plot(x)</span><br><span class="line">plt.plot(x_mu)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-8/output_18_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wav_mulaw = mu_law(normed_wav)</span><br><span class="line">ipd.Audio(wav_mulaw, rate=sr)</span><br></pre></td></tr></table></figure><blockquote><p>to be continued…<br>뒷장에서 계속됩니다.</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      <category domain="https://ikarus-999.github.io/tags/Audio/">Audio</category>
      
      
      <comments>https://ikarus-999.github.io/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-8/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문과-준비7</title>
      <link>https://ikarus-999.github.io/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/</link>
      <guid>https://ikarus-999.github.io/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/</guid>
      <pubDate>Sun, 29 Nov 2020 17:54:54 GMT</pubDate>
      
      <description>딥러닝 시작해보기-7
합성곱(Convolution)
 Convolution?


 정의
합성곱 연산은 두 함수 f, g 가운데 하나의 함수를 반전(reverse), 전이(shift)시킨 다음, 다른 하나의 함수와 곱한 결과를 적분하는 것을 의미한다. 이를 수학 기호로 표시하면 다음과 같다.


 또한 g 함수 대신에 f 함수를 반전, 전이 시키는 경우 다음과 같이 표시할 수도 있다. 이 두 연산은 형태는 다르지만 같은 결과값을 갖는다.


 위의 적분에서 적분 구간은 함수 f와 g가 정의된 범위에 따라서 달라진다.

 또한 두 확률</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-7"><a href="#딥러닝-시작해보기-7" class="headerlink" title="딥러닝 시작해보기-7"></a>딥러닝 시작해보기-7</h2><h3 id="합성곱-Convolution"><a href="#합성곱-Convolution" class="headerlink" title="합성곱(Convolution)"></a>합성곱(Convolution)</h3><p>  Convolution?<br>  <img src="/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_1.png" alt="conv">  </p><p>  <strong>정의</strong><br>  합성곱 연산은 두 함수 f, g 가운데 하나의 함수를 반전(reverse), 전이(shift)시킨 다음, 다른 하나의 함수와 곱한 결과를 적분하는 것을 의미한다. 이를 수학 기호로 표시하면 다음과 같다.<br>  <img src="/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/1.PNG" alt="1"></p><p>  또한 g 함수 대신에 f 함수를 반전, 전이 시키는 경우 다음과 같이 표시할 수도 있다. 이 두 연산은 형태는 다르지만 같은 결과값을 갖는다.<br>  <img src="/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/2.PNG" alt="2"></p><p>  위의 적분에서 적분 구간은 함수 f와 g가 정의된 범위에 따라서 달라진다.</p><p>  또한 두 확률 변수 X와 Y가 있을 때 각각의 확률 밀도 함수를 f와 g라고 하면, X+Y의 확률 밀도 함수는 $$f * g$$로 표시할 수 있다. – <a href="https://ko.wikipedia.org/wiki/%ED%95%A9%EC%84%B1%EA%B3%B1">Wikipedia</a></p><blockquote><p>무엇인지 모르겠죠? 쉽게 말하자면<br>  기존 MLP에서는 이미지가 살짝이라도 회전이 되거나 위치 이동이 있다면 신경망 자체를 다시 학습해야 하지만<br>  CNN은 이미지의 변화가 있어도 재학습 없어도 가능함.</p></blockquote><p>  <img src="/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_2.PNG" alt="conv2"><br>  모든 pixel을 비교할 게 절대 아님. Feature 추출에 중점을 둠.</p><p>  <img src="/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_3.PNG" alt="conv3"><br>  $$C_in$$*$$C_out$$ 번의 합성곱 연산<br>  bias는 하나의 벡터</p><p>  Filter(kernel)의 크기에 따라 영상의 크기가 줄어드는 문제점을 해결하기 위해 padding을 쓴다.<br>  <img src="/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_4.PNG" alt="conv4"><br>  크기가 (2N + 1)인 커널에 상하좌우에 N개 Zero padding을 해주면 된다.</p><p>  Sliding Window 방식으로 커널이 이동되는데 그 크기를 조절하려면 Stride를 쓴다.<br>  너무 크면 출력 Feature Map이 과도하게 줄어드는 경우가 발생한다.</p><h3 id="보다-효율적인-Conv-연산을-하기-위해서는-1x1-Conv를-넣는다"><a href="#보다-효율적인-Conv-연산을-하기-위해서는-1x1-Conv를-넣는다" class="headerlink" title="보다 효율적인 Conv 연산을 하기 위해서는 1x1 Conv를 넣는다"></a>보다 효율적인 Conv 연산을 하기 위해서는 1x1 Conv를 넣는다</h3><p>  연산량, 파라미터 개수를 줄이기 위해 BottleNeck 구조를 활용한다.<br>  하필 1x1 ??<br>  <img src="/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_5.PNG" alt="conv5"><br>  3x3 filter 한개와 1x1 + 3x3 parameter 비교</p><p>  그래도 모르겠다면??</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random((<span class="number">3</span>, <span class="number">3</span>)).shape == (np.random((<span class="number">3</span>, <span class="number">1</span>)) * np.random((<span class="number">1</span>, <span class="number">3</span>)).shape)</span><br><span class="line">&gt;&gt; <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># keras</span></span><br><span class="line"><span class="comment"># k - kernel_size(ex. 3, 5, 7...)</span></span><br><span class="line"><span class="comment"># n_filter - number of filters/channels 필터 갯수</span></span><br><span class="line">conv1_1 = Conv(n_filters, (<span class="number">1</span>, k))(input_1)</span><br><span class="line">conv1_2 = Conv(n_filters, (k, <span class="number">1</span>))(conv1_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 왜 병목?</span></span><br><span class="line">conv2 = Conv2D(<span class="number">96</span>, (<span class="number">1</span>, <span class="number">1</span>), ...)(conv1) </span><br><span class="line"><span class="comment"># 줄였다가(receptive Field는 그대로, Feature map을 미리 줄임.)</span></span><br><span class="line">conv3 = Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), ...)(conv2) </span><br><span class="line">conv4 = Conv2D(<span class="number">128</span>, (<span class="number">1</span>, <span class="number">1</span>), ...)(conv3) <span class="comment"># 다시 늘림</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>  항등행렬을 떠올리면 이해가 갈것이다.</p><h3 id="CNN-만들었는데-너무-느리네-어떻게-하면-빠르게-할-수-있을까…"><a href="#CNN-만들었는데-너무-느리네-어떻게-하면-빠르게-할-수-있을까…" class="headerlink" title="CNN 만들었는데 너무 느리네? 어떻게 하면 빠르게 할 수 있을까…"></a>CNN 만들었는데 너무 느리네? 어떻게 하면 빠르게 할 수 있을까…</h3><p>  Conv filter를 더 넓게 쓴다. –&gt; GPU 연산이 쉬워진다  </p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이렇게 되어있는 걸</span></span><br><span class="line">conv = Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), ...)(conv)</span><br><span class="line">conv = Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), ...)(conv)</span><br><span class="line"><span class="comment"># 아래처럼 바꾼다.</span></span><br><span class="line">conv = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), ...)(conv)</span><br></pre></td></tr></table></figure><blockquote><p>GPU는 병렬로 처리하기 때문에 필터 갯수를 늘리면 더욱 빨라진다.<br>  쉽게 말하면 96개씩 두번보다 128개씩 한번이 더 빠르다.  </p></blockquote><p>  설명.</p><ol><li><p>96 // 3 = 32</p></li><li><p>2- layer을 1- layer로 바꿀땐</p></li><li><p>32 // 2 = 16</p></li><li><p>16^0.5 = 4</p></li><li><p>4 * 32 = 128</p><p>또 다른 방법<br>각 채널에서 별도의 2d conv를 하는 방법<br>in_channels * channel_multipliter 중간채널은 연결되고 1x1 conv로 out_channels에 매핑</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keras</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> SeparableConv2D</span><br><span class="line">net = SeparableConv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>))(net)</span><br><span class="line"><span class="comment"># it&#x27;s almost 1:1 similar to the simple Keras Conv2D layer</span></span><br></pre></td></tr></table></figure><p>출처 :<br><a href="https://colab.research.google.com/drive/1i0Fwh-d8kF05o4QRfJG5dZt_P7G85MCS">source1</a><br><a href="https://towardsdatascience.com/speeding-up-convolutional-neural-networks-240beac5e30f">source2</a></p></li></ol>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>R 끄적여보기</title>
      <link>https://ikarus-999.github.io/2020/11/29/R-%EB%81%84%EC%A0%81%EC%97%AC%EB%B3%B4%EA%B8%B0/</link>
      <guid>https://ikarus-999.github.io/2020/11/29/R-%EB%81%84%EC%A0%81%EC%97%AC%EB%B3%B4%EA%B8%B0/</guid>
      <pubDate>Sun, 29 Nov 2020 02:54:10 GMT</pubDate>
      
      <description>R 시작해보기
R 변수
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="R-시작해보기"><a href="#R-시작해보기" class="headerlink" title="R 시작해보기"></a>R 시작해보기</h2><h3 id="R-변수"><a href="#R-변수" class="headerlink" title="R 변수"></a>R 변수</h3>  <figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">v1 &lt;-<span class="built_in">c</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="literal">NA</span>) <span class="comment"># 벡터</span></span><br><span class="line"></span><br><span class="line">v2 &lt;- <span class="string">&#x27;a&#x27;</span> <span class="comment"># 동적 변수에 문자 assign</span></span><br><span class="line"></span><br><span class="line">v2 = <span class="string">&#x27;a&#x27;</span> <span class="comment"># 정적 변수</span></span><br><span class="line"></span><br><span class="line">rm(v1, v2) <span class="comment"># 변수 지우기</span></span><br><span class="line"></span><br><span class="line">v1 &lt;- <span class="built_in">c</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">v2 &lt;- <span class="built_in">c</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="literal">NA</span>) <span class="comment"># Error</span></span><br><span class="line">v3 &lt;- <span class="built_in">c</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="literal">NULL</span>) <span class="comment"># ok</span></span><br><span class="line">mean(v1)</span><br><span class="line">mean(v2) <span class="comment"># Error</span></span><br><span class="line">mean(v3) <span class="comment"># ok</span></span><br><span class="line"><span class="literal">TRUE</span> | <span class="literal">TRUE</span> <span class="comment"># TRUE</span></span><br><span class="line"><span class="literal">TRUE</span> | <span class="literal">FALSE</span> <span class="comment"># TRUE</span></span><br><span class="line">!<span class="literal">TRUE</span> <span class="comment"># 뒤집기 연산</span></span><br><span class="line">!<span class="literal">FALSE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 범주형 변수</span></span><br><span class="line">key &lt;- factor(<span class="string">&#x27;m&#x27;</span>, <span class="built_in">c</span>(<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;f&#x27;</span>))</span><br><span class="line"></span><br><span class="line">nlevels(key)</span><br><span class="line">levels(key)[<span class="number">1</span>] <span class="comment"># 1레벨 추출</span></span><br><span class="line">levels(key)[<span class="number">2</span>] <span class="comment"># 종류 추출</span></span><br><span class="line"></span><br><span class="line">levels(key) &lt;- <span class="built_in">c</span>(<span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;f&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#vector</span></span><br><span class="line">x &lt;- <span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">x</span><br><span class="line">y &lt;- <span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="string">&quot;a&quot;</span>)</span><br><span class="line">y</span><br><span class="line"><span class="built_in">names</span>(x) &lt;- <span class="built_in">c</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>,<span class="string">&quot;d&quot;</span>,<span class="string">&quot;e&quot;</span>)</span><br><span class="line">x</span><br><span class="line">x[<span class="number">1</span>]</span><br><span class="line">x[<span class="number">2</span>]</span><br><span class="line">x[-<span class="number">1</span>]</span><br><span class="line">x[-<span class="number">2</span>]</span><br><span class="line">x[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">x[<span class="number">3</span>:<span class="number">5</span>]</span><br><span class="line">x[<span class="string">&quot;a&quot;</span>]</span><br><span class="line"><span class="built_in">names</span>(x)[<span class="number">2</span>]</span><br><span class="line"><span class="built_in">length</span>(x)</span><br><span class="line">nrow(x)</span><br><span class="line">NROW(x)</span><br><span class="line"></span><br><span class="line">seq(<span class="number">1</span>,<span class="number">10</span>) <span class="comment"># range</span></span><br><span class="line">seq(<span class="number">1</span>:<span class="number">10</span>)</span><br><span class="line">seq(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">seq(<span class="number">1</span>,<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">NROW(seq(<span class="number">1</span>,<span class="number">10</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">rep</span>(<span class="number">1</span>:<span class="number">10</span>, times=<span class="number">2</span>) <span class="comment">#반복</span></span><br><span class="line"><span class="built_in">rep</span>(<span class="number">1</span>:<span class="number">10</span>, each=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">rep</span>(<span class="number">1</span>:<span class="number">10</span>, each=<span class="number">2</span>, times=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#list</span></span><br><span class="line">x &lt;- <span class="built_in">list</span>(name=<span class="string">&quot;jo&quot;</span>, he=<span class="string">&quot;999&quot;</span>)</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line">x$name</span><br><span class="line">x$he</span><br><span class="line"></span><br><span class="line">x[<span class="number">1</span>]</span><br><span class="line">x[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">x[[<span class="number">1</span>]] <span class="comment"># 인덱싱</span></span><br><span class="line"></span><br><span class="line">x[[<span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line">y &lt;- <span class="built_in">list</span>(name=<span class="string">&quot;k&quot;</span>, he=<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">y</span><br><span class="line">y[[<span class="number">1</span>]]</span><br><span class="line">y[[<span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line">z &lt;- <span class="built_in">list</span>(<span class="built_in">c</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>), <span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">z</span><br><span class="line"></span><br><span class="line">z[[<span class="number">1</span>]][<span class="number">2</span>]</span><br><span class="line">z[[<span class="number">2</span>]][-<span class="number">1</span>]</span><br><span class="line">z[[<span class="number">2</span>]][<span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line"><span class="built_in">length</span>(z[[<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">length</span>(z[[<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#matrix</span></span><br><span class="line">matrix(<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>), nrow=<span class="number">2</span>)</span><br><span class="line">matrix(<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>), nrow=<span class="number">5</span>)</span><br><span class="line">m &lt;- matrix(<span class="number">1</span>:<span class="number">9</span>, nrow=<span class="number">3</span>, dimnames=<span class="built_in">list</span>(<span class="built_in">c</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>),<span class="built_in">c</span>(<span class="string">&quot;A&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;C&quot;</span>)))</span><br><span class="line">m</span><br><span class="line"></span><br><span class="line">rownames(m)</span><br><span class="line">colnames(m)</span><br><span class="line"></span><br><span class="line">m[<span class="number">1</span>:<span class="number">2</span>,]</span><br><span class="line">m[,<span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line">m[<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">m[<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">m[-<span class="number">1</span>,]</span><br><span class="line">m[,-<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">m[-<span class="number">1</span>,-<span class="number">2</span>]</span><br><span class="line">m[-<span class="number">1</span>,-<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">m[m[,<span class="number">3</span>]&gt;<span class="number">8</span>,]</span><br><span class="line"><span class="comment">#m[m[2:3]2]</span></span><br><span class="line"></span><br><span class="line">m[m[,<span class="number">3</span>]&gt;=<span class="number">8</span>,<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>)]</span><br><span class="line">m[<span class="built_in">c</span>(<span class="number">2</span>,<span class="number">3</span>),<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>)]</span><br><span class="line">m[m[,<span class="number">3</span>]&gt;=<span class="number">8</span>,m[<span class="number">1</span>,]&lt;<span class="number">5</span>]</span><br><span class="line">m</span><br><span class="line">m[<span class="built_in">c</span>(<span class="number">2</span>,<span class="number">3</span>),<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">3</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#array</span></span><br><span class="line">array(<span class="number">1</span>:<span class="number">12</span>, <span class="built_in">dim</span>=<span class="built_in">c</span>(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">x &lt;- array(<span class="number">1</span>:<span class="number">12</span>, <span class="built_in">dim</span>=<span class="built_in">c</span>(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">x</span><br><span class="line">x[,,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">m[<span class="number">1</span>,]</span><br><span class="line"></span><br><span class="line">m1 &lt;- matrix(<span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>), nrow=<span class="number">5</span>)</span><br><span class="line">m1</span><br><span class="line"></span><br><span class="line">name &lt;- <span class="built_in">c</span>(<span class="string">&quot;kim&quot;</span>, <span class="string">&quot;lee&quot;</span>, <span class="string">&quot;park&quot;</span>)</span><br><span class="line">age &lt;- <span class="built_in">c</span>(<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>)</span><br><span class="line">gender &lt;-factor(<span class="built_in">c</span>(<span class="string">&quot;M&quot;</span>,<span class="string">&quot;F&quot;</span>,<span class="string">&quot;M&quot;</span>))</span><br><span class="line">df &lt;- data.frame(name, age, gender)</span><br><span class="line">df</span><br><span class="line">str(df)</span><br><span class="line">df[df$age &gt;= <span class="number">20</span> &amp; gender == <span class="string">&quot;F&quot;</span>, <span class="built_in">c</span>(<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)]</span><br><span class="line">df[df$age &gt;= <span class="number">20</span> &amp; gender == <span class="string">&quot;F&quot;</span>, <span class="string">&quot;name&quot;</span>]</span><br><span class="line"></span><br><span class="line">df[df[,<span class="number">2</span>]&gt;=<span class="number">20</span> &amp; df[,<span class="number">3</span>] == <span class="string">&quot;F&quot;</span>, <span class="built_in">c</span>(<span class="number">1</span>,<span class="number">2</span>)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Code/">Code</category>
      
      <category domain="https://ikarus-999.github.io/categories/Code/R/">R</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/R/">R</category>
      
      <category domain="https://ikarus-999.github.io/tags/coding/">coding</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/29/R-%EB%81%84%EC%A0%81%EC%97%AC%EB%B3%B4%EA%B8%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문과-준비6</title>
      <link>https://ikarus-999.github.io/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/</link>
      <guid>https://ikarus-999.github.io/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/</guid>
      <pubDate>Fri, 27 Nov 2020 13:54:12 GMT</pubDate>
      
      <description>딥러닝 시작해보기-6
💥Remind!! 딥러닝에 비선형 활성화 함수를 사용하는 이유
 선형 함수로는 XOR과 같은 non-linear한 문제는 해결이 안됨;;

그러면 Hidden Layer를 늘리면 되지 않을까?
$$f(ax+by) = af(x) + bf(y)$$ 라는 특징 때문에
N-layer 깊이를 아무리 쌓아도 1-Layer로 동작함.

최적화(Opt) 알고리즘
 * 경사하강법(GD)
   $$\theta = \theta - \eta \nabla_\theta S(\theta)$$ 
   
   Network의 paramet</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-6"><a href="#딥러닝-시작해보기-6" class="headerlink" title="딥러닝 시작해보기-6"></a>딥러닝 시작해보기-6</h2><h3 id="💥Remind-딥러닝에-비선형-활성화-함수를-사용하는-이유"><a href="#💥Remind-딥러닝에-비선형-활성화-함수를-사용하는-이유" class="headerlink" title="💥Remind!! 딥러닝에 비선형 활성화 함수를 사용하는 이유"></a>💥Remind!! 딥러닝에 비선형 활성화 함수를 사용하는 이유</h3><p>  선형 함수로는 XOR과 같은 non-linear한 문제는 해결이 안됨;;</p><blockquote><p>그러면 Hidden Layer를 늘리면 되지 않을까?<br>  $$f(ax+by) = af(x) + bf(y)$$ 라는 특징 때문에<br>  N-layer 깊이를 아무리 쌓아도 1-Layer로 동작함.</p></blockquote><h3 id="최적화-Opt-알고리즘"><a href="#최적화-Opt-알고리즘" class="headerlink" title="최적화(Opt) 알고리즘"></a>최적화(Opt) 알고리즘</h3><ul><li><p>경사하강법(GD)<br>$$\theta = \theta - \eta \nabla_\theta S(\theta)$$  </p><p>Network의 parameter=$$\theta $$ 로 할때 손실함수 $$J(\theta)$$의 값을 최소화하기 위해 기울기<br>$$\nabla J(\theta)$$를 이용하는 방법<br>GD에서는 Gradient의 반대 방향으로 일정 크기(lr)만큼 이동하는 것을 반복하여 loss function의 값을 최소화 하는 $$\theta$$의 값을 찾음,  </p></li><li><p>lr $$\eta$$ 는 보통 1e-3 ~ 1e-4 사이에서 사용함.<br>너무 크면 global minimum을 지나치고 너무 작으면 Local Minimum에 빠짐.</p></li></ul><ul><li><p>확률적 경사하강법(SGD)<br>전체 Training set을 사용하는 것을 batch Gradient Descent, 계산량이 많아지는 것을 방지하기 위해<br>mini-batch에 대해서만 손실함수를 계산하는 확률적 GD를 사용함.<br>같은 시간에 더 많은 step를 갈 수 있음, 여러번 반복할 경우 batch의 결과와 비슷함</p></li><li><p>GD vs SGD<br>GD : 확실한데 너무 느림 | SGD : 조금 헤메지만 빠름</p></li><li><p>Momentum : 현재 Gradient를 통해 이동하는 방향과 별개로 과거의 이동방식을 기억하면서 일종의 관성을 주는 방식</p></li><li><p>AdaGrad(Adaptive Gradient)</p><ol><li>많이 변화했던 변수들은 step size를 작게 하는 것<br>자주 등장하거나 변화를 많이 한 변수들은 optimum에 가까이 있을 확률이 높기 때문에 작은 크기로 이동하면서 미세조절</li><li>적게 변화한 변수들은 많이 이동해야할 확률이 높기 때문에 먼저 빠르게 loss값을 줄이는 방식으로 이동하는 방식<br>학습을 계속 진행하면 step size가 너무 줄어드는 단점이 있음.<br><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_05.PNG" alt="AdaGrad"></li></ol></li><li><p>RMSProp<br>합을 지수평균으로 대체하여 Adagrad의 단점을 해결<br>G가 무한정 커지지는 않으면서 최근 변화량의 변수간 상대적인 크기 차이는 유지할 수 있음.<br><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_04.PNG" alt="RMsprop"></p></li><li><p>Adam<br>Momentum + RMSProp</p><ol><li>지금까지 계산해온 기울기의 지수평균을 저장</li><li>rmsprop과 유사하게 Gradient의 제곱값의 지수평균을 저장<br><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_06.PNG" alt="Adam"></li></ol></li></ul><h3 id="Overfitting-과적합"><a href="#Overfitting-과적합" class="headerlink" title="Overfitting(과적합)"></a>Overfitting(과적합)</h3><p>  Training Set의 지엽적인 특성까지 반영해 Variance High로 Training되어서 Training Set을 암기해버리는 현상<br>  <em>Test Set을 잘 예측하지 못함</em><br>  주로 표현력이 높은 모델, 즉 파라미터가 많은 모델에 발생</p><ol><li>정규화(Regularization)</li></ol><ul><li>손실함수에 가중치의 크기를 포함</li><li>가중치가 작아지도록 학습한다는 것은 Outlier(Noise)의 영향을 적게 받음  </li></ul><h4 id="L2-정규화"><a href="#L2-정규화" class="headerlink" title="L2 정규화"></a>L2 정규화</h4><p><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_08.PNG" alt="L2"></p><p><em>Rigde Regression</em></p><h4 id="L1-정규화"><a href="#L1-정규화" class="headerlink" title="L1 정규화"></a>L1 정규화</h4><p><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_09.PNG" alt="L1"><br>Sparse Model에 알맞음.. 작은 가중치들이 거의 0으로 수렴하여 몇개의 중요한 가중치들만 남음. </p><p><em>Lasso Regression</em></p><p>미분 불가능한 점이 있기 때문에 Gradient-Base Learning에는 주의..<br><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_10.PNG" alt="L1주의점"></p><h4 id="DropOut"><a href="#DropOut" class="headerlink" title="DropOut"></a>DropOut</h4><p>각 레이어의 일정 비율로 뉴런의 출력 값을 0으로 만들어 나머지 뉴런들로 학습하는 방법<br>과적합을 효과적으로 예방 가능(Network 내부의 Ensemble 학습으로 볼 수 있음)</p><p>역전파는 ReLU처럼 동작<br>Forward Propagation때 시그널을 통과시킨 뉴런은 Backward때도 통과시킴<br>drop된 뉴런은 Backward Propagation때도 시그널 차단</p><p>반면, TEST때는 모든 뉴런에 신호를 전달함</p><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>학습하는 이전 층의 파라미터 변화로 현재층의 입력 분포가 바뀌는 현상을 내부 공분산 변화(Internal Covariate Shift)<br>이전 층의 작은 파라미터 변화가 증폭되어 뒷 레이어에 큰 영향을 받음.<br>그래서…</p><p>BN(2015)</p><ul><li>Gradient Vanishing, Exploding을 방지하는 대표적인 방법</li><li>직접적인 방법임.</li><li>Training 과정 자체를 안정화시켜 학습속도를 가속화</li><li>평균과 분산을 조절하는 과정이 <em>NN 안에 포함</em> 되어 있다는 것이 핵심적</li></ul><p>Training할때<br>각 Mini Batch마다 $$\gamma$$ 와 $$\beta$$를 구하고 저장해 둠</p><p>Test할때<br>구했던 $$\gamma$$ 와 $$\beta$$의 평균을 사용</p><p><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_11.PNG" alt="BN"></p><h4 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h4><p>일종의 Regularization작업, 데이터가 적을 때 사용하면 매우 효과적<br>즉 데이터 변형</p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문과-준비5</title>
      <link>https://ikarus-999.github.io/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/</link>
      <guid>https://ikarus-999.github.io/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/</guid>
      <pubDate>Wed, 25 Nov 2020 12:49:48 GMT</pubDate>
      
      <description>딥러닝 시작해보기-5
선형대수 배워보기(행렬을 아무리 곱하고 더해도 선모양)
Scala : 크기만 존재하는 양
Vector : 속도, 위치이동, 힘, 공간뒤틀림과 같이 크기와 방향이 모두 존재하는 양


Norm ? n차원 벡터 $$\vec{x} = (x_1, x_2, \cdots x_n)$$
Norm $$\lVert x \rVert = \sqrt{x_1^1 + x_2^2 + \cdots + x_n^2}$$

“원점 O에서 점\(x_1, x_2, \cdots, x_n\) 까지의 거리”

내적 ? Inner product, Dot p</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-5"><a href="#딥러닝-시작해보기-5" class="headerlink" title="딥러닝 시작해보기-5"></a>딥러닝 시작해보기-5</h2><h3 id="선형대수-배워보기-행렬을-아무리-곱하고-더해도-선모양"><a href="#선형대수-배워보기-행렬을-아무리-곱하고-더해도-선모양" class="headerlink" title="선형대수 배워보기(행렬을 아무리 곱하고 더해도 선모양)"></a>선형대수 배워보기(행렬을 아무리 곱하고 더해도 선모양)</h3><h4 id="Scala-크기만-존재하는-양"><a href="#Scala-크기만-존재하는-양" class="headerlink" title="Scala : 크기만 존재하는 양"></a>Scala : 크기만 존재하는 양</h4><h4 id="Vector-속도-위치이동-힘-공간뒤틀림과-같이-크기와-방향이-모두-존재하는-양"><a href="#Vector-속도-위치이동-힘-공간뒤틀림과-같이-크기와-방향이-모두-존재하는-양" class="headerlink" title="Vector : 속도, 위치이동, 힘, 공간뒤틀림과 같이 크기와 방향이 모두 존재하는 양"></a>Vector : 속도, 위치이동, 힘, 공간뒤틀림과 같이 크기와 방향이 모두 존재하는 양</h4><p><img src="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/slido51_1.PNG" alt="스칼라와벡터"></p><h4 id="Norm-n차원-벡터-vec-x-x-1-x-2-cdots-x-n"><a href="#Norm-n차원-벡터-vec-x-x-1-x-2-cdots-x-n" class="headerlink" title="Norm ? n차원 벡터 $$\vec{x} = (x_1, x_2, \cdots x_n)$$"></a>Norm ? n차원 벡터 $$\vec{x} = (x_1, x_2, \cdots x_n)$$</h4><p>Norm $$\lVert x \rVert = \sqrt{x_1^1 + x_2^2 + \cdots + x_n^2}$$</p><blockquote><p>“원점 O에서 점\(x_1, x_2, \cdots, x_n\) 까지의 거리”</p></blockquote><h4 id="내적-Inner-product-Dot-product"><a href="#내적-Inner-product-Dot-product" class="headerlink" title="내적 ?  Inner product, Dot product"></a>내적 ?  Inner product, Dot product</h4><h4 id="행렬끼리-곱할-때는-차원을-주의한다"><a href="#행렬끼리-곱할-때는-차원을-주의한다" class="headerlink" title="행렬끼리 곱할 때는 차원을 주의한다."></a>행렬끼리 곱할 때는 차원을 주의한다.</h4><blockquote><p>A(m, n) * B(n, m) 만 가능</p></blockquote><h4 id="Transpose-전치행렬-행과-열을-뒤바꿈"><a href="#Transpose-전치행렬-행과-열을-뒤바꿈" class="headerlink" title="Transpose: 전치행렬(행과 열을 뒤바꿈)"></a>Transpose: 전치행렬(행과 열을 뒤바꿈)</h4><blockquote><p>A.T</p></blockquote><h3 id="numpy-연산-Element-wise-operation"><a href="#numpy-연산-Element-wise-operation" class="headerlink" title="numpy 연산(Element-wise operation)"></a>numpy 연산(Element-wise operation)</h3><blockquote><p>np.dot(x, y) (aka 내적, dot-product)와  x * y(element-wise)는 서로 다름.</p></blockquote><h3 id="numpy-비교-논리연산-element-wise-operation"><a href="#numpy-비교-논리연산-element-wise-operation" class="headerlink" title="numpy 비교, 논리연산(element-wise operation)"></a>numpy 비교, 논리연산(element-wise operation)</h3><h3 id="numpy-Reductions"><a href="#numpy-Reductions" class="headerlink" title="numpy Reductions"></a>numpy Reductions</h3><blockquote><p>argmax() : 최대값있는 인덱스를 리턴, argmin() : 최소값의 인덱스 리턴</p></blockquote><h3 id="np-all-np-any"><a href="#np-all-np-any" class="headerlink" title="np.all, np.any?"></a>np.all, np.any?</h3><blockquote><p>ALL : Array내 모든 값이 TRUE인가?<br>  any : Array내 값이 하나라도 TRUE인가?</p></blockquote><h3 id="np-mean-np-median-np-std-등-통계함수-사용-가능"><a href="#np-mean-np-median-np-std-등-통계함수-사용-가능" class="headerlink" title="np.mean, np.median, np.std 등 통계함수 사용 가능"></a>np.mean, np.median, np.std 등 통계함수 사용 가능</h3><h4 id="딥러닝에-대한-환상"><a href="#딥러닝에-대한-환상" class="headerlink" title="딥러닝에 대한 환상"></a>딥러닝에 대한 환상</h4><ol><li><p>복잡한 문제도 층을 깊고 넓게 쌓으면 해결된다 –&gt; Gradient Vanhshing, Initialize fault 으하하핰ㅋㅋㅋ</p></li><li><p>$$Sigmoid(z) = \frac{1} {1 + e^{-z}}$$<br>Sigmoid 도함수의 최대값은 1/4 … –&gt; 그래서 Gradient Vanishing 나는거임 ㅇㅇ</p></li></ol><h4 id="가중치-초기화"><a href="#가중치-초기화" class="headerlink" title="가중치 초기화"></a>가중치 초기화</h4><ol><li><p>초기화의 중요성<br>$$t = wx+b$$ 에서 w가 100, b가 50이라면 x가 0.01이더라도 t는 51이 됨<br>역전파때 sigmoid 함수 통과시키면 $$\sigma’ (51)$$ 리턴됨<br>하지만 t가 5만 넘어도 $$\sigma (t)$$ 는 0에 수렴 –&gt; 이것이 바로 Gradient Vanishing…</p></li><li><p>그래서 입력층의 가중치w를 모두 0으로 리셋!<br>Forward Propagation때 두번째 층 뉴런에 모두 같은 값이 전달됨<br>Backward Propagation때 두째 층 가중치가 모두 똑같이 업데이트 ==&gt; 신경망 표현력 제한</p></li><li><p>Bias는 0으로 초기화하는게 일반적으로 효율적</p></li></ol><h4 id="가중치-초기화-2"><a href="#가중치-초기화-2" class="headerlink" title="가중치 초기화 2"></a>가중치 초기화 2</h4><ol><li><p>표준 정규분포를 이용한 가중치 초기화<br>Sigmoid함수의 출력값이 극단적으로(0 or 1)에 치우치는 현상 –&gt; Gradient Vanishing</p></li><li><p>표준편차를 0.01로 하는 정규분포로 초기화<br>가중치가 모여 있음 =&gt; 기울기 소실 문제 어느정도 완화됨</p></li></ol><h4 id="가중치-초기화-3"><a href="#가중치-초기화-3" class="headerlink" title="가중치 초기화 3"></a>가중치 초기화 3</h4><p>  Xavier초기화 방법(2010)<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n_input, n_output) / (n_input) ** <span class="number">0.5</span></span><br></pre></td></tr></table></figure><br>  Sigmoid와 같은 S자 함수의 경우 출력값들이 정규분포 형태이어야 안정적 학습 가능  </p><ul><li><p>Sigmoid function과 Xavier Init방법을 사용했을 경우 그래프<br><img src="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_01.PNG" alt="sigmodi"></p></li><li><p>ReLU 계열 함수에는 적절하지 않음<br>layer를 거쳐갈 수록 0에 수렴(converge)</p><p><img src="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_02.PNG" alt="reluNo"></p><h4 id="가중치-초기화-4"><a href="#가중치-초기화-4" class="headerlink" title="가중치 초기화 4"></a>가중치 초기화 4</h4><p>He 초기화 방법(2015)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n_input, n_output) / (n_input / <span class="number">2</span>) ** <span class="number">0.5</span></span><br></pre></td></tr></table></figure><p>RELU + He init –&gt; 10 layer를 거쳐도 표준편차가 0으로 수렴하지 않음<br><img src="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_03.PNG" alt="relu"></p></li></ul><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul><li>가중치 초기화는 너무나 중요함</li><li>tanh의 경우 Xavier Init 방법이 효율적</li><li>ReLU계열 함수에는 He Init 방법이 효율적</li><li>최근엔 대부분 He Init를 주로 사용</li></ul>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/Linear-Algebra/">Linear Algebra</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문과-준비4</title>
      <link>https://ikarus-999.github.io/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/</link>
      <guid>https://ikarus-999.github.io/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/</guid>
      <pubDate>Tue, 24 Nov 2020 13:24:21 GMT</pubDate>
      
      <description>딥러닝 시작해보기-4
인공신경망과 손실함수
 1. 인공신경망의 기본 구조
    
     * 뇌의 학습방법을 수학적으로 모델링한 기계학습 알고리즘
     * 기본 구조 : y = Wx+b
       
       \(x_i\) : 입력, \(w_i\): 가중치, b : bias, f: 활성화함수
       u : 결합(Net), z: 출력
     * 뉴런에는 선형 결합과 활성화 함수 기능이 들어있음
     * 입력층, 은닉층, 출력층으로 구성됨
     * 각 노드의 뉴런 출력은 직접 전달되는 정보에만 의존할 뿐 다른 노</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-4"><a href="#딥러닝-시작해보기-4" class="headerlink" title="딥러닝 시작해보기-4"></a>딥러닝 시작해보기-4</h2><h3 id="인공신경망과-손실함수"><a href="#인공신경망과-손실함수" class="headerlink" title="인공신경망과 손실함수"></a>인공신경망과 손실함수</h3><ol><li><p>인공신경망의 기본 구조</p><ul><li>뇌의 학습방법을 수학적으로 모델링한 기계학습 알고리즘</li><li>기본 구조 : y = Wx+b<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_1.PNG" alt="인공신경망"><br>\(x_i\) : 입력, \(w_i\): 가중치, b : bias, f: 활성화함수<br>u : 결합(Net), z: 출력</li><li>뉴런에는 선형 결합과 활성화 함수 기능이 들어있음</li><li>입력층, 은닉층, 출력층으로 구성됨</li><li>각 노드의 뉴런 출력은 직접 전달되는 정보에만 의존할 뿐 다른 노드와는 무관</li><li>그래서? 병렬처리가 가능함.</li></ul></li><li><p>손실 함수(Loss or Cost function)</p><ul><li>신경망의 출력값과 실제 결과값의 차이를 정의하는 함수</li><li>신경망 학습목표는 손실함수를 최소화 하는 방향으로 움직여야 함</li><li>SGD, Adam 등의 학습 최적화 알고리즘</li></ul></li><li><p>손실 함수</p><ul><li>회귀(Regression)<br>제곱 오차(MSE) 사용, 최근에는 rmse, mae의 장점이 있는 Huber Loss 사용하는 추세<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_4.PNG" alt="회귀"></li></ul></li></ol><ul><li><p>Huber Loss?<br>MAE + MSE -&gt; for Time Series Data!!<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_5.PNG" alt="huber"><br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_7.PNG" alt="huber Loss"></p><ul><li>분류(Classification)<br>활성화 함수 : softmax, 손실함수 : cross-entropy<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_2.PNG" alt="분류"></li></ul></li></ul><h3 id="알고리즘과-역전파"><a href="#알고리즘과-역전파" class="headerlink" title="알고리즘과 역전파"></a>알고리즘과 역전파</h3><ol><li><p>학습 알고리즘</p><ul><li>경사 하강법: 기울기를 이용하여 손실함수 S(\theta) 값을 최적화</li><li>gradient(기울기)의 반대 방향으로 일정 크기만큼 이동하는 것을 반복하여<br>손실함수의 값을 최소화하는 \theta의 값을 찾음</li><li>\[\theta = \theta - \eta \nabla_\theta S(\theta)\]</li><li>이 떄 \eta 는 미리 정해진 learning rate(step size) 이고 보통 1e-3 ~ 1e-4 정도를 사용</li></ul></li><li><p>역전파</p><ul><li><p>계산 그래프</p></li><li><p>노드는 연산을, 엣지는 데이터의 흐름방향<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_8.PNG" alt="Chain Rule"></p></li><li><p>sigmoid 함수 역전파<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_9.PNG" alt="sigmoidBP"></p></li><li><p>합성함수 미분법(Chain Rule)<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_10.PNG" alt="합성미분"></p></li><li><p>행렬연산과 역전파 1<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_11.PNG" alt="행렬역전파"></p></li><li><p>이진분류 2-layer NN 역전파<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_12.PNG" alt="이진역전파"></p></li></ul><blockquote><p>to be continued…</p></blockquote></li></ol>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문과-준비3</title>
      <link>https://ikarus-999.github.io/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/</link>
      <guid>https://ikarus-999.github.io/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/</guid>
      <pubDate>Sun, 22 Nov 2020 19:44:55 GMT</pubDate>
      
      <description>딥러닝 시작해보기-3
차원수 늘리기, 줄이기(TF2.x)
1
2
3
4
5
6
7


x = tf.expand_dims(x, 1)
x.shape # (x.shape, 1)

x[..., tf.newaxis].shape # (x.shape, 1)

np.squeeze(x[0]).shape # x.shape 차원 줄이기



TF2.x Layers
Convolution

 * filters : layer에서 출력될때 몇개의 filter
 * kernel_size : filter(weight) 의 사이즈
 * strides : 몇 개의 </description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-3"><a href="#딥러닝-시작해보기-3" class="headerlink" title="딥러닝 시작해보기-3"></a>딥러닝 시작해보기-3</h2><h3 id="차원수-늘리기-줄이기-TF2-x"><a href="#차원수-늘리기-줄이기-TF2-x" class="headerlink" title="차원수 늘리기, 줄이기(TF2.x)"></a>차원수 늘리기, 줄이기(TF2.x)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.expand_dims(x, <span class="number">1</span>)</span><br><span class="line">x.shape <span class="comment"># (x.shape, 1)</span></span><br><span class="line"></span><br><span class="line">x[..., tf.newaxis].shape <span class="comment"># (x.shape, 1)</span></span><br><span class="line"></span><br><span class="line">np.squeeze(x[<span class="number">0</span>]).shape <span class="comment"># x.shape 차원 줄이기</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="TF2-x-Layers"><a href="#TF2-x-Layers" class="headerlink" title="TF2.x Layers"></a>TF2.x Layers</h3><p>Convolution</p><ul><li>filters : layer에서 출력될때 몇개의 filter</li><li>kernel_size : filter(weight) 의 사이즈</li><li>strides : 몇 개의 pixel만큼 skip하면서 sliding window 할 것인지</li><li>padding : same, zero</li><li>activation : 활성화 함수(<em>Linear function은 층을 쌓는 의미가 없다</em>)</li></ul><blockquote><p>to be Continued…</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>TF2</title>
      <link>https://ikarus-999.github.io/2020/11/22/TF2/</link>
      <guid>https://ikarus-999.github.io/2020/11/22/TF2/</guid>
      <pubDate>Sun, 22 Nov 2020 10:56:34 GMT</pubDate>
      
      <description>Tensorflow 2.x 사용법
Tensor 생성
 * tf.constant() : list, tuple, Array 를 Tensor로 바꿈
   
   
 * tensor = tf.constant(arr)
   
   
 * tensor.dtype : 데이터 타입 확인
   
   
 * tf.cast(tensor, dtype=tf.uint8) : TF int8로 데이터타입 바꾸기
   
   
   
 * tensor.numpy() : numpy array로 바꾸기
   
   

Tensor에 랜덤한 숫자들 생성
 * num</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Tensorflow-2-x-사용법"><a href="#Tensorflow-2-x-사용법" class="headerlink" title="Tensorflow 2.x 사용법"></a>Tensorflow 2.x 사용법</h2><h3 id="Tensor-생성"><a href="#Tensor-생성" class="headerlink" title="Tensor 생성"></a>Tensor 생성</h3><ul><li><p>tf.constant() : list, tuple, Array 를 Tensor로 바꿈</p></li><li><p>tensor = tf.constant(arr)</p></li><li><p>tensor.dtype : 데이터 타입 확인</p></li><li><p>tf.cast(tensor, dtype=tf.uint8) : TF int8로 데이터타입 바꾸기<br><img src="/2020/11/22/TF2/lect_2.PNG" alt="cast사용법"></p></li><li><p>tensor.numpy() : numpy array로 바꾸기</p></li></ul><h3 id="Tensor에-랜덤한-숫자들-생성"><a href="#Tensor에-랜덤한-숫자들-생성" class="headerlink" title="Tensor에 랜덤한 숫자들 생성"></a>Tensor에 랜덤한 숫자들 생성</h3><ul><li>numpy에서는 기본적인 normal distribution 생성<br>np.random.randn(9) : 9개의 불연속적이며 일정한 분포 난수 생성</li></ul><p>Distribution에 따른 난수 생성</p><ul><li><p>tf.random.normal<br>중심극한 이론에 의한 연속적인 모양</p></li><li><p>tf.random.uniform<br>불연속적이며 일정한 분포</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/Tensorflow2/">Tensorflow2</category>
      
      <category domain="https://ikarus-999.github.io/tags/TF2/">TF2</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/22/TF2/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝 입문과 준비2</title>
      <link>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/</link>
      <guid>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/</guid>
      <pubDate>Sun, 22 Nov 2020 10:44:36 GMT</pubDate>
      
      <description>딥러닝 시작해보기-2
Broadcast
두개의 행렬 shape가 서로 달라도
한쪽의 차원이 같거나, 연산하는 값이 한 개일때
shape에 맞게 복사해서 연산함

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21


arr = np.arange(6).reshape(-1, 3)
# [[0, 1, 2], 
#  [3, 4, 5]]

arr + 3
# [[3, 4, 5],
#  [6, 7, 8]]

arr * 3
# [[0, 3, 6],
#  [9, 12 15]

arr + np.array([</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-2"><a href="#딥러닝-시작해보기-2" class="headerlink" title="딥러닝 시작해보기-2"></a>딥러닝 시작해보기-2</h2><h3 id="Broadcast"><a href="#Broadcast" class="headerlink" title="Broadcast"></a>Broadcast</h3><p>두개의 행렬 shape가 서로 달라도<br>한쪽의 차원이 같거나, 연산하는 값이 한 개일때<br>shape에 맞게 복사해서 연산함</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">6</span>).reshape(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># [[0, 1, 2], </span></span><br><span class="line"><span class="comment">#  [3, 4, 5]]</span></span><br><span class="line"></span><br><span class="line">arr + <span class="number">3</span></span><br><span class="line"><span class="comment"># [[3, 4, 5],</span></span><br><span class="line"><span class="comment">#  [6, 7, 8]]</span></span><br><span class="line"></span><br><span class="line">arr * <span class="number">3</span></span><br><span class="line"><span class="comment"># [[0, 3, 6],</span></span><br><span class="line"><span class="comment">#  [9, 12 15]</span></span><br><span class="line"></span><br><span class="line">arr + np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># [[1, 3, 5],</span></span><br><span class="line"><span class="comment">#  [4, 6, 8]]</span></span><br><span class="line"></span><br><span class="line">np.add(arr, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 모든 원소에 1을 더함</span></span><br><span class="line"></span><br><span class="line">np.multiply(arr, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 모든 원소에 3을 곱함</span></span><br></pre></td></tr></table></figure><h3 id="argmax-argmin"><a href="#argmax-argmin" class="headerlink" title="argmax, argmin"></a>argmax, argmin</h3><p> 배열의 큰 값이나 작은 값의 index return<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">54</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">np.argmax(arr) <span class="comment"># 54</span></span><br><span class="line">np.argmin(arr) <span class="comment"># 1</span></span><br><span class="line">np.unique(arr) <span class="comment"># 유일한 값 출력</span></span><br></pre></td></tr></table></figure></p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝 입문과 준비</title>
      <link>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/</link>
      <guid>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/</guid>
      <pubDate>Sun, 22 Nov 2020 09:45:13 GMT</pubDate>
      
      <description>딥러닝 시작해보기
Tensor 이해하기
 1. 차원
    
     * 0차원(상수) : Scalar값
     * 1차원(리스트 씌운 상수), 2차원(2d), 3차원(3d), 4차원(4-d), n차원(n-d) : Tensor
     * Numpy로 Tensor 표현과 응용이 가능
    
    1
    2
    3
    4
    5
    6
    
    
    import numpy as np
    arr = np.array([[3, 6, 9], [2, 4, 8]])
    print(arr.dtype) # </description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기"><a href="#딥러닝-시작해보기" class="headerlink" title="딥러닝 시작해보기"></a>딥러닝 시작해보기</h2><h3 id="Tensor-이해하기"><a href="#Tensor-이해하기" class="headerlink" title="Tensor 이해하기"></a>Tensor 이해하기</h3><ol><li><p>차원</p><ul><li>0차원(상수) : Scalar값</li><li>1차원(리스트 씌운 상수), 2차원(2d), 3차원(3d), 4차원(4-d), n차원(n-d) : Tensor</li><li>Numpy로 Tensor 표현과 응용이 가능</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">arr = np.array([[<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>]])</span><br><span class="line">print(arr.dtype) <span class="comment"># dtype(&#x27;float64&#x27;)</span></span><br><span class="line">print(arr.shape) <span class="comment"># (2, 3)</span></span><br><span class="line">print(arr.size) <span class="comment"># 2 * 3 = 6</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>차원 늘리기와 줄이기</p><ul><li><p>reshape, -1 활용</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr.reshape(<span class="number">-1</span>) <span class="comment"># 1차원으로 펼치기</span></span><br><span class="line">arr.reshape(<span class="number">-1</span>, <span class="number">3</span>) <span class="comment"># 첫번째 차원은 알아서, 두번째 차원은 shape 3</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>Ravel() : arr의 차원을 1로 바꿈(==&gt; Flatten)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]) <span class="comment"># (2, 3)</span></span><br><span class="line">arr.ravel()</span><br><span class="line">arr.shape <span class="comment">#(6, )</span></span><br></pre></td></tr></table></figure></li><li><p>np.expand_dims() : 값을 유지하고 차원만 늘릴때</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.expand_dims(arr, <span class="number">-1</span>) <span class="comment">#(6, 1)</span></span><br><span class="line">arr.shape</span><br></pre></td></tr></table></figure><ul><li>numpy array를 빠르게 채우는 방법!</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0으로 채우기</span></span><br><span class="line">arr2 = np.zeros([<span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># 3 * 4의 0이 채워진 배열</span></span><br><span class="line">one2 = np.ones([<span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># 3 * 4의 1로 채워진 배열</span></span><br><span class="line"></span><br><span class="line">five2 = np.ones([<span class="number">3</span>, <span class="number">4</span>]) * <span class="number">5</span> <span class="comment"># 1로 채운 값에 5를 다 곱함</span></span><br><span class="line">arr2 = np.arange(n, m) <span class="comment"># n ~ m-1까지의  수로 배열 채우기</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># array([n ~ m-1])</span></span><br><span class="line">arr = np.arange(<span class="number">5</span>, <span class="number">11</span>).reshape(<span class="number">2</span>, <span class="number">-1</span>) </span><br><span class="line"><span class="comment"># 5 ~ 10 : 6개의 숫자, (2, 3)</span></span><br><span class="line"></span><br><span class="line">arr <span class="comment"># array([5, 6, 7]</span></span><br><span class="line">    <span class="comment">#       [8, 9, 10])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>모양이 맞지 않으면 Error…<br>5, 6, 7, 8, 9는 5개의 숫자<br>5 * 1 만 가능한.<br><img src="/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/slide_lect.PNG" alt="모양다름"></li></ul></li><li><p>Index &amp; slicing</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 리스트 인덱스 &amp; 슬라이싱</span></span><br><span class="line"></span><br><span class="line">nums = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">nums[:<span class="number">-1</span>] <span class="comment"># 마지막 숫자 전까지 표시</span></span><br><span class="line"></span><br><span class="line">nums[::<span class="number">-1</span>] <span class="comment"># 리스트 안의 숫자를 거꾸로 표현</span></span><br><span class="line"></span><br><span class="line">nums = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">print(nums[<span class="number">0</span>][<span class="number">1</span>]) = <span class="number">2</span> <span class="comment"># 첫번째 리스트 안의 인덱스가 1인 숫자</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = np.array([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>])</span><br><span class="line">print(arr[<span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># 10 --&gt; 인덱싱 [행, 열]</span></span><br><span class="line"></span><br><span class="line">print(arr[<span class="number">1</span>:, <span class="number">1</span>:]) <span class="comment"># [[9, 10]]</span></span><br></pre></td></tr></table></figure><ol start="4"><li>Boolean Indexing<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = np.random.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">print(data&lt;=<span class="number">0</span>) <span class="comment"># False, True로 나옴</span></span><br><span class="line"></span><br><span class="line">data[data &lt;=<span class="number">0</span>] = <span class="number">1</span> <span class="comment"># 0 이하인 것을 1로 채우다</span></span><br></pre></td></tr></table></figure></li></ol>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>GAN프로젝트_try</title>
      <link>https://ikarus-999.github.io/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/</link>
      <guid>https://ikarus-999.github.io/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/</guid>
      <pubDate>Tue, 17 Nov 2020 12:28:07 GMT</pubDate>
      
      <description>Style GAN toy 프로젝트
StyleGAN의 특징
 1. 이미지를 Style의 조합으로 보고
    Generator의 각 Layer마다 Style 정보를 입히는 방식으로 이미지 합성
    이 때 각 Layer에서 추가되는 Style은 이미지의 Coarse Feature(포즈, 성별 등)부터
    Fine Detail(머리색, 피부톤 등)까지
    각기 다른 Level의 Visual 속성들을 조절 가능
    StyleGAN은 생각보다 안정적이고 높은 퀄리티의 이미지 생성 

네트워크 구조(Module)
 1. GAN이</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Style-GAN-toy-프로젝트"><a href="#Style-GAN-toy-프로젝트" class="headerlink" title="Style GAN toy 프로젝트"></a>Style GAN toy 프로젝트</h2><h3 id="StyleGAN의-특징"><a href="#StyleGAN의-특징" class="headerlink" title="StyleGAN의 특징"></a>StyleGAN의 특징</h3><ol><li>이미지를 Style의 조합으로 보고<br>Generator의 각 Layer마다 Style 정보를 입히는 방식으로 이미지 합성<br>이 때 각 Layer에서 추가되는 Style은 이미지의 Coarse Feature(포즈, 성별 등)부터<br>Fine Detail(머리색, 피부톤 등)까지<br>각기 다른 Level의 Visual 속성들을 조절 가능<br>StyleGAN은 생각보다 안정적이고 높은 퀄리티의 이미지 생성  </li></ol><h3 id="네트워크-구조-Module"><a href="#네트워크-구조-Module" class="headerlink" title="네트워크 구조(Module)"></a>네트워크 구조(Module)</h3><ol><li>GAN이란 어떤 것일까???<br><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/GANdesc.png" alt="이것이 GAN"></li></ol><p><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide21.PNG" alt="GAN model"></p><ul><li>Instance Norm?<br><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide22.PNG" alt="Instance Norm"></li></ul><p><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide23.PNG" alt="Instance Norm2"></p><ul><li>Generator 구조 설명</li></ul><p><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide322.png" alt="model"></p><p>왼쪽이 Traditional Network, 오른쪽이 이 논문에서 제안한 Style-gased Generator. </p><p>왼쪽 네트워크와 오른쪽에 Synthesis Network가 똑같은 구조를 갖고 있지만,<br>이전 GAN에서는 Latent z를 바로 Input으로 넣어줬던 것과는 다르게,<br>StyleGAN에서는 학습된 Constant, (w) 값을 입력으로 사용함. </p><p>새롭게 Mapping Network와 Noise가 추가됨..</p><p>W를 Feature에 매핑하는 경우<br>W는 Z처럼 고정된 분포를 따르지 않음. </p><p>Sampling density는 학습된 Piecewise Continuous Mapping f(z)<br>(f는 Mapping Network 입니다)에 의해 정해짐. </p><p>따라서, Warping(틀어짐)이 많이 일어나지 않음.<br>그렇기 때문에 Factors of variation은 더욱 Linear하고, Disentangled (얽히지 않음).<br>이것이 바로 z를 곧바로 Feature에 매핑하는 것보다 w에 매핑하는 것의 장점입니다</p><p>기존의 Generator (a)는<br>Input Latent Vector (z)가 직접 Convolution, Upsampling 등을 거쳐 이미지로 변환되는 구조. </p><p>Style-based Generator (b) 의 경우,<br>(z)가  Fully-connected Layer로 구성된 Mapping Network을 거쳐<br>Intermediate Latent Vector (w) 먼저 변환. </p><p>(w)는 Constant Tensor가 이미지로 변환되는 과정에서<br>스타일을 입히는 역할을 수행.</p><p>다양한 스타일의 이미지를 생성.</p><ul><li>Style Transfer를 실시간으로 가능케하는 Adaptive Instance Norm</li></ul><p>Synthesis Network (합성 네트워크)<br>z를 중간 latent space W에 매핑을 한 뒤에 이 w는 “A”를 거쳐서 style, y=(ys,yb)<br>로 변형됨. 이때 A는 학습된 affine transform 임. 그리고 이 style들은<br>AdaIN(adaptive instance normalization) opeartion을 control 함.<br><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/Adain.png" alt="AdaIN"></p><p>AdaIN은 style transfer를 할 때 많이 쓰이는 방법으로, 임의의 style transfer를 실시간으로 가능하게 함.<br>여기서 feature map xi는<br>normalized 된 다음에, style로 변환된 두 y로 scaled, biased 됨. (style이 입혀짐)<br>이 과정을 매 layer 마다 반복함. 그리고 이러한 방법은 scale-specific control 을 가능하게 함.</p><blockquote><p>To be continued…</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      <category domain="https://ikarus-999.github.io/tags/GAN/">GAN</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>training_pc</title>
      <link>https://ikarus-999.github.io/2020/11/16/training-pc/</link>
      <guid>https://ikarus-999.github.io/2020/11/16/training-pc/</guid>
      <pubDate>Sun, 15 Nov 2020 17:00:28 GMT</pubDate>
      
      <description>딥러닝 공부 후기
비정형 데이터를 다룰려면 GPU는 필수다.

Why GPU ??
 1. CPU보다 더 빠른 병렬 처리 가능

 *  행렬곱 계산이 CPU보다 훨씬 빠름

 2. 계산 그래프 빌드, 처리 속도가 빠름.

 * 비정형 데이터 처리엔 GPU가 필수

음성 딥러닝, 나도 해볼까?
음성 딥러닝

음성 딥러닝은 결코 쉽지 않다. 딥러닝계의 보스급.
신호처리 배워야 그나마 수월하다.
초반 Feature Extraction 경험을 쌓는것을 권장한다.
RNN계열의 LSTM으로 시작. 하지만,
Attention, Transforme</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-공부-후기"><a href="#딥러닝-공부-후기" class="headerlink" title="딥러닝 공부 후기"></a>딥러닝 공부 후기</h2><blockquote><p>비정형 데이터를 다룰려면 GPU는 필수다.</p></blockquote><h3 id="Why-GPU"><a href="#Why-GPU" class="headerlink" title="Why GPU ??"></a>Why GPU ??</h3><ol><li>CPU보다 더 빠른 병렬 처리 가능</li></ol><ul><li> 행렬곱 계산이 CPU보다 훨씬 빠름</li></ul><ol start="2"><li>계산 그래프 빌드, 처리 속도가 빠름.</li></ol><ul><li>비정형 데이터 처리엔 GPU가 필수</li></ul><h3 id="음성-딥러닝-나도-해볼까"><a href="#음성-딥러닝-나도-해볼까" class="headerlink" title="음성 딥러닝, 나도 해볼까?"></a>음성 딥러닝, 나도 해볼까?</h3><p><a href="https://github.com/topics/vggish">음성 딥러닝</a></p><p>음성 딥러닝은 결코 쉽지 않다. 딥러닝계의 보스급.<br>신호처리 배워야 그나마 수월하다.<br>초반 Feature Extraction 경험을 쌓는것을 권장한다.<br>RNN계열의 LSTM으로 시작. 하지만,<br>Attention, Transformer 날코딩 등 논문 구현 경험이 매우 중요하다.<br>Mel Spectrogram을 한다 해도 원리를 잘 알아야 나중에 모델링과 데이터 분해가 쉽다.</p><h3 id="딥러닝-입문-방법"><a href="#딥러닝-입문-방법" class="headerlink" title="딥러닝 입문 방법"></a>딥러닝 입문 방법</h3><p>딥러닝 입문 하려면 수학은 필수. 정말 중요.</p><p><a href="https://paperswithcode.com/">논문 구현 시도해보기1</a><br><a href="https://arxiv.org/abs/1706.03762">논문 구현 시도해보기2</a></p><blockquote><p>어떤 논문에는 <em>파라미터 하나도 없는</em> 것도 있다.<br>레이어 구조도만 있고 파라미터가 없는 것은 진짜 구현난이도 보스급.</p></blockquote><h3 id="딥러닝-자격증-취득-후기"><a href="#딥러닝-자격증-취득-후기" class="headerlink" title="딥러닝 자격증 취득 후기"></a>딥러닝 자격증 취득 후기</h3><p>이것이 바로 딥러닝 자격증!!<br><img src="https://api.accredible.com/v1/frontend/credential_website_embed_image/certificate/25378511," alt="우와~" title="WoW!"></p><ol><li>문제 유형</li></ol><ul><li><p>Category 1: Basic model</p></li><li><p>Category 2: Model from learning dataset</p></li><li><p>Category 3: Image classification<br>Convolutional Neural Network with real-world image dataset</p></li><li><p>Category 4: Natural language processing (NLP)<br>NLP Text Classification with real-world text dataset</p></li><li><p>Category 5: Time series, sequences and predictions<br>Sequence Model with real-world numeric dataset</p></li><li><p>이 시험은 응시자가 TensorFlow 2.x를 통해 모델을 빌드하여 문제를 해결할 수 있는지<br>테스트합니다.</p></li><li><p>머신러닝(ML) 및 딥러닝의 기본 원칙</p></li><li><p>TensorFlow 2.x에서 ML 모델 개발하기</p></li><li><p>심층신경망 및 합성곱 신경망(CNN)을 통한 이미지 인식, 객체 탐지, 텍스트 인식 알고리즘 빌드</p></li><li><p>컴퓨터가 정보를 ‘보는’ 방식과 플롯 손실 및 정확도 이해할 수 있도록 다른 크기 및 형태의 실제 이미지를 활용하여 합성곱에서 이미지의 경로를 시각화</p></li><li><p>과적합을 예방하기 위한 확장 및 드롭아웃과 같은 전략 탐색</p></li><li><p>TensorFlow를 이용하여 자연어 처리 문제를 해결하기 위해 신경망 적용</p></li></ul><blockquote><p>Google 공식 페이지 발췌</p></blockquote><ol start="2"><li>합격 점수 / 규칙</li></ol><blockquote><p>허용된 인터넷 브라우징은 Tensorflow document, 총 100점 만점에 90점 커트라인</p></blockquote><ul><li>난이도는 category 5가 가장 높음</li></ul><blockquote><p>시험 시간 : 5시간 (컴퓨터 딥러닝 훈련시간 포함)</p></blockquote><ol start="3"><li>따면 좋은 점</li></ol><p>개발자 네트워크에 join 할 수가 있어요~</p><p><a href="https://www.tensorflow.org/certificate-network?hl=ko">자격증 네트워크</a></p><p><a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials">참고문서</a></p><p>그럼 20000~!</p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/16/training-pc/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
