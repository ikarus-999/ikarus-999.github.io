<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ikarus&#39;s BLOG</title>
    <link>https://ikarus-999.github.io/</link>
    
    <image>
      <url>https://www.gravatar.com/avatar/5a9f87e79678fa1d3ad8a772986a89a0</url>
      <title>ikarus&#39;s BLOG</title>
      <link>https://ikarus-999.github.io/</link>
    </image>
    
    <atom:link href="https://ikarus-999.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>데이터쟁이 이카루스 블로그입니다</description>
    <pubDate>Sat, 28 Nov 2020 07:40:16 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>딥러닝-입문과-준비6</title>
      <link>https://ikarus-999.github.io/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/</link>
      <guid>https://ikarus-999.github.io/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/</guid>
      <pubDate>Fri, 27 Nov 2020 13:54:12 GMT</pubDate>
      
      <description>딥러닝 시작해보기-6
💥Remind!! 딥러닝에 비선형 활성화 함수를 사용하는 이유
 선형 함수로는 XOR과 같은 non-linear한 문제는 해결이 안됨;;

그러면 Hidden Layer를 늘리면 되지 않을까?
$$f(ax+by) = af(x) + bf(y)$$ 라는 특징 때문에
N-layer 깊이를 아무리 쌓아도 1-Layer로 동작함.

최적화(Opt) 알고리즘
 * 경사하강법(GD)
   $$\theta = \theta - \eta \nabla_\theta S(\theta)$$ 
   
   Network의 paramet</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-6"><a href="#딥러닝-시작해보기-6" class="headerlink" title="딥러닝 시작해보기-6"></a>딥러닝 시작해보기-6</h2><h3 id="💥Remind-딥러닝에-비선형-활성화-함수를-사용하는-이유"><a href="#💥Remind-딥러닝에-비선형-활성화-함수를-사용하는-이유" class="headerlink" title="💥Remind!! 딥러닝에 비선형 활성화 함수를 사용하는 이유"></a>💥Remind!! 딥러닝에 비선형 활성화 함수를 사용하는 이유</h3><p>  선형 함수로는 XOR과 같은 non-linear한 문제는 해결이 안됨;;</p><blockquote><p>그러면 Hidden Layer를 늘리면 되지 않을까?<br>  $$f(ax+by) = af(x) + bf(y)$$ 라는 특징 때문에<br>  N-layer 깊이를 아무리 쌓아도 1-Layer로 동작함.</p></blockquote><h3 id="최적화-Opt-알고리즘"><a href="#최적화-Opt-알고리즘" class="headerlink" title="최적화(Opt) 알고리즘"></a>최적화(Opt) 알고리즘</h3><ul><li><p>경사하강법(GD)<br>$$\theta = \theta - \eta \nabla_\theta S(\theta)$$  </p><p>Network의 parameter=$$\theta $$ 로 할때 손실함수 $$J(\theta)$$의 값을 최소화하기 위해 기울기<br>$$\nabla J(\theta)$$를 이용하는 방법<br>GD에서는 Gradient의 반대 방향으로 일정 크기(lr)만큼 이동하는 것을 반복하여 loss function의 값을 최소화 하는 $$\theta$$의 값을 찾음,  </p></li><li><p>lr $$\eta$$ 는 보통 1e-3 ~ 1e-4 사이에서 사용함.<br>너무 크면 global minimum을 지나치고 너무 작으면 Local Minimum에 빠짐.</p></li></ul><ul><li><p>확률적 경사하강법(SGD)<br>전체 Training set을 사용하는 것을 batch Gradient Descent, 계산량이 많아지는 것을 방지하기 위해<br>mini-batch에 대해서만 손실함수를 계산하는 확률적 GD를 사용함.<br>같은 시간에 더 많은 step를 갈 수 있음, 여러번 반복할 경우 batch의 결과와 비슷함</p></li><li><p>GD vs SGD<br>GD : 확실한데 너무 느림 | SGD : 조금 헤메지만 빠름</p></li><li><p>Momentum : 현재 Gradient를 통해 이동하는 방향과 별개로 과거의 이동방식을 기억하면서 일종의 관성을 주는 방식</p></li><li><p>AdaGrad(Adaptive Gradient)</p><ol><li>많이 변화했던 변수들은 step size를 작게 하는 것<br>자주 등장하거나 변화를 많이 한 변수들은 optimum에 가까이 있을 확률이 높기 때문에 작은 크기로 이동하면서 미세조절</li><li>적게 변화한 변수들은 많이 이동해야할 확률이 높기 때문에 먼저 빠르게 loss값을 줄이는 방식으로 이동하는 방식<br>학습을 계속 진행하면 step size가 너무 줄어드는 단점이 있음.<br><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_05.PNG" alt="AdaGrad"></li></ol></li><li><p>RMSProp<br>합을 지수평균으로 대체하여 Adagrad의 단점을 해결<br>G가 무한정 커지지는 않으면서 최근 변화량의 변수간 상대적인 크기 차이는 유지할 수 있음.<br><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_04.PNG" alt="RMsprop"></p></li><li><p>Adam<br>Momentum + RMSProp</p><ol><li>지금까지 계산해온 기울기의 지수평균을 저장</li><li>rmsprop과 유사하게 Gradient의 제곱값의 지수평균을 저장<br><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_06.PNG" alt="Adam"></li></ol></li></ul><h3 id="Overfitting-과적합"><a href="#Overfitting-과적합" class="headerlink" title="Overfitting(과적합)"></a>Overfitting(과적합)</h3><p>  Training Set의 지엽적인 특성까지 반영해 Variance High로 Training되어서 Training Set을 암기해버리는 현상<br>  <em>Test Set을 잘 예측하지 못함</em><br>  주로 표현력이 높은 모델, 즉 파라미터가 많은 모델에 발생</p><ol><li>정규화(Regularization)</li></ol><ul><li>손실함수에 가중치의 크기를 포함</li><li>가중치가 작아지도록 학습한다는 것은 Outlier(Noise)의 영향을 적게 받음  </li></ul><h4 id="L2-정규화"><a href="#L2-정규화" class="headerlink" title="L2 정규화"></a>L2 정규화</h4><p><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_08.PNG" alt="L2"></p><p><em>Rigde Regression</em></p><h4 id="L1-정규화"><a href="#L1-정규화" class="headerlink" title="L1 정규화"></a>L1 정규화</h4><p><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_09.PNG" alt="L1"><br>Sparse Model에 알맞음.. 작은 가중치들이 거의 0으로 수렴하여 몇개의 중요한 가중치들만 남음. </p><p><em>Lasso Regression</em></p><p>미분 불가능한 점이 있기 때문에 Gradient-Base Learning에는 주의..<br><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_10.PNG" alt="L1주의점"></p><h4 id="DropOut"><a href="#DropOut" class="headerlink" title="DropOut"></a>DropOut</h4><p>각 레이어의 일정 비율로 뉴런의 출력 값을 0으로 만들어 나머지 뉴런들로 학습하는 방법<br>과적합을 효과적으로 예방 가능(Network 내부의 Ensemble 학습으로 볼 수 있음)</p><p>역전파는 ReLU처럼 동작<br>Forward Propagation때 시그널을 통과시킨 뉴런은 Backward때도 통과시킴<br>drop된 뉴런은 Backward Propagation때도 시그널 차단</p><p>반면, TEST때는 모든 뉴런에 신호를 전달함</p><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>학습하는 이전 층의 파라미터 변화로 현재층의 입력 분포가 바뀌는 현상을 내부 공분산 변화(Internal Covariate Shift)<br>이전 층의 작은 파라미터 변화가 증폭되어 뒷 레이어에 큰 영향을 받음.<br>그래서…</p><p>BN(2015)</p><ul><li>Gradient Vanishing, Exploding을 방지하는 대표적인 방법</li><li>직접적인 방법임.</li><li>Training 과정 자체를 안정화시켜 학습속도를 가속화</li><li>평균과 분산을 조절하는 과정이 <em>NN 안에 포함</em> 되어 있다는 것이 핵심적</li></ul><p>Training할때<br>각 Mini Batch마다 $$\gamma$$ 와 $$\beta$$를 구하고 저장해 둠</p><p>Test할때<br>구했던 $$\gamma$$ 와 $$\beta$$의 평균을 사용</p><p><img src="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_11.PNG" alt="BN"></p><h4 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h4><p>일종의 Regularization작업, 데이터가 적을 때 사용하면 매우 효과적<br>즉 데이터 변형</p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문과-준비5</title>
      <link>https://ikarus-999.github.io/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/</link>
      <guid>https://ikarus-999.github.io/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/</guid>
      <pubDate>Wed, 25 Nov 2020 12:49:48 GMT</pubDate>
      
      <description>딥러닝 시작해보기-5
선형대수 배워보기(행렬을 아무리 곱하고 더해도 선모양)
Scala : 크기만 존재하는 양
Vector : 속도, 위치이동, 힘, 공간뒤틀림과 같이 크기와 방향이 모두 존재하는 양


Norm ? n차원 벡터 $$\vec{x} = (x_1, x_2, \cdots x_n)$$
Norm $$\lVert x \rVert = \sqrt{x_1^1 + x_2^2 + \cdots + x_n^2}$$

“원점 O에서 점\(x_1, x_2, \cdots, x_n\) 까지의 거리”

내적 ? Inner product, Dot p</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-5"><a href="#딥러닝-시작해보기-5" class="headerlink" title="딥러닝 시작해보기-5"></a>딥러닝 시작해보기-5</h2><h3 id="선형대수-배워보기-행렬을-아무리-곱하고-더해도-선모양"><a href="#선형대수-배워보기-행렬을-아무리-곱하고-더해도-선모양" class="headerlink" title="선형대수 배워보기(행렬을 아무리 곱하고 더해도 선모양)"></a>선형대수 배워보기(행렬을 아무리 곱하고 더해도 선모양)</h3><h4 id="Scala-크기만-존재하는-양"><a href="#Scala-크기만-존재하는-양" class="headerlink" title="Scala : 크기만 존재하는 양"></a>Scala : 크기만 존재하는 양</h4><h4 id="Vector-속도-위치이동-힘-공간뒤틀림과-같이-크기와-방향이-모두-존재하는-양"><a href="#Vector-속도-위치이동-힘-공간뒤틀림과-같이-크기와-방향이-모두-존재하는-양" class="headerlink" title="Vector : 속도, 위치이동, 힘, 공간뒤틀림과 같이 크기와 방향이 모두 존재하는 양"></a>Vector : 속도, 위치이동, 힘, 공간뒤틀림과 같이 크기와 방향이 모두 존재하는 양</h4><p><img src="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/slido51_1.PNG" alt="스칼라와벡터"></p><h4 id="Norm-n차원-벡터-vec-x-x-1-x-2-cdots-x-n"><a href="#Norm-n차원-벡터-vec-x-x-1-x-2-cdots-x-n" class="headerlink" title="Norm ? n차원 벡터 $$\vec{x} = (x_1, x_2, \cdots x_n)$$"></a>Norm ? n차원 벡터 $$\vec{x} = (x_1, x_2, \cdots x_n)$$</h4><p>Norm $$\lVert x \rVert = \sqrt{x_1^1 + x_2^2 + \cdots + x_n^2}$$</p><blockquote><p>“원점 O에서 점\(x_1, x_2, \cdots, x_n\) 까지의 거리”</p></blockquote><h4 id="내적-Inner-product-Dot-product"><a href="#내적-Inner-product-Dot-product" class="headerlink" title="내적 ?  Inner product, Dot product"></a>내적 ?  Inner product, Dot product</h4><h4 id="행렬끼리-곱할-때는-차원을-주의한다"><a href="#행렬끼리-곱할-때는-차원을-주의한다" class="headerlink" title="행렬끼리 곱할 때는 차원을 주의한다."></a>행렬끼리 곱할 때는 차원을 주의한다.</h4><blockquote><p>A(m, n) * B(n, m) 만 가능</p></blockquote><h4 id="Transpose-전치행렬-행과-열을-뒤바꿈"><a href="#Transpose-전치행렬-행과-열을-뒤바꿈" class="headerlink" title="Transpose: 전치행렬(행과 열을 뒤바꿈)"></a>Transpose: 전치행렬(행과 열을 뒤바꿈)</h4><blockquote><p>A.T</p></blockquote><h3 id="numpy-연산-Element-wise-operation"><a href="#numpy-연산-Element-wise-operation" class="headerlink" title="numpy 연산(Element-wise operation)"></a>numpy 연산(Element-wise operation)</h3><blockquote><p>np.dot(x, y) (aka 내적, dot-product)와  x * y(element-wise)는 서로 다름.</p></blockquote><h3 id="numpy-비교-논리연산-element-wise-operation"><a href="#numpy-비교-논리연산-element-wise-operation" class="headerlink" title="numpy 비교, 논리연산(element-wise operation)"></a>numpy 비교, 논리연산(element-wise operation)</h3><h3 id="numpy-Reductions"><a href="#numpy-Reductions" class="headerlink" title="numpy Reductions"></a>numpy Reductions</h3><blockquote><p>argmax() : 최대값있는 인덱스를 리턴, argmin() : 최소값의 인덱스 리턴</p></blockquote><h3 id="np-all-np-any"><a href="#np-all-np-any" class="headerlink" title="np.all, np.any?"></a>np.all, np.any?</h3><blockquote><p>ALL : Array내 모든 값이 TRUE인가?<br>  any : Array내 값이 하나라도 TRUE인가?</p></blockquote><h3 id="np-mean-np-median-np-std-등-통계함수-사용-가능"><a href="#np-mean-np-median-np-std-등-통계함수-사용-가능" class="headerlink" title="np.mean, np.median, np.std 등 통계함수 사용 가능"></a>np.mean, np.median, np.std 등 통계함수 사용 가능</h3><h4 id="딥러닝에-대한-환상"><a href="#딥러닝에-대한-환상" class="headerlink" title="딥러닝에 대한 환상"></a>딥러닝에 대한 환상</h4><ol><li><p>복잡한 문제도 층을 깊고 넓게 쌓으면 해결된다 –&gt; Gradient Vanhshing, Initialize fault 으하하핰ㅋㅋㅋ</p></li><li><p>$$Sigmoid(z) = \frac{1} {1 + e^{-z}}$$<br>Sigmoid 도함수의 최대값은 1/4 … –&gt; 그래서 Gradient Vanishing 나는거임 ㅇㅇ</p></li></ol><h4 id="가중치-초기화"><a href="#가중치-초기화" class="headerlink" title="가중치 초기화"></a>가중치 초기화</h4><ol><li><p>초기화의 중요성<br>$$t = wx+b$$ 에서 w가 100, b가 50이라면 x가 0.01이더라도 t는 51이 됨<br>역전파때 sigmoid 함수 통과시키면 $$\sigma’ (51)$$ 리턴됨<br>하지만 t가 5만 넘어도 $$\sigma (t)$$ 는 0에 수렴 –&gt; 이것이 바로 Gradient Vanishing…</p></li><li><p>그래서 입력층의 가중치w를 모두 0으로 리셋!<br>Forward Propagation때 두번째 층 뉴런에 모두 같은 값이 전달됨<br>Backward Propagation때 두째 층 가중치가 모두 똑같이 업데이트 ==&gt; 신경망 표현력 제한</p></li><li><p>Bias는 0으로 초기화하는게 일반적으로 효율적</p></li></ol><h4 id="가중치-초기화-2"><a href="#가중치-초기화-2" class="headerlink" title="가중치 초기화 2"></a>가중치 초기화 2</h4><ol><li><p>표준 정규분포를 이용한 가중치 초기화<br>Sigmoid함수의 출력값이 극단적으로(0 or 1)에 치우치는 현상 –&gt; Gradient Vanishing</p></li><li><p>표준편차를 0.01로 하는 정규분포로 초기화<br>가중치가 모여 있음 =&gt; 기울기 소실 문제 어느정도 완화됨</p></li></ol><h4 id="가중치-초기화-3"><a href="#가중치-초기화-3" class="headerlink" title="가중치 초기화 3"></a>가중치 초기화 3</h4><p>  Xavier초기화 방법(2010)<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n_input, n_output) / (n_input) ** <span class="number">0.5</span></span><br></pre></td></tr></table></figure><br>  Sigmoid와 같은 S자 함수의 경우 출력값들이 정규분포 형태이어야 안정적 학습 가능  </p><ul><li><p>Sigmoid function과 Xavier Init방법을 사용했을 경우 그래프<br><img src="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_01.PNG" alt="sigmodi"></p></li><li><p>ReLU 계열 함수에는 적절하지 않음<br>layer를 거쳐갈 수록 0에 수렴(converge)</p><p><img src="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_02.PNG" alt="reluNo"></p><h4 id="가중치-초기화-4"><a href="#가중치-초기화-4" class="headerlink" title="가중치 초기화 4"></a>가중치 초기화 4</h4><p>He 초기화 방법(2015)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n_input, n_output) / (n_input / <span class="number">2</span>) ** <span class="number">0.5</span></span><br></pre></td></tr></table></figure><p>RELU + He init –&gt; 10 layer를 거쳐도 표준편차가 0으로 수렴하지 않음<br><img src="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_03.PNG" alt="relu"></p></li></ul><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul><li>가중치 초기화는 너무나 중요함</li><li>tanh의 경우 Xavier Init 방법이 효율적</li><li>ReLU계열 함수에는 He Init 방법이 효율적</li><li>최근엔 대부분 He Init를 주로 사용</li></ul>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/Linear-Algebra/">Linear Algebra</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문과-준비4</title>
      <link>https://ikarus-999.github.io/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/</link>
      <guid>https://ikarus-999.github.io/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/</guid>
      <pubDate>Tue, 24 Nov 2020 13:24:21 GMT</pubDate>
      
      <description>딥러닝 시작해보기-4
인공신경망과 손실함수
 1. 인공신경망의 기본 구조
    
     * 뇌의 학습방법을 수학적으로 모델링한 기계학습 알고리즘
     * 기본 구조 : y = Wx+b
       
       \(x_i\) : 입력, \(w_i\): 가중치, b : bias, f: 활성화함수
       u : 결합(Net), z: 출력
     * 뉴런에는 선형 결합과 활성화 함수 기능이 들어있음
     * 입력층, 은닉층, 출력층으로 구성됨
     * 각 노드의 뉴런 출력은 직접 전달되는 정보에만 의존할 뿐 다른 노</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-4"><a href="#딥러닝-시작해보기-4" class="headerlink" title="딥러닝 시작해보기-4"></a>딥러닝 시작해보기-4</h2><h3 id="인공신경망과-손실함수"><a href="#인공신경망과-손실함수" class="headerlink" title="인공신경망과 손실함수"></a>인공신경망과 손실함수</h3><ol><li><p>인공신경망의 기본 구조</p><ul><li>뇌의 학습방법을 수학적으로 모델링한 기계학습 알고리즘</li><li>기본 구조 : y = Wx+b<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_1.PNG" alt="인공신경망"><br>\(x_i\) : 입력, \(w_i\): 가중치, b : bias, f: 활성화함수<br>u : 결합(Net), z: 출력</li><li>뉴런에는 선형 결합과 활성화 함수 기능이 들어있음</li><li>입력층, 은닉층, 출력층으로 구성됨</li><li>각 노드의 뉴런 출력은 직접 전달되는 정보에만 의존할 뿐 다른 노드와는 무관</li><li>그래서? 병렬처리가 가능함.</li></ul></li><li><p>손실 함수(Loss or Cost function)</p><ul><li>신경망의 출력값과 실제 결과값의 차이를 정의하는 함수</li><li>신경망 학습목표는 손실함수를 최소화 하는 방향으로 움직여야 함</li><li>SGD, Adam 등의 학습 최적화 알고리즘</li></ul></li><li><p>손실 함수</p><ul><li>회귀(Regression)<br>제곱 오차(MSE) 사용, 최근에는 rmse, mae의 장점이 있는 Huber Loss 사용하는 추세<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_4.PNG" alt="회귀"></li></ul></li></ol><ul><li><p>Huber Loss?<br>MAE + MSE -&gt; for Time Series Data!!<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_5.PNG" alt="huber"><br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_7.PNG" alt="huber Loss"></p><ul><li>분류(Classification)<br>활성화 함수 : softmax, 손실함수 : cross-entropy<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_2.PNG" alt="분류"></li></ul></li></ul><h3 id="알고리즘과-역전파"><a href="#알고리즘과-역전파" class="headerlink" title="알고리즘과 역전파"></a>알고리즘과 역전파</h3><ol><li><p>학습 알고리즘</p><ul><li>경사 하강법: 기울기를 이용하여 손실함수 S(\theta) 값을 최적화</li><li>gradient(기울기)의 반대 방향으로 일정 크기만큼 이동하는 것을 반복하여<br>손실함수의 값을 최소화하는 \theta의 값을 찾음</li><li>\[\theta = \theta - \eta \nabla_\theta S(\theta)\]</li><li>이 떄 \eta 는 미리 정해진 learning rate(step size) 이고 보통 1e-3 ~ 1e-4 정도를 사용</li></ul></li><li><p>역전파</p><ul><li><p>계산 그래프</p></li><li><p>노드는 연산을, 엣지는 데이터의 흐름방향<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_8.PNG" alt="Chain Rule"></p></li><li><p>sigmoid 함수 역전파<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_9.PNG" alt="sigmoidBP"></p></li><li><p>합성함수 미분법(Chain Rule)<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_10.PNG" alt="합성미분"></p></li><li><p>행렬연산과 역전파 1<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_11.PNG" alt="행렬역전파"></p></li><li><p>이진분류 2-layer NN 역전파<br><img src="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_12.PNG" alt="이진역전파"></p></li></ul><blockquote><p>to be continued…</p></blockquote></li></ol>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝-입문과-준비3</title>
      <link>https://ikarus-999.github.io/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/</link>
      <guid>https://ikarus-999.github.io/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/</guid>
      <pubDate>Sun, 22 Nov 2020 19:44:55 GMT</pubDate>
      
      <description>딥러닝 시작해보기-3
차원수 늘리기, 줄이기(TF2.x)
1
2
3
4
5
6
7


x = tf.expand_dims(x, 1)
x.shape # (x.shape, 1)

x[..., tf.newaxis].shape # (x.shape, 1)

np.squeeze(x[0]).shape # x.shape 차원 줄이기



TF2.x Layers
Convolution

 * filters : layer에서 출력될때 몇개의 filter
 * kernel_size : filter(weight) 의 사이즈
 * strides : 몇 개의 </description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-3"><a href="#딥러닝-시작해보기-3" class="headerlink" title="딥러닝 시작해보기-3"></a>딥러닝 시작해보기-3</h2><h3 id="차원수-늘리기-줄이기-TF2-x"><a href="#차원수-늘리기-줄이기-TF2-x" class="headerlink" title="차원수 늘리기, 줄이기(TF2.x)"></a>차원수 늘리기, 줄이기(TF2.x)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.expand_dims(x, <span class="number">1</span>)</span><br><span class="line">x.shape <span class="comment"># (x.shape, 1)</span></span><br><span class="line"></span><br><span class="line">x[..., tf.newaxis].shape <span class="comment"># (x.shape, 1)</span></span><br><span class="line"></span><br><span class="line">np.squeeze(x[<span class="number">0</span>]).shape <span class="comment"># x.shape 차원 줄이기</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="TF2-x-Layers"><a href="#TF2-x-Layers" class="headerlink" title="TF2.x Layers"></a>TF2.x Layers</h3><p>Convolution</p><ul><li>filters : layer에서 출력될때 몇개의 filter</li><li>kernel_size : filter(weight) 의 사이즈</li><li>strides : 몇 개의 pixel만큼 skip하면서 sliding window 할 것인지</li><li>padding : same, zero</li><li>activation : 활성화 함수(<em>Linear function은 층을 쌓는 의미가 없다</em>)</li></ul><blockquote><p>to be Continued…</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>TF2</title>
      <link>https://ikarus-999.github.io/2020/11/22/TF2/</link>
      <guid>https://ikarus-999.github.io/2020/11/22/TF2/</guid>
      <pubDate>Sun, 22 Nov 2020 10:56:34 GMT</pubDate>
      
      <description>Tensorflow 2.x 사용법
Tensor 생성
 * tf.constant() : list, tuple, Array 를 Tensor로 바꿈
   
   
 * tensor = tf.constant(arr)
   
   
 * tensor.dtype : 데이터 타입 확인
   
   
 * tf.cast(tensor, dtype=tf.uint8) : TF int8로 데이터타입 바꾸기
   
   
   
 * tensor.numpy() : numpy array로 바꾸기
   
   

Tensor에 랜덤한 숫자들 생성
 * num</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Tensorflow-2-x-사용법"><a href="#Tensorflow-2-x-사용법" class="headerlink" title="Tensorflow 2.x 사용법"></a>Tensorflow 2.x 사용법</h2><h3 id="Tensor-생성"><a href="#Tensor-생성" class="headerlink" title="Tensor 생성"></a>Tensor 생성</h3><ul><li><p>tf.constant() : list, tuple, Array 를 Tensor로 바꿈</p></li><li><p>tensor = tf.constant(arr)</p></li><li><p>tensor.dtype : 데이터 타입 확인</p></li><li><p>tf.cast(tensor, dtype=tf.uint8) : TF int8로 데이터타입 바꾸기<br><img src="/2020/11/22/TF2/lect_2.PNG" alt="cast사용법"></p></li><li><p>tensor.numpy() : numpy array로 바꾸기</p></li></ul><h3 id="Tensor에-랜덤한-숫자들-생성"><a href="#Tensor에-랜덤한-숫자들-생성" class="headerlink" title="Tensor에 랜덤한 숫자들 생성"></a>Tensor에 랜덤한 숫자들 생성</h3><ul><li>numpy에서는 기본적인 normal distribution 생성<br>np.random.randn(9) : 9개의 불연속적이며 일정한 분포 난수 생성</li></ul><p>Distribution에 따른 난수 생성</p><ul><li><p>tf.random.normal<br>중심극한 이론에 의한 연속적인 모양</p></li><li><p>tf.random.uniform<br>불연속적이며 일정한 분포</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/Tensorflow2/">Tensorflow2</category>
      
      <category domain="https://ikarus-999.github.io/tags/TF2/">TF2</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/22/TF2/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝 입문과 준비2</title>
      <link>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/</link>
      <guid>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/</guid>
      <pubDate>Sun, 22 Nov 2020 10:44:36 GMT</pubDate>
      
      <description>딥러닝 시작해보기-2
Broadcast
두개의 행렬 shape가 서로 달라도
한쪽의 차원이 같거나, 연산하는 값이 한 개일때
shape에 맞게 복사해서 연산함

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21


arr = np.arange(6).reshape(-1, 3)
# [[0, 1, 2], 
#  [3, 4, 5]]

arr + 3
# [[3, 4, 5],
#  [6, 7, 8]]

arr * 3
# [[0, 3, 6],
#  [9, 12 15]

arr + np.array([</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기-2"><a href="#딥러닝-시작해보기-2" class="headerlink" title="딥러닝 시작해보기-2"></a>딥러닝 시작해보기-2</h2><h3 id="Broadcast"><a href="#Broadcast" class="headerlink" title="Broadcast"></a>Broadcast</h3><p>두개의 행렬 shape가 서로 달라도<br>한쪽의 차원이 같거나, 연산하는 값이 한 개일때<br>shape에 맞게 복사해서 연산함</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">6</span>).reshape(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># [[0, 1, 2], </span></span><br><span class="line"><span class="comment">#  [3, 4, 5]]</span></span><br><span class="line"></span><br><span class="line">arr + <span class="number">3</span></span><br><span class="line"><span class="comment"># [[3, 4, 5],</span></span><br><span class="line"><span class="comment">#  [6, 7, 8]]</span></span><br><span class="line"></span><br><span class="line">arr * <span class="number">3</span></span><br><span class="line"><span class="comment"># [[0, 3, 6],</span></span><br><span class="line"><span class="comment">#  [9, 12 15]</span></span><br><span class="line"></span><br><span class="line">arr + np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># [[1, 3, 5],</span></span><br><span class="line"><span class="comment">#  [4, 6, 8]]</span></span><br><span class="line"></span><br><span class="line">np.add(arr, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 모든 원소에 1을 더함</span></span><br><span class="line"></span><br><span class="line">np.multiply(arr, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 모든 원소에 3을 곱함</span></span><br></pre></td></tr></table></figure><h3 id="argmax-argmin"><a href="#argmax-argmin" class="headerlink" title="argmax, argmin"></a>argmax, argmin</h3><p> 배열의 큰 값이나 작은 값의 index return<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">54</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">np.argmax(arr) <span class="comment"># 54</span></span><br><span class="line">np.argmin(arr) <span class="comment"># 1</span></span><br><span class="line">np.unique(arr) <span class="comment"># 유일한 값 출력</span></span><br></pre></td></tr></table></figure></p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>딥러닝 입문과 준비</title>
      <link>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/</link>
      <guid>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/</guid>
      <pubDate>Sun, 22 Nov 2020 09:45:13 GMT</pubDate>
      
      <description>딥러닝 시작해보기
Tensor 이해하기
 1. 차원
    
     * 0차원(상수) : Scalar값
     * 1차원(리스트 씌운 상수), 2차원(2d), 3차원(3d), 4차원(4-d), n차원(n-d) : Tensor
     * Numpy로 Tensor 표현과 응용이 가능
    
    1
    2
    3
    4
    5
    6
    
    
    import numpy as np
    arr = np.array([[3, 6, 9], [2, 4, 8]])
    print(arr.dtype) # </description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-시작해보기"><a href="#딥러닝-시작해보기" class="headerlink" title="딥러닝 시작해보기"></a>딥러닝 시작해보기</h2><h3 id="Tensor-이해하기"><a href="#Tensor-이해하기" class="headerlink" title="Tensor 이해하기"></a>Tensor 이해하기</h3><ol><li><p>차원</p><ul><li>0차원(상수) : Scalar값</li><li>1차원(리스트 씌운 상수), 2차원(2d), 3차원(3d), 4차원(4-d), n차원(n-d) : Tensor</li><li>Numpy로 Tensor 표현과 응용이 가능</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">arr = np.array([[<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>]])</span><br><span class="line">print(arr.dtype) <span class="comment"># dtype(&#x27;float64&#x27;)</span></span><br><span class="line">print(arr.shape) <span class="comment"># (2, 3)</span></span><br><span class="line">print(arr.size) <span class="comment"># 2 * 3 = 6</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>차원 늘리기와 줄이기</p><ul><li><p>reshape, -1 활용</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr.reshape(<span class="number">-1</span>) <span class="comment"># 1차원으로 펼치기</span></span><br><span class="line">arr.reshape(<span class="number">-1</span>, <span class="number">3</span>) <span class="comment"># 첫번째 차원은 알아서, 두번째 차원은 shape 3</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>Ravel() : arr의 차원을 1로 바꿈(==&gt; Flatten)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]) <span class="comment"># (2, 3)</span></span><br><span class="line">arr.ravel()</span><br><span class="line">arr.shape <span class="comment">#(6, )</span></span><br></pre></td></tr></table></figure></li><li><p>np.expand_dims() : 값을 유지하고 차원만 늘릴때</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.expand_dims(arr, <span class="number">-1</span>) <span class="comment">#(6, 1)</span></span><br><span class="line">arr.shape</span><br></pre></td></tr></table></figure><ul><li>numpy array를 빠르게 채우는 방법!</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0으로 채우기</span></span><br><span class="line">arr2 = np.zeros([<span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># 3 * 4의 0이 채워진 배열</span></span><br><span class="line">one2 = np.ones([<span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># 3 * 4의 1로 채워진 배열</span></span><br><span class="line"></span><br><span class="line">five2 = np.ones([<span class="number">3</span>, <span class="number">4</span>]) * <span class="number">5</span> <span class="comment"># 1로 채운 값에 5를 다 곱함</span></span><br><span class="line">arr2 = np.arange(n, m) <span class="comment"># n ~ m-1까지의  수로 배열 채우기</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># array([n ~ m-1])</span></span><br><span class="line">arr = np.arange(<span class="number">5</span>, <span class="number">11</span>).reshape(<span class="number">2</span>, <span class="number">-1</span>) </span><br><span class="line"><span class="comment"># 5 ~ 10 : 6개의 숫자, (2, 3)</span></span><br><span class="line"></span><br><span class="line">arr <span class="comment"># array([5, 6, 7]</span></span><br><span class="line">    <span class="comment">#       [8, 9, 10])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>모양이 맞지 않으면 Error…<br>5, 6, 7, 8, 9는 5개의 숫자<br>5 * 1 만 가능한.<br><img src="/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/slide_lect.PNG" alt="모양다름"></li></ul></li><li><p>Index &amp; slicing</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 리스트 인덱스 &amp; 슬라이싱</span></span><br><span class="line"></span><br><span class="line">nums = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">nums[:<span class="number">-1</span>] <span class="comment"># 마지막 숫자 전까지 표시</span></span><br><span class="line"></span><br><span class="line">nums[::<span class="number">-1</span>] <span class="comment"># 리스트 안의 숫자를 거꾸로 표현</span></span><br><span class="line"></span><br><span class="line">nums = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">print(nums[<span class="number">0</span>][<span class="number">1</span>]) = <span class="number">2</span> <span class="comment"># 첫번째 리스트 안의 인덱스가 1인 숫자</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = np.array([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>])</span><br><span class="line">print(arr[<span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># 10 --&gt; 인덱싱 [행, 열]</span></span><br><span class="line"></span><br><span class="line">print(arr[<span class="number">1</span>:, <span class="number">1</span>:]) <span class="comment"># [[9, 10]]</span></span><br></pre></td></tr></table></figure><ol start="4"><li>Boolean Indexing<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = np.random.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">print(data&lt;=<span class="number">0</span>) <span class="comment"># False, True로 나옴</span></span><br><span class="line"></span><br><span class="line">data[data &lt;=<span class="number">0</span>] = <span class="number">1</span> <span class="comment"># 0 이하인 것을 1로 채우다</span></span><br></pre></td></tr></table></figure></li></ol>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>GAN프로젝트_try</title>
      <link>https://ikarus-999.github.io/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/</link>
      <guid>https://ikarus-999.github.io/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/</guid>
      <pubDate>Tue, 17 Nov 2020 12:28:07 GMT</pubDate>
      
      <description>Style GAN toy 프로젝트
StyleGAN의 특징
 1. 이미지를 Style의 조합으로 보고
    Generator의 각 Layer마다 Style 정보를 입히는 방식으로 이미지 합성
    이 때 각 Layer에서 추가되는 Style은 이미지의 Coarse Feature(포즈, 성별 등)부터
    Fine Detail(머리색, 피부톤 등)까지
    각기 다른 Level의 Visual 속성들을 조절 가능
    StyleGAN은 생각보다 안정적이고 높은 퀄리티의 이미지 생성 

네트워크 구조(Module)
 1. GAN이</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Style-GAN-toy-프로젝트"><a href="#Style-GAN-toy-프로젝트" class="headerlink" title="Style GAN toy 프로젝트"></a>Style GAN toy 프로젝트</h2><h3 id="StyleGAN의-특징"><a href="#StyleGAN의-특징" class="headerlink" title="StyleGAN의 특징"></a>StyleGAN의 특징</h3><ol><li>이미지를 Style의 조합으로 보고<br>Generator의 각 Layer마다 Style 정보를 입히는 방식으로 이미지 합성<br>이 때 각 Layer에서 추가되는 Style은 이미지의 Coarse Feature(포즈, 성별 등)부터<br>Fine Detail(머리색, 피부톤 등)까지<br>각기 다른 Level의 Visual 속성들을 조절 가능<br>StyleGAN은 생각보다 안정적이고 높은 퀄리티의 이미지 생성  </li></ol><h3 id="네트워크-구조-Module"><a href="#네트워크-구조-Module" class="headerlink" title="네트워크 구조(Module)"></a>네트워크 구조(Module)</h3><ol><li>GAN이란 어떤 것일까???<br><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/GANdesc.png" alt="이것이 GAN"></li></ol><p><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide21.PNG" alt="GAN model"></p><ul><li>Instance Norm?<br><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide22.PNG" alt="Instance Norm"></li></ul><p><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide23.PNG" alt="Instance Norm2"></p><ul><li>Generator 구조 설명</li></ul><p><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide322.png" alt="model"></p><p>왼쪽이 Traditional Network, 오른쪽이 이 논문에서 제안한 Style-gased Generator. </p><p>왼쪽 네트워크와 오른쪽에 Synthesis Network가 똑같은 구조를 갖고 있지만,<br>이전 GAN에서는 Latent z를 바로 Input으로 넣어줬던 것과는 다르게,<br>StyleGAN에서는 학습된 Constant, (w) 값을 입력으로 사용함. </p><p>새롭게 Mapping Network와 Noise가 추가됨..</p><p>W를 Feature에 매핑하는 경우<br>W는 Z처럼 고정된 분포를 따르지 않음. </p><p>Sampling density는 학습된 Piecewise Continuous Mapping f(z)<br>(f는 Mapping Network 입니다)에 의해 정해짐. </p><p>따라서, Warping(틀어짐)이 많이 일어나지 않음.<br>그렇기 때문에 Factors of variation은 더욱 Linear하고, Disentangled (얽히지 않음).<br>이것이 바로 z를 곧바로 Feature에 매핑하는 것보다 w에 매핑하는 것의 장점입니다</p><p>기존의 Generator (a)는<br>Input Latent Vector (z)가 직접 Convolution, Upsampling 등을 거쳐 이미지로 변환되는 구조. </p><p>Style-based Generator (b) 의 경우,<br>(z)가  Fully-connected Layer로 구성된 Mapping Network을 거쳐<br>Intermediate Latent Vector (w) 먼저 변환. </p><p>(w)는 Constant Tensor가 이미지로 변환되는 과정에서<br>스타일을 입히는 역할을 수행.</p><p>다양한 스타일의 이미지를 생성.</p><ul><li>Style Transfer를 실시간으로 가능케하는 Adaptive Instance Norm</li></ul><p>Synthesis Network (합성 네트워크)<br>z를 중간 latent space W에 매핑을 한 뒤에 이 w는 “A”를 거쳐서 style, y=(ys,yb)<br>로 변형됨. 이때 A는 학습된 affine transform 임. 그리고 이 style들은<br>AdaIN(adaptive instance normalization) opeartion을 control 함.<br><img src="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/Adain.png" alt="AdaIN"></p><p>AdaIN은 style transfer를 할 때 많이 쓰이는 방법으로, 임의의 style transfer를 실시간으로 가능하게 함.<br>여기서 feature map xi는<br>normalized 된 다음에, style로 변환된 두 y로 scaled, biased 됨. (style이 입혀짐)<br>이 과정을 매 layer 마다 반복함. 그리고 이러한 방법은 scale-specific control 을 가능하게 함.</p><blockquote><p>To be continued…</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      <category domain="https://ikarus-999.github.io/tags/GAN/">GAN</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>training_pc</title>
      <link>https://ikarus-999.github.io/2020/11/16/training-pc/</link>
      <guid>https://ikarus-999.github.io/2020/11/16/training-pc/</guid>
      <pubDate>Sun, 15 Nov 2020 17:00:28 GMT</pubDate>
      
      <description>딥러닝 공부 후기
비정형 데이터를 다룰려면 GPU는 필수다.

Why GPU ??
 1. CPU보다 더 빠른 병렬 처리 가능

 *  행렬곱 계산이 CPU보다 훨씬 빠름

 2. 계산 그래프 빌드, 처리 속도가 빠름.

 * 비정형 데이터 처리엔 GPU가 필수

음성 딥러닝, 나도 해볼까?
음성 딥러닝

음성 딥러닝은 결코 쉽지 않다. 딥러닝계의 보스급.
신호처리 배워야 그나마 수월하다.
초반 Feature Extraction 경험을 쌓는것을 권장한다.
RNN계열의 LSTM으로 시작. 하지만,
Attention, Transforme</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝-공부-후기"><a href="#딥러닝-공부-후기" class="headerlink" title="딥러닝 공부 후기"></a>딥러닝 공부 후기</h2><blockquote><p>비정형 데이터를 다룰려면 GPU는 필수다.</p></blockquote><h3 id="Why-GPU"><a href="#Why-GPU" class="headerlink" title="Why GPU ??"></a>Why GPU ??</h3><ol><li>CPU보다 더 빠른 병렬 처리 가능</li></ol><ul><li> 행렬곱 계산이 CPU보다 훨씬 빠름</li></ul><ol start="2"><li>계산 그래프 빌드, 처리 속도가 빠름.</li></ol><ul><li>비정형 데이터 처리엔 GPU가 필수</li></ul><h3 id="음성-딥러닝-나도-해볼까"><a href="#음성-딥러닝-나도-해볼까" class="headerlink" title="음성 딥러닝, 나도 해볼까?"></a>음성 딥러닝, 나도 해볼까?</h3><p><a href="https://github.com/topics/vggish">음성 딥러닝</a></p><p>음성 딥러닝은 결코 쉽지 않다. 딥러닝계의 보스급.<br>신호처리 배워야 그나마 수월하다.<br>초반 Feature Extraction 경험을 쌓는것을 권장한다.<br>RNN계열의 LSTM으로 시작. 하지만,<br>Attention, Transformer 날코딩 등 논문 구현 경험이 매우 중요하다.<br>Mel Spectrogram을 한다 해도 원리를 잘 알아야 나중에 모델링과 데이터 분해가 쉽다.</p><h3 id="딥러닝-입문-방법"><a href="#딥러닝-입문-방법" class="headerlink" title="딥러닝 입문 방법"></a>딥러닝 입문 방법</h3><p>딥러닝 입문 하려면 수학은 필수. 정말 중요.</p><p><a href="https://paperswithcode.com/">논문 구현 시도해보기1</a><br><a href="https://arxiv.org/abs/1706.03762">논문 구현 시도해보기2</a></p><blockquote><p>어떤 논문에는 <em>파라미터 하나도 없는</em> 것도 있다.<br>레이어 구조도만 있고 파라미터가 없는 것은 진짜 구현난이도 보스급.</p></blockquote><h3 id="딥러닝-자격증-취득-후기"><a href="#딥러닝-자격증-취득-후기" class="headerlink" title="딥러닝 자격증 취득 후기"></a>딥러닝 자격증 취득 후기</h3><p>이것이 바로 딥러닝 자격증!!<br><img src="https://api.accredible.com/v1/frontend/credential_website_embed_image/certificate/25378511," alt="우와~" title="WoW!"></p><ol><li>문제 유형</li></ol><ul><li><p>Category 1: Basic model</p></li><li><p>Category 2: Model from learning dataset</p></li><li><p>Category 3: Image classification<br>Convolutional Neural Network with real-world image dataset</p></li><li><p>Category 4: Natural language processing (NLP)<br>NLP Text Classification with real-world text dataset</p></li><li><p>Category 5: Time series, sequences and predictions<br>Sequence Model with real-world numeric dataset</p></li><li><p>이 시험은 응시자가 TensorFlow 2.x를 통해 모델을 빌드하여 문제를 해결할 수 있는지<br>테스트합니다.</p></li><li><p>머신러닝(ML) 및 딥러닝의 기본 원칙</p></li><li><p>TensorFlow 2.x에서 ML 모델 개발하기</p></li><li><p>심층신경망 및 합성곱 신경망(CNN)을 통한 이미지 인식, 객체 탐지, 텍스트 인식 알고리즘 빌드</p></li><li><p>컴퓨터가 정보를 ‘보는’ 방식과 플롯 손실 및 정확도 이해할 수 있도록 다른 크기 및 형태의 실제 이미지를 활용하여 합성곱에서 이미지의 경로를 시각화</p></li><li><p>과적합을 예방하기 위한 확장 및 드롭아웃과 같은 전략 탐색</p></li><li><p>TensorFlow를 이용하여 자연어 처리 문제를 해결하기 위해 신경망 적용</p></li></ul><blockquote><p>Google 공식 페이지 발췌</p></blockquote><ol start="2"><li>합격 점수 / 규칙</li></ol><blockquote><p>허용된 인터넷 브라우징은 Tensorflow document, 총 100점 만점에 90점 커트라인</p></blockquote><ul><li>난이도는 category 5가 가장 높음</li></ul><blockquote><p>시험 시간 : 5시간 (컴퓨터 딥러닝 훈련시간 포함)</p></blockquote><ol start="3"><li>따면 좋은 점</li></ol><p>개발자 네트워크에 join 할 수가 있어요~</p><p><a href="https://www.tensorflow.org/certificate-network?hl=ko">자격증 네트워크</a></p><p><a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials">참고문서</a></p><p>그럼 20000~!</p>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/11/16/training-pc/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>fpost3</title>
      <link>https://ikarus-999.github.io/2020/10/26/fpost3/</link>
      <guid>https://ikarus-999.github.io/2020/10/26/fpost3/</guid>
      <pubDate>Sun, 25 Oct 2020 15:01:21 GMT</pubDate>
      
      <description>일상을 끄적이다 - Mel Spectrogram 자동 분석을 위한 test page 입니다.
M2U - March Of Fear
비탄으로 가득 찬 이 도시 안에 떨어지는 Melody,

감정을 연기하는 춤추는 Endless rain…

거짓말로 얼룩진 관계들 속에 웃을 수 있는 거야?

세계를 노래하는 The March of Fear.

We live in the tragedy! 유명한 비극과 같은

잔혹한 세계 속에 우리는 웃음짓고 있어.

이 세계를 비웃는 고결한 수호자들에게

지켜낼 것들은 두려웠던 자신뿐이니까!

숨을 멈춘 </description>
      
      
      
      <content:encoded><![CDATA[<h2 id="일상을-끄적이다-Mel-Spectrogram-자동-분석을-위한-test-page-입니다"><a href="#일상을-끄적이다-Mel-Spectrogram-자동-분석을-위한-test-page-입니다" class="headerlink" title="일상을 끄적이다 - Mel Spectrogram 자동 분석을 위한 test page 입니다."></a>일상을 끄적이다 - Mel Spectrogram 자동 분석을 위한 test page 입니다.</h2><h3 id="M2U-March-Of-Fear"><a href="#M2U-March-Of-Fear" class="headerlink" title="M2U - March Of Fear"></a>M2U - March Of Fear</h3><p>비탄으로 가득 찬 이 도시 안에 떨어지는 Melody,</p><p>감정을 연기하는 춤추는 Endless rain…</p><p>거짓말로 얼룩진 관계들 속에 웃을 수 있는 거야?</p><p>세계를 노래하는 The March of Fear.</p><p>We live in the tragedy! 유명한 비극과 같은</p><p>잔혹한 세계 속에 우리는 웃음짓고 있어.</p><p>이 세계를 비웃는 고결한 수호자들에게</p><p>지켜낼 것들은 두려웠던 자신뿐이니까!</p><p>숨을 멈춘 사람들 속에서</p><p>청명하게 울려퍼지는 Aria…</p><p>병든 마음으로 가득찬 이 세계에 폭격을 날려!</p><p>지금 웃고 있는 너도 Sociopath, 알잖아?</p><p>우리는 이 곳에 생명이 깃든 포성을 던져!</p><h3 id="에픽세븐-OST-「Promise」"><a href="#에픽세븐-OST-「Promise」" class="headerlink" title="에픽세븐 OST 「Promise」"></a>에픽세븐 OST 「Promise」</h3><p>깊은 절망 속에 갇혀 쓰러져 가는 나의 두 손을<br>가만히 잡아 준 따스한 너의 온기를 아직<br>난 기억해<br>Now I can hear you<br>눈을 감으면<br>손 끝에서 널 느껴<br>Now I can hear you<br>I can find you<br>이젠 알 수 있어<br>　<br>약속해 Promise I promise 잊지 않을게<br>우리 함께 나눈 약속들을<br>Promise I promise 어둠을 지난<br>N 번째 하늘 아래 이곳에<br>끝이 보이지 않아도 때론 지쳐서 주저 앉아도<br>약속해 희망을 우리 마음에 간절히 모아<br>빛을 향해<br>Now I can hear you<br>눈을 감으면<br>손 끝에서 널 느껴<br>Now I can hear you<br>I can find you<br>이젠 알 수 있어<br>약속해 Promise I promise 잊지 않을게<br>우리 함께 나눈 약속들을<br>Promise I promise 어둠을 지난<br>N 번째 하늘 아래 이곳에<br>　<br>다시 시작된 시간 다시 걸어가는 길<br>함께하기에 나는 빛날 수 있어<br>멀리 조금씩 보여 따뜻한 파란 빛이<br>우리 마음을 여기에 모아 시작해 한번 더<br>　<br>약속해 Promise I promise 잊지 않을게<br>우리 함께 나눈 약속들을<br>Promise I promise 어둠을 지난<br>N 번째 하늘 아래 </p><p>약속들을</p><p>Promise I promise 어둠을 지난<br>N 번째 하늘 아래 이곳에</p><blockquote><p>Mel Spectrogram 분석 중인 음악입니다. <em><em>GPU와 스토리지 지원좀 부탁드립니다.</em></em></p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/ilsang/">ilsang</category>
      
      <category domain="https://ikarus-999.github.io/categories/ilsang/Music/">Music</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/Music-EDA/">Music EDA</category>
      
      <category domain="https://ikarus-999.github.io/tags/Script/">Script</category>
      
      
      <comments>https://ikarus-999.github.io/2020/10/26/fpost3/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>fpost2</title>
      <link>https://ikarus-999.github.io/2020/10/24/fpost2/</link>
      <guid>https://ikarus-999.github.io/2020/10/24/fpost2/</guid>
      <pubDate>Sat, 24 Oct 2020 14:08:30 GMT</pubDate>
      
      <description>머신러닝과 딥러닝의 차이
머신러닝
 1. 머신러닝은 정형 데이터

 * 표 형태의 데이터

 2. 딥러닝은 비정형 데이터

 * 그림, 사진, 오디오 형태의 자연 데이터

머신러닝 기초 준비물
충분한 용량(RAM 32GB++, SSD 512GB++, i7-10Gen++)의 데스크탑 권장!

가장 먼저 파이썬, 텐서플로우 를 설치합니다.

설치 버전 확인!! 매우 중요합니다!

 1. 파이썬
 2. 텐서플로우
 3. 넘파이 &amp; 싸이킷런

파이썬 버전을 여러개 설치 가능합니다. 이때는 환경 변수, IDE 경로 셋팅이 중요합니다.

윈도</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="머신러닝과-딥러닝의-차이"><a href="#머신러닝과-딥러닝의-차이" class="headerlink" title="머신러닝과 딥러닝의 차이"></a>머신러닝과 딥러닝의 차이</h2><h3 id="머신러닝"><a href="#머신러닝" class="headerlink" title="머신러닝"></a>머신러닝</h3><ol><li>머신러닝은 정형 데이터</li></ol><ul><li>표 형태의 데이터</li></ul><ol start="2"><li>딥러닝은 비정형 데이터</li></ol><ul><li>그림, 사진, 오디오 형태의 자연 데이터</li></ul><h3 id="머신러닝-기초-준비물"><a href="#머신러닝-기초-준비물" class="headerlink" title="머신러닝 기초 준비물"></a>머신러닝 기초 준비물</h3><blockquote><p>충분한 용량(RAM 32GB++, SSD 512GB++, i7-10Gen++)의 데스크탑 권장!</p></blockquote><blockquote><p>가장 먼저 <strong>파이썬</strong>, <strong>텐서플로우</strong> 를 설치합니다.</p><blockquote><p><em>설치 버전 확인!!</em> 매우 중요합니다!</p></blockquote></blockquote><ol><li>파이썬</li><li>텐서플로우</li><li>넘파이 &amp; 싸이킷런</li></ol><blockquote><p>파이썬 버전을 여러개 설치 가능합니다. 이때는 <strong>환경 변수, IDE 경로 셋팅이 중요</strong>합니다.</p><blockquote><p>윈도우라면 CMD보다는 powershell을 쓰는것이 편합니다.</p></blockquote></blockquote><ol start="4"><li>제일 쉬운 방법은 아나콘다🛎… 하지만 용량이 큽니다.</li></ol><blockquote><p>써본 결과 리눅스에 도커가 최적 환경입니다⚙️. 추후 포스팅 예정 입니다.📯</p></blockquote><h3 id="Lets-Burn-the-GPU-🔥🔥🔥-TF2-0-wow"><a href="#Lets-Burn-the-GPU-🔥🔥🔥-TF2-0-wow" class="headerlink" title="Lets Burn the GPU!!🔥🔥🔥(TF2.0 wow!!)"></a>Lets Burn the GPU!!🔥🔥🔥(TF2.0 wow!!)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensorflow 실행모드를 확인합니다</span></span><br><span class="line">print(tf.executing_eagerly())</span><br><span class="line"></span><br><span class="line">shape = (<span class="built_in">int</span>(<span class="number">10000</span>), <span class="built_in">int</span>(<span class="number">10000</span>))</span><br><span class="line">startTime = datetime.now()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&quot;/gpu&quot;</span>):</span><br><span class="line">    random_matrix = tf.random.uniform(shape=shape, minval=<span class="number">0</span>, maxval=<span class="number">1</span>)</span><br><span class="line">    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))</span><br><span class="line">    sum_operation = tf.reduce_sum(dot_operation)</span><br><span class="line"></span><br><span class="line">result = sum_operation</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;\n&quot;</span> * <span class="number">2</span>)</span><br><span class="line">print(<span class="string">&quot;Time taken:&quot;</span>, datetime.now() - startTime)</span><br><span class="line">print(<span class="string">&quot;\n&quot;</span> * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/Machine-Learning/">Machine Learning</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/ML/">ML</category>
      
      <category domain="https://ikarus-999.github.io/tags/DL/">DL</category>
      
      
      <comments>https://ikarus-999.github.io/2020/10/24/fpost2/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>firstpost1</title>
      <link>https://ikarus-999.github.io/2020/10/24/firstpost1/</link>
      <guid>https://ikarus-999.github.io/2020/10/24/firstpost1/</guid>
      <pubDate>Sat, 24 Oct 2020 13:42:28 GMT</pubDate>
      
      <description>HEXO 블로그 설치와 사용법
새 글 쓰는 법
1


$ hexo new [레이아웃이름] &quot;새 포스트이름&quot;


레이아웃 디폴트(바로 발행) : post

바로 발행되지 않는 글 : draft

1


$ hexo publish &quot;새 포스트이름&quot;


으로 draft에서 publish 합니다.

publish는 잘 쓰지 않는다 파일명 에러나기 때문. 그냥
냅다 MarkDown으로 쓰고 hexo g -d로 해버림</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="HEXO-블로그-설치와-사용법"><a href="#HEXO-블로그-설치와-사용법" class="headerlink" title="HEXO 블로그 설치와 사용법"></a>HEXO 블로그 설치와 사용법</h2><h3 id="새-글-쓰는-법"><a href="#새-글-쓰는-법" class="headerlink" title="새 글 쓰는 법"></a>새 글 쓰는 법</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new [레이아웃이름] <span class="string">&quot;새 포스트이름&quot;</span></span><br></pre></td></tr></table></figure><p>레이아웃 디폴트(바로 발행) : post</p><p>바로 발행되지 않는 글 : draft</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo publish <span class="string">&quot;새 포스트이름&quot;</span></span><br></pre></td></tr></table></figure><p>으로 draft에서 publish 합니다.</p><blockquote><p>publish는 잘 쓰지 않는다 파일명 에러나기 때문. 그냥<br>냅다 MarkDown으로 쓰고 hexo g -d로 해버림</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://ikarus-999.github.io/categories/GitPage/">GitPage</category>
      
      <category domain="https://ikarus-999.github.io/categories/GitPage/Hexo/">Hexo</category>
      
      
      <category domain="https://ikarus-999.github.io/tags/HEXO/">HEXO</category>
      
      <category domain="https://ikarus-999.github.io/tags/blog/">blog</category>
      
      
      <comments>https://ikarus-999.github.io/2020/10/24/firstpost1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Hello World</title>
      <link>https://ikarus-999.github.io/2020/10/24/hello-world/</link>
      <guid>https://ikarus-999.github.io/2020/10/24/hello-world/</guid>
      <pubDate>Sat, 24 Oct 2020 11:12:41 GMT</pubDate>
      
      <description>Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.

Quick Start
Create a new post
1


$ hexo new &quot;My New Post&quot;


More info: Writing

Run server
1


$ hexo s</description>
      
      
      
      <content:encoded><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content:encoded>
      
      
      
      
      <comments>https://ikarus-999.github.io/2020/10/24/hello-world/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
